{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs\n",
    "\n",
    "Put the inputs to the first cell.\n",
    "\n",
    "* `path_to_bert_training_data`: Path to a pickle file that contains both training and dev data. \n",
    "    1. Prepare the data\n",
    "        * The dataset should be a list of dictionaries. Each dictionary should correspond to one sample. The keys of the dictionary are \"tokens\", which stores a list of strings corresponding to the tokenized version of the input sentence, and \"tags\", the IOB tag for each token.\n",
    "            ```python\n",
    "            data = [\n",
    "                        {\"tokens\":['token_a','token_b'],\"tags\":['tag_a','tag_b']},\n",
    "                        {\"tokens\":['token_a','token_b','token_c'],\"tags\":['tag_a','tag_b','tag_c']},\n",
    "                        ...\n",
    "                    ]\n",
    "            ```\n",
    "        * Create two such variables, one for the training set and another one for the dev set (say `train_data` and `dev_data`). \n",
    "        * IOB tags: `\"O\", \"B_ORG\", \"I_ORG\", \"B_GRT\", \"I_GRT\"`\n",
    "        * Tokenization of the input: Please make sure you tokenize the input with:\n",
    "        ```python\n",
    "        from transformers import PreTrainedTokenizerFast\n",
    "        tokenizer = PreTrainedTokenizerFast.from_pretrained('bert-base-cased')\n",
    "        ```\n",
    "        If a word is split into subwords, make sure to tag it appropriately. Example:\n",
    "            * word: `\"word_xyz\"`, tag: `\"O\"`, tokenized: `\"word\", \"##_\", \"##xyz\"`\n",
    "                * Corresponding tags for the tokenized wordpieces: `\"O\",-100,-100`\n",
    "        Hence, assign the tag to the first WordPiece, and assing the integer -100 to the rest of the wordpieces\n",
    "    2. Pickle the data\n",
    "    ```python\n",
    "with open(path_to_bert_training_data,'wb') as f:\n",
    "            pickle.dump(train_data,f)\n",
    "            pickle.dump(dev_data,f)\n",
    "    ```\n",
    "    3. How will the data be unpickled here?\n",
    "    ```python\n",
    "with open(path_to_bert_training_data,'rb') as f:\n",
    "            train_dataset=pickle.load(f)\n",
    "            dev_dataset=pickle.load(f)\n",
    "    ```\n",
    "* `path_to_bert_sc`: Path to the folder containing BERT Scopus. Use \"bert-base-cased\" if BERT Scopus not available.\n",
    "\n",
    "# Output\n",
    "* `bert_sc_ner.pt`: Trained BERT (Scopus) NER model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_bert_training_data = 'bert_training_data.pkl'\n",
    "path_to_bert_sc = 'bert-base-cased'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credits\n",
    "* https://huggingface.co/transformers/custom_datasets.html#tok-ner\n",
    "* https://github.com/huggingface/notebooks/blob/master/examples/token_classification.ipynb\n",
    "* https://medium.com/@prakashakshay/fine-tuning-bert-model-using-pytorch-f34148d58a37\n",
    "* https://mccormickml.com/2019/07/22/BERT-fine-tuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7df28981-291a-407f-b26f-ddab3a9ba7ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from datasets import load_metric\n",
    "import gc\n",
    "from transformers import BertForTokenClassification, PreTrainedTokenizerFast\n",
    "seed_num=0\n",
    "np.random.seed(seed_num)\n",
    "random.seed(seed_num)\n",
    "torch.manual_seed(seed_num)\n",
    "torch.cuda.manual_seed(seed_num)\n",
    "torch.cuda.manual_seed_all(seed_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4350317b-025b-4204-9d39-b638b6a3afab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#From the example GitHub Notebook\n",
    "def compute_metrics(p,id2tag):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [id2tag[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2tag[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    print(\"\\t\\tORG Precision: \",results['_ORG']['precision'])\n",
    "    print(\"\\t\\tORG Recall: \",results['_ORG']['recall'])\n",
    "    print(\"\\t\\tORG F1: \",results['_ORG']['f1'])\n",
    "    print(\"\\t\\tGRT Precision: \",results['_GRT']['precision'])\n",
    "    print(\"\\t\\tGRT Recall: \",results['_GRT']['recall'])\n",
    "    print(\"\\t\\tGRT F1: \",results['_GRT']['f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4a9870fc-ca06-452e-883a-a7645d548f24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Evaluate the model on the train_monitor set\n",
    "def eval_on_valid(model, train_monitor_loader,id2tag):\n",
    "    #Accumulate the predictions here\n",
    "    val_preds = np.zeros((0,512,5))\n",
    "    #Accumulate the labels here\n",
    "    val_lbls = np.zeros((0,512))\n",
    "    #Accumulate the oss here\n",
    "    val_loss = 0\n",
    "    #Loop over minibatches\n",
    "    for i_val, batch_val in enumerate(train_monitor_loader):\n",
    "        #Get the max length in this batch and crop based on that\n",
    "        seq_lens = batch_val['seq_len']\n",
    "        max_len_for_batch = max(seq_lens.cpu().detach().numpy())\n",
    "        #Get inputs and labels for that batch and crop\n",
    "        input_ids_val = torch.tensor(batch_val['input_ids'][:,:max_len_for_batch].detach().numpy()).to(device)\n",
    "        attention_mask_val = torch.tensor(batch_val['attention_mask'][:,:max_len_for_batch].detach().numpy()).to(device)\n",
    "        labels_val = torch.tensor(batch_val['labels'][:,:max_len_for_batch].detach().numpy()).to(device)\n",
    "        #Do a forward pass\n",
    "        outputs_val = model(input_ids_val, attention_mask=attention_mask_val, labels=labels_val)\n",
    "        #First index is the loss. Since the output loss is the mean over minibatch samples,\n",
    "        #we multiply it with batch size. Later, we divide it by the number of samples\n",
    "        val_loss += outputs_val[0].item()\n",
    "        #Save the loss and labels\n",
    "        these_preds = outputs_val[1].cpu().detach().numpy()\n",
    "        these_labels= labels_val.cpu().detach().numpy()\n",
    "        #Pad the predictions again\n",
    "        new_preds = np.ones((len(input_ids_val),512,5)) * -100\n",
    "        new_labels= np.ones((len(input_ids_val),512)) * -100\n",
    "        new_preds[:,:max_len_for_batch,:] = these_preds\n",
    "        new_labels[:,:max_len_for_batch] = these_labels\n",
    "        #Store in array\n",
    "        val_preds = np.concatenate([val_preds,new_preds],axis=0)\n",
    "        val_lbls = np.concatenate([val_lbls,new_labels],axis=0)\n",
    "    print(\"\\tValidation Loss: \",val_loss/len(train_monitor_loader))\n",
    "    p = (val_preds, val_lbls)\n",
    "    print(\"\\tValidation Results: \")\n",
    "    compute_metrics(p,id2tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "85b6eee8-9f2d-4455-830a-a04622d0762b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Class for funding bodies dataset\n",
    "\n",
    "class FB_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels,at_mask,seq_lens):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.at_mask = at_mask\n",
    "        self.seq_lens = seq_lens\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = dict()\n",
    "        item['input_ids'] = torch.tensor(self.encodings[idx])\n",
    "        item['attention_mask'] = torch.tensor(self.at_mask[idx])\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        item['seq_len'] =self.seq_lens[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "#Add [CLS] and [SEP] tokens, pad until \"pad_len\" chars.\n",
    "def add_and_pad(lst,pad_len,cls,sep,pad):\n",
    "    new_lst = []\n",
    "    for item in lst:\n",
    "        new_item = [cls] + item + [sep]\n",
    "        while len(new_item) != pad_len:\n",
    "            new_item.append(pad)\n",
    "        new_lst.append(new_item)\n",
    "    return new_lst\n",
    "    \n",
    "def convert_to_fb_dataset(original_input,tokenizer,max_len=512):\n",
    "    tag2id = {'I_GRT':0, 'O':1,'B_GRT':2, 'B_ORG':3, 'I_ORG':4, -100:-100}\n",
    "    texts = [x['tokens'] for x in original_input]\n",
    "    tags = [x['tags'] for x in original_input]\n",
    "    encodings = tokenizer(texts, is_split_into_words=True,add_special_tokens =False)['input_ids']\n",
    "    seq_lens = [len(x)+2 for x in encodings]\n",
    "    encodings = add_and_pad(encodings,max_len,101,102,0)\n",
    "    attention_mask = [[0 if num==0 else 1 for num in lst]  for lst in encodings]\n",
    "    labels = add_and_pad(tags,max_len,-100,-100,-100)\n",
    "    labels = [[tag2id[x] for x in label]  for label in labels]\n",
    "    \n",
    "    return FB_Dataset(encodings, labels,attention_mask,seq_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f51740db-ebd4-4d07-be29-a7d4cf2cf6b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(path_to_bert_sc, num_labels=5)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "205bbb72-c24d-4023-80ef-49d8655d8017",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open(path_to_bert_training_data,'rb') as f:\n",
    "    train_dataset=pickle.load(f)\n",
    "    train_monitor_dataset=pickle.load(f)\n",
    "\n",
    "\"\"\"\n",
    "Example of what train_dataset (or train_monitor_dataset) should look like\n",
    "train_dataset = [{\"tokens\":['This','work','was','supported','by','National','Institute','of','Health','with','grant','number','ABC','##12','##3','.'],\n",
    "                   \"tags\": ['O','O','O','O','O','B_ORG','I_ORG','I_ORG','I_ORG','O','O','O','B_GRT',-100,-100,'O']} ,\n",
    "                 {\"tokens\":['Financial','support','received','from','Dutch','Ministry','of','Health','with','Grant','234'],\n",
    "                   \"tags\": ['O','O','O','O','B_ORG','I_ORG','I_ORG','I_ORG','O','O','B_GRT']} \n",
    "                ]\n",
    "\"\"\"\n",
    "train_dataset = convert_to_fb_dataset(train_dataset,tokenizer)\n",
    "train_monitor_dataset = convert_to_fb_dataset(train_monitor_dataset,tokenizer)\n",
    "\n",
    "id2tag= {0: 'I_GRT', 1: 'O', 2: 'B_GRT', 3: 'B_ORG', 4: 'I_ORG'}\n",
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "863bdadb-eb6c-48a2-9fcd-63a3eb694ed8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Pick the device\n",
    "device = torch.device('cuda')\n",
    "\n",
    "#Put model to device\n",
    "model.to(device)\n",
    "\n",
    "#Put model to training mode\n",
    "model.train()\n",
    "\n",
    "#Define training batch size\n",
    "batch_size=8#increase this\n",
    "num_epochs = 3\n",
    "\n",
    "#Get training sample generator\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "train_monitor_loader = DataLoader(train_monitor_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#Initialize optimizer\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=2e-5) \n",
    "\n",
    "#Determine how many steps each epoch will take\n",
    "print(\"Steps per epoch: \" ,len(train_loader))\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optim, \n",
    "                                            num_warmup_steps = 50, \n",
    "                                            num_training_steps = len(train_loader)*num_epochs)\n",
    "\n",
    "minibatch_losses = []\n",
    "#Every x step, print training loss\n",
    "every_x_step=500\n",
    "\n",
    "#Loop over epochs\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch: \",epoch)\n",
    "    #Loop over minibatches\n",
    "    #Accumulate training statistics\n",
    "    train_loss = 0\n",
    "    train_preds = np.zeros((0,512,5))\n",
    "    train_lbls = np.zeros((0,512))\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        gc.collect()\n",
    "        start = time.time()\n",
    "        #reset gradients\n",
    "        optim.zero_grad()\n",
    "        #Get the max length in this batch and crop based on that\n",
    "        seq_lens = batch['seq_len']\n",
    "        max_len_for_batch = max(seq_lens.cpu().detach().numpy())\n",
    "        #get inputs\n",
    "        input_ids = torch.tensor(batch['input_ids'][:,:max_len_for_batch].detach().numpy()).to(device)\n",
    "        attention_mask = torch.tensor(batch['attention_mask'][:,:max_len_for_batch].detach().numpy()).to(device)\n",
    "        labels = torch.tensor(batch['labels'][:,:max_len_for_batch].detach().numpy()).to(device)\n",
    "        #When we call a classification model with the labels argument, the first returned element is the Cross Entropy loss between the predictions and the passed labels. \n",
    "        #Calculate loss\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        #attention_mask.detach()\n",
    "        del attention_mask\n",
    "        #loss is reduced by mean (so it roughly corresponds to loss of one sample)\n",
    "        #https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "        #https://huggingface.co/transformers/_modules/transformers/models/bert/modeling_bert.html#BertForTokenClassification.forward\n",
    "        loss = outputs[0]\n",
    "        #Print loss\n",
    "        train_loss+=loss.item()\n",
    "        #Second index is the predictions, store them\n",
    "        these_preds = outputs[1].cpu().detach().numpy()\n",
    "        these_labels= labels.cpu().detach().numpy()\n",
    "        del outputs\n",
    "        #labels.detach()\n",
    "        del labels\n",
    "        #Pad the predictions again\n",
    "        new_preds = np.ones((len(input_ids),512,5)) * -100\n",
    "        new_labels= np.ones((len(input_ids),512)) * -100\n",
    "        #input_ids.detach()\n",
    "        del input_ids\n",
    "        new_preds[:,:max_len_for_batch,:] = these_preds\n",
    "        new_labels[:,:max_len_for_batch] = these_labels\n",
    "        #Save the labels\n",
    "        train_lbls = np.concatenate([train_lbls,new_labels],axis=0)\n",
    "        train_preds = np.concatenate([train_preds,new_preds],axis=0)\n",
    "        #backpropagation\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        #update the parameters\n",
    "        optim.step()\n",
    "        scheduler.step()\n",
    "        end = time.time()\n",
    "        minibatch_losses.append(loss.item())\n",
    "        torch.cuda.empty_cache()\n",
    "        #Every x step, print validation scores\n",
    "        if (i+1)%every_x_step==0:          \n",
    "            #Print training loss for this minibatch\n",
    "            print(\"\\tStep \",i+1,\"/\",len(train_loader))\n",
    "            print(\"\\t\\tBatch padding: \",max_len_for_batch)\n",
    "            print(\"\\t\\tMinibatch training loss: \",loss.item())\n",
    "            print(\"\\t\\tTime for this minibatch: \",end-start)\n",
    "            #Check val loss\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                #Evaluate the current model on the validation set\n",
    "                eval_on_valid(model, train_monitor_loader,id2tag)\n",
    "            model.train()\n",
    "            #Save Model\n",
    "            #with open(\"/dbfs/mnt/els-nlp-experts1/data/Gizem/bert_epoch_\"+str(epoch)+\"_step_\"+str(i)+'.pt','wb') as f:\n",
    "            #    torch.save(model, f)\n",
    "       \n",
    "    print(\"\\tApproximate Training loss for this epoch: \",train_loss/len(train_loader))\n",
    "    print(\"\\tApproximate Training results: \")\n",
    "    compute_metrics((train_preds, train_lbls),id2tag)\n",
    "    #Check val loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        #Evaluate the current model on the validation set\n",
    "        eval_on_valid(model, train_monitor_loader,id2tag)\n",
    "    model.train()\n",
    "with open(\"bert_sc_ner.pt\",'wb') as f:\n",
    "    torch.save(model, f)\n",
    "\n",
    "\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "KEEP_Train_BERT_NER_PretrainedBERT_3_3epsched",
   "notebookOrigID": 3311484,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
