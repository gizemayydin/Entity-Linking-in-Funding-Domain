{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "### Inputs\n",
    "Change the values in the second cell for inputs.\n",
    "1. `batch_size`: The batch size for model predictions. If the value is higher, the model will run faster. However, it would use more memory. Set it to an integer that is a power of 2.\n",
    "2. `device_str`: Set to `\"cuda\"` to use GPU, `\"cpu\"` to use CPU.\n",
    "3. `path_to_model`: Path to the NER model\n",
    "4. `file_path`: Write the sentences line by line to a text file. This variable should hold the path to the text file.\n",
    "5. If you would like to input just a few sentences, comment-out this code piece:\n",
    "```python\n",
    "#Input sentences from file\n",
    "file_path = \"sentences.txt\"\n",
    "with open(file_path,'r',encoding='utf-8') as f:\n",
    "         sentences = [[sent] for sent in f.read().splitlines()]\n",
    "```\n",
    "and then uncomment this code piece to input your sentences there:\n",
    "```python\n",
    "#Input sentences manually.\n",
    "sentences = [[\"Sentence 1\"],\n",
    "                [\"Sentence 2\"],\n",
    "                [\"Sentence 3\"],\n",
    "                [\"...\"]]\n",
    "```\n",
    "\n",
    "Can predict on sequences longer than 512 tokens.\n",
    "\n",
    "### Run\n",
    "Run the whole notebook after setting the variables\n",
    "\n",
    "### Output\n",
    "The output of the model with some extra information is contained in the DataFrame variable`df_sent_new`. Columns:\n",
    "* `Sentence`: Input sentence\n",
    "* `Start_Idx`: Start index of the sentence (== 0)\n",
    "* `End_Idx`: End index of the sentence (== length)\n",
    "* `Sentence_Tokenized`: Tokenized version of the sentence\n",
    "* `Token_Spans`: For each token, contains a tuple representing the token's span indexes (Start_Index, End_Index)\n",
    "* `Preds`:\tFor each token, contains the predicted tag.\n",
    "* `Probs`: For each token, contains the predicted probabilities for each token.\n",
    "* `Entities`: For each sentence, contains a list of named entities with following information:\n",
    "    * Mention\n",
    "    * Type\n",
    "    * Span\n",
    "\n",
    "## Library versions\n",
    "* transformers == 3.5.1\n",
    "* torch == 1.7.1 (+cu110)\n",
    "* pandas == 1.1.3\n",
    "* numpy == 1.19.2\n",
    "* scipy == 1.5.2\n",
    "* seqeval == 1.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import torch\n",
    "from seqeval.metrics.sequence_labeling import get_entities\n",
    "from NERFunctions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############INPUTS################\n",
    "\n",
    "#Batch size (decrease if you get memory erros)\n",
    "batch_size =16\n",
    "#'cuda' for gpu 'cpu' for cpu\n",
    "device_str = 'cuda'\n",
    "#path to model\n",
    "path_to_model = \"bert_sc_ner.pt\"\n",
    "#Input sentences from file\n",
    "file_path = \"sentences.txt\"\n",
    "with open(file_path,'r',encoding='utf-8') as f:\n",
    "    sentences = [[sent] for sent in f.read().splitlines()]\n",
    "\n",
    "\n",
    "#Input sentences manually.\n",
    "#sentences = [[\"Sentence 1\"],\n",
    "#             [\"Sentence 2\"],\n",
    "#             [\"Sentence 3\"],\n",
    "#             [\"...\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############VARIABLES################\n",
    "device = torch.device(device_str)\n",
    "#Path to NER model\n",
    "model = torch.load(path_to_model)\n",
    "#Load tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
    "#Define tag ids\n",
    "id2tag = {0: 'I-GRT', 1: 'O', 2: 'B-GRT', 3: 'B-ORG', 4: 'I-ORG'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########PROCESS DATASET##########\n",
    "#Create a dataframe with the input sentences.\n",
    "df_sent = pd.DataFrame(sentences,columns=['Sentence'])\n",
    "df_sent['Start_Idx'] = 0\n",
    "df_sent['End_Idx'] = [len(x) for x in df_sent.Sentence.values]\n",
    "df_sent['ID'] = None\n",
    "\n",
    "#This variable will contain the dataset with long sentences\n",
    "df_sent_withlong = df_sent.copy(deep=True)\n",
    "\n",
    "#Add BERT tokenization to the dataset to prepare input\n",
    "res = tokenize_input_bert(df_sent_withlong,'Sentence',tokenizer)\n",
    "df_sent_withlong['Sentence_Tokenized'] = res[0]\n",
    "df_sent_withlong['Token_Spans'] = res[1]\n",
    "#Token encodings does not have [CLS] an [SEP at the moment]\n",
    "df_sent_withlong['Token_Encoding'] = res[2]\n",
    "\n",
    "\n",
    "#Define max token length (512-2=510)\n",
    "#Split big sentences\n",
    "#df_sent -> big sentences splitted\n",
    "#df_sent_withlong -> original\n",
    "max_len = 510\n",
    "df_sent, too_long_df_sent =split_long_sentences_old(df_sent_withlong,max_len)\n",
    "\n",
    "#Prepare \"df_sent\" for the model\n",
    "max_len = 512\n",
    "df_sent_texts = df_sent['Sentence_Tokenized'].values\n",
    "df_sent_encodings = df_sent['Token_Encoding'].values\n",
    "df_sent_seq_lens = df_sent['Token_Encoding'].apply(lambda x: len(x)+2)\n",
    "#This is to distinguish the wordpieces\n",
    "df_sent_labels = get_labels(df_sent)\n",
    "df_sent_labels = add_and_pad(df_sent_labels,max_len,-1,-1,-1)\n",
    "df_sent_encodings = add_and_pad(df_sent_encodings,max_len,101,102,0)\n",
    "df_sent_attention_mask = [[0 if num==0 else 1 for num in lst]  for lst in df_sent_encodings]\n",
    "\n",
    "#Create datasets\n",
    "sent_dataset = FB_Dataset(df_sent_encodings, df_sent_labels,df_sent_attention_mask,df_sent_seq_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########PREDICT##########\n",
    "model.eval()\n",
    "model.to(device)\n",
    "data_loader = DataLoader(sent_dataset, batch_size=batch_size, shuffle=False)\n",
    "print('Getting Predictions...')\n",
    "with torch.no_grad():\n",
    "    preds = np.zeros((0,512,5))\n",
    "    #Loop over minibatches\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        print(i,\"/\",len(data_loader))\n",
    "        #Get the max length in this batch and crop based on that\n",
    "        seq_lens = batch['seq_len']\n",
    "        max_len_for_batch = max(seq_lens.cpu().detach().numpy())\n",
    "        #Get inputs and labels for that batch and crop\n",
    "        input_ids_ = torch.tensor(batch['input_ids'][:,:max_len_for_batch].detach().numpy()).to(device)\n",
    "        attention_mask_ = torch.tensor(batch['attention_mask'][:,:max_len_for_batch].detach().numpy()).to(device)\n",
    "        #Do a forward pass\n",
    "        outputs = model(input_ids_, attention_mask=attention_mask_)\n",
    "        #Save the predictions\n",
    "        these_preds = outputs[0].cpu().detach().numpy()\n",
    "        #Pad the predictions again\n",
    "        new_preds = np.ones((len(input_ids_),512,5)) * -100\n",
    "        new_preds[:,:max_len_for_batch,:] = these_preds\n",
    "        preds = np.concatenate([preds,new_preds],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############PREPROCESS THE OUTPUT################\n",
    "taglist = ['I-GRT','O','B-GRT', 'B-ORG',  'I-ORG']\n",
    "#Get softmax of the logits\n",
    "preds=softmax(preds,axis=2)\n",
    "#Get predicted label index\n",
    "preds_labels = np.argmax(preds,axis=2)\n",
    "#Get the probability of the predicted label\n",
    "preds_probs = []\n",
    "for row in preds:\n",
    "    new_row =[]\n",
    "    for tok in row:\n",
    "        new_row.append(dict(zip(taglist,np.round(tok,4))))\n",
    "    preds_probs.append(new_row)\n",
    "#Get predicted label and discard special tokens\n",
    "preds_tagged = []\n",
    "preds_probs_tagged = []\n",
    "for i in range(len(df_sent_labels)):\n",
    "    lbl = df_sent_labels[i]\n",
    "    preds_ = preds_labels[i]\n",
    "    probs_ = preds_probs[i]\n",
    "    new_preds = []\n",
    "    new_probs = []\n",
    "    for j in range(len(lbl)):\n",
    "        enc = lbl[j]\n",
    "        pred = preds_[j]\n",
    "        prob = probs_[j]\n",
    "        if enc != -1:\n",
    "            new_preds.append(id2tag[pred])\n",
    "            new_probs.append(prob)\n",
    "    preds_tagged.append(new_preds)\n",
    "    preds_probs_tagged.append(new_probs)\n",
    "df_sent['Preds'] = preds_tagged\n",
    "df_sent['Probs'] = preds_probs_tagged\n",
    "\n",
    "\n",
    "#Part of validation without the split sentences\n",
    "df_sent_ok = df_sent[df_sent.index<(len(df_sent_withlong)-len(too_long_df_sent))].copy(deep=True)\n",
    "#Part of validation with the split sentences\n",
    "df_sent_merge = df_sent[df_sent.index>=(len(df_sent_withlong)-len(too_long_df_sent))]\n",
    "\n",
    "#Get the merged predictions for the split sentences\n",
    "preds = df_sent_merge.groupby('ID').Preds.apply(sum)\n",
    "probs = df_sent_merge.groupby('ID').Probs.apply(sum)\n",
    "\n",
    "#Extract the part that we will paste (these are the long sentences)\n",
    "to_be_pasted = df_sent_withlong[df_sent_withlong.index.isin(too_long_df_sent)].copy(deep=True)\n",
    "\n",
    "#Append predictions for the long sentences\n",
    "new_preds = []\n",
    "new_probs = []\n",
    "for index, row in to_be_pasted.iterrows():\n",
    "    new_preds.append(preds[index])\n",
    "    new_probs.append(probs[index])\n",
    "to_be_pasted['Preds'] = new_preds\n",
    "to_be_pasted['Probs'] = new_probs\n",
    "\n",
    "#Construct the new validation set by merging them\n",
    "df_sent_new = pd.concat([df_sent_ok,to_be_pasted])\n",
    "\n",
    "#Make sure we did not miss anything\n",
    "print(df_sent_new.shape[0] == df_sent_withlong.shape[0])\n",
    "\n",
    "df_sent_new.reset_index(drop=True,inplace=True)\n",
    "\n",
    "df_sent_new.drop(['Token_Encoding','ID'],axis=1,inplace=True)\n",
    "\n",
    "#Get the tokenized words (wordpieces alltogether)\n",
    "res = tokenize_with_bert(df_sent_new,tokenizer)\n",
    "df_sent_new['Sentence_Tokenized'] = res[0]\n",
    "df_sent_new['Token_Spans'] = res[1]\n",
    "\n",
    "##### Extract Named Entities with Spans ####\n",
    "entities = []\n",
    "for index, row in df_sent_new.iterrows():\n",
    "    these_entities = []\n",
    "    extracted= get_entities(row['Preds'])\n",
    "    for item in extracted:\n",
    "        item_dict = dict()\n",
    "        item_dict['Mention'] = row['Sentence'][int(row['Token_Spans'][item[1]][0]):int(row['Token_Spans'][item[2]][1])]\n",
    "        item_dict['Type'] = item[0]\n",
    "        item_dict['Character_Spans'] = (row['Token_Spans'][item[1]][0],row['Token_Spans'][item[2]][1])\n",
    "        item_dict['Token_Spans'] = (item[1],item[2]+1)\n",
    "        these_entities.append(item_dict)\n",
    "    entities.append(these_entities)\n",
    "df_sent_new['Entities'] = entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sent_new.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
