{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############LIBRARIES################\n",
    "import sys\n",
    "sys.path.append(\"..\\\\..\\\\CreateNERSilverSet_NewPipelineOutput\")\n",
    "sys.path.append(\"..\\\\..\\\\PrepareBERT_forDatabricks\")\n",
    "\n",
    "from NERSilverSetFromSentences import *\n",
    "from GrantAndOrganizationMentionsFromGoldSet import grant_and_organization_mentions_from_gold_set\n",
    "from AnnotateForBert import * \n",
    "\n",
    "import pickle\n",
    "from transformers import BertTokenizerFast\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aydxng\\Anaconda3\\envs\\Python37\\lib\\site-packages\\urllib3\\connectionpool.py:988: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "C:\\Users\\aydxng\\Anaconda3\\envs\\Python37\\lib\\site-packages\\urllib3\\connectionpool.py:988: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    }
   ],
   "source": [
    "############VARIABLES################\n",
    "#Load tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
    "#Define tag ids\n",
    "id2tag = {0: 'I_GRT', 1: 'O', 2: 'B_GRT', 3: 'B_ORG', 4: 'I_ORG'}\n",
    "tag2id = {'I_GRT': 0, 'O': 1, 'B_GRT': 2, 'B_ORG': 3, 'I_ORG': 4, -100:-100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############FUNCTIONS################\n",
    "\n",
    "#Add [CLS] and [SEP] tokens, pad until \"pad_len\" chars.\n",
    "def add_and_pad(lst,pad_len,cls,sep,pad):\n",
    "    new_lst = []\n",
    "    for item in lst:\n",
    "        new_item = [cls] + item + [sep]\n",
    "        while len(new_item) != pad_len:\n",
    "            new_item.append(pad)\n",
    "        new_lst.append(new_item)\n",
    "    return new_lst\n",
    "\n",
    "#Class for funding bodies dataset\n",
    "class FB_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels,at_mask,seq_lens):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.at_mask = at_mask\n",
    "        self.seq_lens = seq_lens\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = dict()\n",
    "        item['input_ids'] = torch.tensor(self.encodings[idx])\n",
    "        item['attention_mask'] = torch.tensor(self.at_mask[idx])\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        item['seq_len'] =self.seq_lens[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######READ THE DATA##########\n",
    "path = \"C:\\\\Users\\\\aydxng\\\\Documents\\\\ds-fundingbodies-linkingcomponent-masterthesis\\\\Thesis\\\\CreateNERSilverSet_NewPipelineOutput\\\\\"\n",
    "with open(path+'df_sent_divided.pkl','rb') as f:\n",
    "    df= pickle.load(f)\n",
    "    \n",
    "#Discard the samples in \"Validation\" folder\n",
    "folders_validation = ['Validation\\\\Round_2','Validation\\\\Round_3','Validation\\\\Round_6','Validation\\\\Round_7','Validation\\\\Round_9',\n",
    "                      'Validation\\\\Round_11','Validation\\\\Round_13','Validation\\\\Round_17','Validation\\\\Round_18','Validation\\\\Round_19',\n",
    "                      'Validation\\\\Round_20','Validation\\\\Round_22','Validation\\\\Round_23','Validation\\\\Round_25','Validation\\\\Round_27',\n",
    "                      'Validation\\\\Round_28','Validation\\\\Round_30','Validation\\\\Round_32']\n",
    "df = df[~df.Folder.isin(folders_validation)].copy(deep=True)\n",
    "\n",
    "#Split train/valid/test\n",
    "train = df[df.Dataset=='Training'].reset_index(drop=True).copy(deep=True)\n",
    "valid = df[df.Dataset=='Validation'].reset_index(drop=True).copy(deep=True)\n",
    "\n",
    "#Drop not-needed columns\n",
    "train.drop(['Dataset','Pipeline_Span_Tags_IOB','Pipeline_Span_Tags_GRT_IOB','Pipeline_Span_Tags_ORG_IOB'],\n",
    "       axis=1, inplace=True)\n",
    "valid.drop(['Dataset','Pipeline_Span_Tags_IOB','Pipeline_Span_Tags_GRT_IOB','Pipeline_Span_Tags_ORG_IOB'],\n",
    "       axis=1, inplace=True)\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in file:  00812K_622654249.ann\n",
      "Error in file:  00901K_618203602.ann\n",
      "Error in file:  07147K_618282364.ann\n",
      "Error in file:  09209K_358501993.ann\n",
      "Error in file:  06041T_618636215.ann\n",
      "Error in file:  06821T_620696511.ann\n",
      "Error in file:  07347T_621231937.ann\n",
      "Error in file:  07451R_605779630.ann\n",
      "Error in file:  07964R_355725573.ann\n",
      "Error in file:  09112R_619440406.ann\n",
      "Error in file:  16263R_359218191.ann\n",
      "Error in file:  17677R_354095974.ann\n",
      "Error in file:  17677R_354095974.ann\n",
      "Error in file:  S0141933115000630.ann\n",
      "Error in file:  S0141933115000630.ann\n",
      "Error in file:  S0141933115000630.ann\n",
      "Error in file:  S0165011412002217.ann\n",
      "Error in file:  S0166093415003067.ann\n",
      "Error in file:  S0166218X1400016X.ann\n",
      "Error in file:  S0168192316303598.ann\n",
      "Error in file:  S0168192316303598.ann\n",
      "Error in file:  S0169260714002946.ann\n",
      "Error in file:  S0267364914001277.ann\n",
      "Error in file:  S0268005X16300492.ann\n",
      "Error in file:  S0301679X1500242X.ann\n",
      "Error in file:  S0304397511003926.ann\n",
      "Error in file:  S0304397512002356.ann\n",
      "Error in file:  S0304401716304034.ann\n",
      "Error in file:  S0304401716304034.ann\n",
      "Error in file:  S0304423813004317.ann\n",
      "Error in file:  S0370269310009767.ann\n",
      "Error in file:  S0370269310009767.ann\n",
      "Error in file:  S0370269310009767.ann\n",
      "Error in file:  S0380133005702409.ann\n",
      "Error in file:  S0723202014001064.ann\n",
      "Error in file:  S0747717111002021.ann\n",
      "Error in file:  S089360801400046X.ann\n",
      "Error in file:  S0893965914000068.ann\n",
      "Error in file:  S0893965914000068.ann\n",
      "Error in file:  S0896841111001223.ann\n",
      "Error in file:  S0925400513014391.ann\n",
      "Error in file:  S0925400514012507.ann\n",
      "Error in file:  S0960852415004009.ann\n",
      "Error in file:  S0960852415004009.ann\n",
      "Error in file:  S0960852415004009.ann\n",
      "Error in file:  S1000936115000266.ann\n",
      "Error in file:  S100207050860014X.ann\n",
      "Error in file:  S1874569506800163.ann\n",
      "Error in file:  0094-32265685.ann\n",
      "Error in file:  0739-51911113.ann\n",
      "Error in file:  0739-51911113.ann\n",
      "Error in file:  0736-51465135.ann\n",
      "Error in file:  0736-51465135.ann\n",
      "Error in file:  0736-51465135.ann\n",
      "Error in file:  0736-51465135.ann\n",
      "Error in file:  0736-51465135.ann\n",
      "Error in file:  0736-51465135.ann\n",
      "Error in file:  0736-51465135.ann\n"
     ]
    }
   ],
   "source": [
    "######GET GOLD NAMED ENTITIES##########\n",
    "path_to_gold = \"C:\\\\Users\\\\aydxng\\\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\\\Desktop\\\\ElsevierData\\\\Dataset2020July\\\\Dataset2020July\\\\\"\n",
    "folders_kpi = ['GoldSet_2019\\\\KPI\\\\']\n",
    "folders_trainin_train = ['GoldSet_2019\\\\Trainin-Train\\\\']\n",
    "folders_trainin_validation = ['GoldSet_2019\\\\Trainin-Validation\\\\']\n",
    "folders_trainin_test = ['GoldSet_2019\\\\Trainin-Test\\\\']\n",
    "folders_regression = ['GoldSet_2019\\\\Regression\\\\']\n",
    "folders_improvement = ['Improvement\\\\Round_1\\\\','Improvement\\\\Round_10\\\\','Improvement\\\\Round_12\\\\','Improvement\\\\Round_14\\\\','Improvement\\\\Round_15\\\\','Improvement\\\\Round_21\\\\','Improvement\\\\Round_24\\\\','Improvement\\\\Round_26\\\\','Improvement\\\\Round_29\\\\','Improvement\\\\Round_31\\\\','Improvement\\\\Round_4\\\\','Improvement\\\\Round_5\\\\','Improvement\\\\Round_8\\\\' ]\n",
    "#Gold named entities per folder\n",
    "org_dict_kpi, grant_dict_kpi = grant_and_organization_mentions_from_gold_set([path_to_gold+x for x in folders_kpi])\n",
    "org_dict_trainin_train, grant_dict_trainin_train = grant_and_organization_mentions_from_gold_set([path_to_gold+x for x in folders_trainin_train])\n",
    "org_dict_trainin_validation, grant_dict_trainin_validation = grant_and_organization_mentions_from_gold_set([path_to_gold+x for x in folders_trainin_validation])\n",
    "org_dict_trainin_test, grant_dict_trainin_test = grant_and_organization_mentions_from_gold_set([path_to_gold+x for x in folders_trainin_test])\n",
    "org_dict_regression, grant_dict_regression = grant_and_organization_mentions_from_gold_set([path_to_gold+x for x in folders_regression])\n",
    "org_dict_improvement, grant_dict_improvement = grant_and_organization_mentions_from_gold_set([path_to_gold+x for x in folders_improvement])\n",
    "org_dict_gold = dict()\n",
    "grant_dict_gold = dict()\n",
    "org_dict_gold.update(org_dict_kpi)\n",
    "grant_dict_gold.update(grant_dict_kpi)\n",
    "org_dict_gold.update(org_dict_trainin_train)\n",
    "grant_dict_gold.update(grant_dict_trainin_train)\n",
    "org_dict_gold.update(org_dict_trainin_validation)\n",
    "grant_dict_gold.update(grant_dict_trainin_validation)\n",
    "org_dict_gold.update(org_dict_trainin_test)\n",
    "grant_dict_gold.update(grant_dict_trainin_test)\n",
    "org_dict_gold.update(org_dict_regression)\n",
    "grant_dict_gold.update(grant_dict_regression)\n",
    "org_dict_gold.update(org_dict_improvement)\n",
    "grant_dict_gold.update(grant_dict_improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########PROCESS TRAINING SET##########\n",
    "#Add BERT tokenization\n",
    "res = tokenize_input_bert(train,'Sentence',tokenizer)\n",
    "train['Sentence_Tokenized'] = res[0]\n",
    "train['Token_Spans'] = res[1]\n",
    "#Token encodings does not have [CLS] an [SEP at the moment]\n",
    "train['Token_Encoding'] = res[2]\n",
    "\n",
    "#Get Gold span tags for organizations\n",
    "train['Gold_Span_Tags_ORG_IOB'] = get_span_tags_iob_bert(train,org_dict_gold,\"\",\"Token_Spans\")\n",
    "#Get Gold span tags for grants\n",
    "train['Gold_Span_Tags_GRT_IOB'] = get_span_tags_iob_bert(train,grant_dict_gold,\"\",\"Token_Spans\")\n",
    "#Fix the tags\n",
    "train['Gold_Span_Tags_ORG_IOB'] = fix_iob_tags(train['Gold_Span_Tags_ORG_IOB'].values)\n",
    "train['Gold_Span_Tags_GRT_IOB'] = fix_iob_tags(train['Gold_Span_Tags_GRT_IOB'].values)\n",
    "#Merge the tags\n",
    "train['Gold_Span_Tags_IOB'] = merge_grant_and_org_annotations_iob_bert(train,'Gold_Span_Tags_GRT_IOB','Gold_Span_Tags_ORG_IOB')\n",
    "\n",
    "#Define max token length (512-2=510)\n",
    "#Split big sentences\n",
    "max_len = 510\n",
    "train, too_long_train =split_long_sentences(train,max_len)\n",
    "\n",
    "#Separate a part for monitoring the training\n",
    "train_monitor_idx = np.random.choice(train[train.Folder=='GoldSet_2019\\\\KPI'].Filename.unique(),1000,replace=False)\n",
    "train_monitor = train[train.Filename.isin(train_monitor_idx)].reset_index(drop=True).copy(deep=True)\n",
    "train = train[~train.Filename.isin(train_monitor_idx)].reset_index(drop=True).copy(deep=True)\n",
    "\n",
    "max_len = 512\n",
    "\n",
    "#Get tokenized sentences\n",
    "train_texts = train['Sentence_Tokenized'].values\n",
    "train_monitor_texts = train_monitor['Sentence_Tokenized'].values\n",
    "\n",
    "#Get gold tags\n",
    "train_tags = train['Gold_Span_Tags_IOB'].values\n",
    "train_monitor_tags = train_monitor['Gold_Span_Tags_IOB'].values\n",
    "\n",
    "#Get the encoded values for each token\n",
    "train_encodings = train['Token_Encoding'].values\n",
    "train_monitor_encodings = train_monitor['Token_Encoding'].values\n",
    "\n",
    "#Get lengths of sequences\n",
    "train_seq_lens = train['Token_Encoding'].apply(lambda x: len(x)+2)\n",
    "train_monitor_seq_lens = train_monitor['Token_Encoding'].apply(lambda x: len(x)+2)\n",
    "\n",
    "#Pad the encodings and add [CLS] and [SEP] token\n",
    "train_encodings = add_and_pad(train_encodings,max_len,101,102,0)\n",
    "train_monitor_encodings = add_and_pad(train_monitor_encodings,max_len,101,102,0)\n",
    "\n",
    "#Set attention mask to 0 for padding\n",
    "train_attention_mask = [[0 if num==0 else 1 for num in lst]  for lst in train_encodings]\n",
    "train_monitor_attention_mask = [[0 if num==0 else 1 for num in lst]  for lst in train_monitor_encodings]\n",
    "\n",
    "#Get the labels, padded and special tokens added\n",
    "train_labels = add_and_pad(train_tags,max_len,-100,-100,-100)\n",
    "train_monitor_labels = add_and_pad(train_monitor_tags,max_len,-100,-100,-100)\n",
    "\n",
    "#Convert labels to ids\n",
    "train_labels = [[tag2id[x] for x in label]  for label in train_labels]\n",
    "train_monitor_labels = [[tag2id[x] for x in label]  for label in train_monitor_labels]\n",
    "\n",
    "#Create datasets\n",
    "train_dataset = FB_Dataset(train_encodings, train_labels,train_attention_mask,train_seq_lens)\n",
    "train_monitor_dataset = FB_Dataset(train_monitor_encodings, train_monitor_labels,train_monitor_attention_mask,train_monitor_seq_lens)\n",
    "\n",
    "#Write to file\n",
    "with open(\"bert_training_data.pkl\",\"wb\") as f:\n",
    "    pickle.dump(train_dataset,f)\n",
    "    pickle.dump(train_monitor_dataset,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########PROCESS VALIDATION SET##########\n",
    "#This variable will contain validation set with long sentences\n",
    "valid_withlong = valid.copy(deep=True)\n",
    "del valid\n",
    "\n",
    "#Add BERT tokenization to validation set\n",
    "res = tokenize_input_bert(valid_withlong,'Sentence',tokenizer)\n",
    "valid_withlong['Sentence_Tokenized'] = res[0]\n",
    "valid_withlong['Token_Spans'] = res[1]\n",
    "#Token encodings does not have [CLS] an [SEP at the moment]\n",
    "valid_withlong['Token_Encoding'] = res[2]\n",
    "\n",
    "#Define max token length (512-2=510)\n",
    "#Split big sentences\n",
    "#valid -> big sentences splitted\n",
    "#valid_withlong -> original\n",
    "max_len = 510\n",
    "valid, too_long_valid =split_long_sentences_old(valid_withlong,max_len)\n",
    "\n",
    "#Add tags to both of them\n",
    "#Get Gold span tags for organizations\n",
    "valid['Gold_Span_Tags_ORG_IOB'] = get_span_tags_iob_bert(valid,org_dict_gold,\"\",\"Token_Spans\")\n",
    "#Get Gold span tags for grants\n",
    "valid['Gold_Span_Tags_GRT_IOB'] = get_span_tags_iob_bert(valid,grant_dict_gold,\"\",\"Token_Spans\")\n",
    "#Fix the tags\n",
    "valid['Gold_Span_Tags_ORG_IOB'] = fix_iob_tags(valid['Gold_Span_Tags_ORG_IOB'].values)\n",
    "valid['Gold_Span_Tags_GRT_IOB'] = fix_iob_tags(valid['Gold_Span_Tags_GRT_IOB'].values)\n",
    "#Merge the tags\n",
    "valid['Gold_Span_Tags_IOB'] = merge_grant_and_org_annotations_iob_bert(valid,'Gold_Span_Tags_GRT_IOB','Gold_Span_Tags_ORG_IOB')\n",
    "#Get Gold span tags for organizations\n",
    "valid_withlong['Gold_Span_Tags_ORG_IOB'] = get_span_tags_iob_bert(valid_withlong,org_dict_gold,\"\",\"Token_Spans\")\n",
    "#Get Gold span tags for grants\n",
    "valid_withlong['Gold_Span_Tags_GRT_IOB'] = get_span_tags_iob_bert(valid_withlong,grant_dict_gold,\"\",\"Token_Spans\")\n",
    "#Fix the tags\n",
    "valid_withlong['Gold_Span_Tags_ORG_IOB'] = fix_iob_tags(valid_withlong['Gold_Span_Tags_ORG_IOB'].values)\n",
    "valid_withlong['Gold_Span_Tags_GRT_IOB'] = fix_iob_tags(valid_withlong['Gold_Span_Tags_GRT_IOB'].values)\n",
    "#Merge the tags\n",
    "valid_withlong['Gold_Span_Tags_IOB'] = merge_grant_and_org_annotations_iob_bert(valid_withlong,'Gold_Span_Tags_GRT_IOB','Gold_Span_Tags_ORG_IOB')\n",
    "\n",
    "#Prepare \"valid\" for the model\n",
    "max_len = 512\n",
    "valid_texts = valid['Sentence_Tokenized'].values\n",
    "valid_tags = valid['Gold_Span_Tags_IOB'].values\n",
    "valid_encodings = valid['Token_Encoding'].values\n",
    "valid_seq_lens = valid['Token_Encoding'].apply(lambda x: len(x)+2)\n",
    "valid_encodings = add_and_pad(valid_encodings,max_len,101,102,0)\n",
    "valid_attention_mask = [[0 if num==0 else 1 for num in lst]  for lst in valid_encodings]\n",
    "valid_labels = add_and_pad(valid_tags,max_len,-100,-100,-100)\n",
    "valid_labels = [[tag2id[x] for x in label]  for label in valid_labels]\n",
    "\n",
    "#Create datasets\n",
    "val_dataset = FB_Dataset(valid_encodings, valid_labels,valid_attention_mask,valid_seq_lens)\n",
    "\n",
    "#Write to file\n",
    "with open(\"bert_validation_data.pkl\",\"wb\") as f:\n",
    "    pickle.dump(val_dataset,f)\n",
    "    pickle.dump(valid_withlong,f)\n",
    "    pickle.dump(valid,f)\n",
    "    pickle.dump(too_long_valid,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
