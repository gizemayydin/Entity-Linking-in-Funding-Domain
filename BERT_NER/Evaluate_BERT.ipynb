{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############LIBRARIES################\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from datasets import load_metric\n",
    "from seqeval.scheme import IOB2\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############FUNCTIONS################\n",
    "#From the example GitHub Notebook\n",
    "def compute_metrics(p,id2tag):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [id2tag[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2tag[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    print(\"\\t\\tORG Precision: \",results['_ORG']['precision'])\n",
    "    print(\"\\t\\tORG Recall: \",results['_ORG']['recall'])\n",
    "    print(\"\\t\\tORG F1: \",results['_ORG']['f1'])\n",
    "    print(\"\\t\\tGRT Precision: \",results['_GRT']['precision'])\n",
    "    print(\"\\t\\tGRT Recall: \",results['_GRT']['recall'])\n",
    "    print(\"\\t\\tGRT F1: \",results['_GRT']['f1'])\n",
    "    \n",
    "    \n",
    "#Evaluate the model on the train_monitor set\n",
    "def eval_on_valid(model, train_monitor_loader,id2tag):\n",
    "    #Accumulate the predictions here\n",
    "    val_preds = np.zeros((0,512,5))\n",
    "    #Accumulate the labels here\n",
    "    val_lbls = np.zeros((0,512))\n",
    "    #Accumulate the oss here\n",
    "    val_loss = 0\n",
    "    #Loop over minibatches\n",
    "    for i_val, batch_val in enumerate(train_monitor_loader):\n",
    "        print(i_val,\"/\",len(train_monitor_loader))\n",
    "        #Get the max length in this batch and crop based on that\n",
    "        seq_lens = batch_val['seq_len']\n",
    "        max_len_for_batch = max(seq_lens.cpu().detach().numpy())\n",
    "        #Get inputs and labels for that batch and crop\n",
    "        input_ids_val = torch.tensor(batch_val['input_ids'][:,:max_len_for_batch].detach().numpy()).to(device)\n",
    "        attention_mask_val = torch.tensor(batch_val['attention_mask'][:,:max_len_for_batch].detach().numpy()).to(device)\n",
    "        labels_val = torch.tensor(batch_val['labels'][:,:max_len_for_batch].detach().numpy()).to(device)\n",
    "        #Do a forward pass\n",
    "        outputs_val = model(input_ids_val, attention_mask=attention_mask_val, labels=labels_val)\n",
    "        #First index is the loss. Since the output loss is the mean over minibatch samples,\n",
    "        #we multiply it with batch size. Later, we divide it by the number of samples\n",
    "        val_loss += outputs_val[0].item()\n",
    "        #Save the loss and labels\n",
    "        these_preds = outputs_val[1].cpu().detach().numpy()\n",
    "        these_labels= labels_val.cpu().detach().numpy()\n",
    "        #Pad the predictions again\n",
    "        new_preds = np.ones((len(input_ids_val),512,5)) * -100\n",
    "        new_labels= np.ones((len(input_ids_val),512)) * -100\n",
    "        new_preds[:,:max_len_for_batch,:] = these_preds\n",
    "        new_labels[:,:max_len_for_batch] = these_labels\n",
    "        #Store in array\n",
    "        val_preds = np.concatenate([val_preds,new_preds],axis=0)\n",
    "        val_lbls = np.concatenate([val_lbls,new_labels],axis=0)\n",
    "    print(\"\\tValidation Loss: \",val_loss/len(train_monitor_loader))\n",
    "    p = (val_preds, val_lbls)\n",
    "    print(\"\\tValidation Results: \")\n",
    "    compute_metrics(p,id2tag)\n",
    "    return val_preds\n",
    "\n",
    "\n",
    "#Class for funding bodies dataset\n",
    "class FB_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels,at_mask,seq_lens):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.at_mask = at_mask\n",
    "        self.seq_lens = seq_lens\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = dict()\n",
    "        item['input_ids'] = torch.tensor(self.encodings[idx])\n",
    "        item['attention_mask'] = torch.tensor(self.at_mask[idx])\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        item['seq_len'] =self.seq_lens[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############VARIABLES################\n",
    "id2tag = {0: 'I_GRT', 1: 'O', 2: 'B_GRT', 3: 'B_ORG', 4: 'I_ORG'}\n",
    "tag2id = {'I_GRT':0,  'O':1,  'B_GRT':2,  'B_ORG':3, 'I_ORG':4,-100:-100}\n",
    "with open(\"bert_validation_data.pkl\",\"rb\") as f:\n",
    "    val_dataset=pickle.load(f)\n",
    "    valid_withlong=pickle.load(f)\n",
    "    valid=pickle.load(f)\n",
    "    too_long_valid=pickle.load(f)\n",
    "#Load metric for evaluation\n",
    "metric = load_metric(\"seqeval\")\n",
    "#Load the model and put in evaluation mode\n",
    "model = torch.load(\"bert_epoch_1.pt\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 158\n",
      "1 / 158\n",
      "2 / 158\n",
      "3 / 158\n",
      "4 / 158\n",
      "5 / 158\n",
      "6 / 158\n",
      "7 / 158\n",
      "8 / 158\n",
      "9 / 158\n",
      "10 / 158\n",
      "11 / 158\n",
      "12 / 158\n",
      "13 / 158\n",
      "14 / 158\n",
      "15 / 158\n",
      "16 / 158\n",
      "17 / 158\n",
      "18 / 158\n",
      "19 / 158\n",
      "20 / 158\n",
      "21 / 158\n",
      "22 / 158\n",
      "23 / 158\n",
      "24 / 158\n",
      "25 / 158\n",
      "26 / 158\n",
      "27 / 158\n",
      "28 / 158\n",
      "29 / 158\n",
      "30 / 158\n",
      "31 / 158\n",
      "32 / 158\n",
      "33 / 158\n",
      "34 / 158\n",
      "35 / 158\n",
      "36 / 158\n",
      "37 / 158\n",
      "38 / 158\n",
      "39 / 158\n",
      "40 / 158\n",
      "41 / 158\n",
      "42 / 158\n",
      "43 / 158\n",
      "44 / 158\n",
      "45 / 158\n",
      "46 / 158\n",
      "47 / 158\n",
      "48 / 158\n",
      "49 / 158\n",
      "50 / 158\n",
      "51 / 158\n",
      "52 / 158\n",
      "53 / 158\n",
      "54 / 158\n",
      "55 / 158\n",
      "56 / 158\n",
      "57 / 158\n",
      "58 / 158\n",
      "59 / 158\n",
      "60 / 158\n",
      "61 / 158\n",
      "62 / 158\n",
      "63 / 158\n",
      "64 / 158\n",
      "65 / 158\n",
      "66 / 158\n",
      "67 / 158\n",
      "68 / 158\n",
      "69 / 158\n",
      "70 / 158\n",
      "71 / 158\n",
      "72 / 158\n",
      "73 / 158\n",
      "74 / 158\n",
      "75 / 158\n",
      "76 / 158\n",
      "77 / 158\n",
      "78 / 158\n",
      "79 / 158\n",
      "80 / 158\n",
      "81 / 158\n",
      "82 / 158\n",
      "83 / 158\n",
      "84 / 158\n",
      "85 / 158\n",
      "86 / 158\n",
      "87 / 158\n",
      "88 / 158\n",
      "89 / 158\n",
      "90 / 158\n",
      "91 / 158\n",
      "92 / 158\n",
      "93 / 158\n",
      "94 / 158\n",
      "95 / 158\n",
      "96 / 158\n",
      "97 / 158\n",
      "98 / 158\n",
      "99 / 158\n",
      "100 / 158\n",
      "101 / 158\n",
      "102 / 158\n",
      "103 / 158\n",
      "104 / 158\n",
      "105 / 158\n",
      "106 / 158\n",
      "107 / 158\n",
      "108 / 158\n",
      "109 / 158\n",
      "110 / 158\n",
      "111 / 158\n",
      "112 / 158\n",
      "113 / 158\n",
      "114 / 158\n",
      "115 / 158\n",
      "116 / 158\n",
      "117 / 158\n",
      "118 / 158\n",
      "119 / 158\n",
      "120 / 158\n",
      "121 / 158\n",
      "122 / 158\n",
      "123 / 158\n",
      "124 / 158\n",
      "125 / 158\n",
      "126 / 158\n",
      "127 / 158\n",
      "128 / 158\n",
      "129 / 158\n",
      "130 / 158\n",
      "131 / 158\n",
      "132 / 158\n",
      "133 / 158\n",
      "134 / 158\n",
      "135 / 158\n",
      "136 / 158\n",
      "137 / 158\n",
      "138 / 158\n",
      "139 / 158\n",
      "140 / 158\n",
      "141 / 158\n",
      "142 / 158\n",
      "143 / 158\n",
      "144 / 158\n",
      "145 / 158\n",
      "146 / 158\n",
      "147 / 158\n",
      "148 / 158\n",
      "149 / 158\n",
      "150 / 158\n",
      "151 / 158\n",
      "152 / 158\n",
      "153 / 158\n",
      "154 / 158\n",
      "155 / 158\n",
      "156 / 158\n",
      "157 / 158\n",
      "\tValidation Loss:  0.17250497421084704\n",
      "\tValidation Results: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aydxng\\Anaconda3\\envs\\Python37\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_ORG seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\aydxng\\Anaconda3\\envs\\Python37\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: I_ORG seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\aydxng\\Anaconda3\\envs\\Python37\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_GRT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\aydxng\\Anaconda3\\envs\\Python37\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: I_GRT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tORG Precision:  0.7845483005366727\n",
      "\t\tORG Recall:  0.8579812924130342\n",
      "\t\tORG F1:  0.819623302671923\n",
      "\t\tGRT Precision:  0.9459433509361498\n",
      "\t\tGRT Recall:  0.9742879746835443\n",
      "\t\tGRT F1:  0.9599064646563064\n"
     ]
    }
   ],
   "source": [
    "############RUN THE MODEL################\n",
    "device = torch.device('cuda')\n",
    "\n",
    "#Put model to device\n",
    "model.to(device)\n",
    "\n",
    "#Set batch size\n",
    "batch_size=32\n",
    "\n",
    "#Initialize the valid loader without shuffling\n",
    "valid_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#Get the predictions and preliminary results\n",
    "with torch.no_grad():\n",
    "    val_preds = eval_on_valid(model, valid_loader,id2tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "############PREPROCESS THE OUTPUT################\n",
    "\n",
    "#Get predicted label index\n",
    "val_preds2 = np.argmax(val_preds,axis=2)\n",
    "#Get the labels\n",
    "valid_labels = val_dataset.labels\n",
    "#Get predicted label and discard -100 tags\n",
    "val_preds_tagged = []\n",
    "for i in range(len(valid_labels)):\n",
    "    lbls = valid_labels[i]\n",
    "    preds = val_preds2[i]\n",
    "    new_preds = []\n",
    "    for j in range(len(lbls)):\n",
    "        lbl = lbls[j]\n",
    "        pred = preds[j]\n",
    "        if lbl != -100:\n",
    "            new_preds.append(id2tag[pred])\n",
    "    val_preds_tagged.append(new_preds)\n",
    "valid['Preds'] = val_preds_tagged\n",
    "\n",
    "\n",
    "#Part of validation without the split sentences\n",
    "valid_ok = valid[valid.index<(len(valid_withlong)-len(too_long_valid))].copy(deep=True)\n",
    "#Part of validation with the split sentences\n",
    "valid_merge = valid[valid.index>=(len(valid_withlong)-len(too_long_valid))]\n",
    "\n",
    "#Get the merged predictions for the split sentences\n",
    "preds = valid_merge.groupby('ID').Preds.apply(sum)\n",
    "\n",
    "#Extract the part that we will paste (these are the long sentences)\n",
    "to_be_pasted = valid_withlong[valid_withlong.index.isin(too_long_valid)].copy(deep=True)\n",
    "\n",
    "#Append predictions for the long sentences\n",
    "new_preds = []\n",
    "for index, row in to_be_pasted.iterrows():\n",
    "    new_preds.append(preds[index])\n",
    "to_be_pasted['Preds'] = new_preds\n",
    "\n",
    "#Construct the new validation set by merging them\n",
    "valid_new = pd.concat([valid_ok,to_be_pasted])\n",
    "\n",
    "#Make sure we did not miss anything\n",
    "print(valid_new.shape[0] == valid_withlong.shape[0])\n",
    "\n",
    "#Get gold labels wihout the -100\n",
    "new_gold = []\n",
    "for index, row in valid_new.iterrows():\n",
    "    new_gold.append([x for x in row['Gold_Span_Tags_IOB'] if x!=-100])\n",
    "valid_new['Gold'] = new_gold\n",
    "\n",
    "valid_new.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aydxng\\Anaconda3\\envs\\Python37\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_ORG seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\aydxng\\Anaconda3\\envs\\Python37\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: I_ORG seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\aydxng\\Anaconda3\\envs\\Python37\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_GRT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\aydxng\\Anaconda3\\envs\\Python37\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: I_GRT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        _GRT    0.94594   0.97429   0.95991     10112\n",
      "        _ORG    0.78444   0.85796   0.81955     16355\n",
      "\n",
      "   micro avg    0.84387   0.90241   0.87216     26467\n",
      "   macro avg    0.86519   0.91613   0.88973     26467\n",
      "weighted avg    0.84614   0.90241   0.87318     26467\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#####GET THE RESULTS########\n",
    "print(classification_report(list(valid_new.Gold.values),list(valid_new.Preds.values),scheme=IOB2,\n",
    "                           digits=5,mode='default'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
