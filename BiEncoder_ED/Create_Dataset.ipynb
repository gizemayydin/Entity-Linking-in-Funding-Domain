{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import rdflib\n",
    "from glob import glob\n",
    "from pandas import DataFrame, notnull\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Gold Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileReader:\n",
    "    def __init__(self,dir_name,out_filename, multiple_folders):\n",
    "        self.ann_dict = dict()     #stores information  contained in .ann files\n",
    "        self.dir_name = dir_name   #name of the directory containing data\n",
    "        self.org_name = []              #columns of the table\n",
    "        self.org_entity_id = []         #columns of the table\n",
    "        self.org_found = []             #columns of the table\n",
    "        self.org_snippet = []           #columns of the table\n",
    "        self.org_offset_start = []      #columns of the table\n",
    "        self.org_offset_end = []        #columns of the table\n",
    "        self.org_folder = []            #columns of the table\n",
    "        self.org_filename = []          #columns of the table\n",
    "        self.mention_offset_start = []  #columns of the table\n",
    "        self.mention_offset_end = []    #columns of the table\n",
    "        self.is_acronym = []            #columns of the table\n",
    "        self.multiple_folders = multiple_folders #stores the multiple folders input\n",
    "        self.df = DataFrame()       #dataframe\n",
    "        \n",
    "        #Annotation information is obtained using different functions\n",
    "        #depending on the structure of the input directory.\n",
    "        if self.multiple_folders:\n",
    "            self.get_annotation_information_multiple_folders() #fill ann_dict with info from the files\n",
    "        else:\n",
    "            self.get_annotation_information()                  #fill ann_dict with info from the files\n",
    "        self.get_columns()                  #construct columns \n",
    "        self.create_df()                    #create dataframe\n",
    "        self.df.to_csv(out_filename)        #write output dataframe to csv\n",
    "         \n",
    "    def extract_values(self, list_line):\n",
    "        \"\"\"\n",
    "        Input: Line of annotation file as a list where line is splitted with whitespace\n",
    "        Output: Key,Value pair containing information of the line\n",
    "        Some annotation classes are not implemented yet.  \n",
    "        \"\"\"\n",
    "        annotation_type = list_line[1]\n",
    "        key = None\n",
    "        value = None\n",
    "        if annotation_type == \"Organization\" or annotation_type == 'Sponsor':\n",
    "            #key = ('Organization','annotation_row_id')\n",
    "            #value = [Offset_1,Offset_2,Organization_Name]\n",
    "            key = ('Organization',list_line[0])\n",
    "            #Workaround for annotations like \";\"\n",
    "            end_ind = 3\n",
    "            while \";\" in list_line[end_ind]:\n",
    "                end_ind += 1    \n",
    "            value = [int(list_line[2]),int(list_line[end_ind]),' '.join(map(str,list_line[(end_ind+1):]))]\n",
    "        \n",
    "        elif annotation_type == \"Organization-Acronym\" or annotation_type == 'Sponsor-Acronym':\n",
    "            #key = ('Organization-Acronym','annotation_row_id')\n",
    "            #value = [Offset_1,Offset_2,Organization_Acronym]\n",
    "            key = ('Organization-Acronym',list_line[0])\n",
    "            #Workaround for annotations like \";\"\n",
    "            end_ind = 3\n",
    "            while \";\" in list_line[end_ind]:\n",
    "                end_ind += 1    \n",
    "            value = [int(list_line[2]),int(list_line[end_ind]),' '.join(map(str,list_line[(end_ind+1):]))]\n",
    "\n",
    "        elif annotation_type == \"Found_In_FundRef\":\n",
    "            #key = ('Found_In_FundRef','annotation_row_id_of_organization')\n",
    "            #value = True/False\n",
    "            key = ('Found_In_FundRef',list_line[2])\n",
    "            value = True if list_line[3] == 'Yes' else False\n",
    "        elif annotation_type == \"AnnotatorNotes\":\n",
    "            #key = ('AnnotatorNotes','annotation_row_id_of_organization')\n",
    "            #value = Entity_id\n",
    "            key = ('AnnotatorNotes',list_line[2])\n",
    "            value = list_line[3]\n",
    "        elif annotation_type == \"Funding_Snippet\" :\n",
    "            #key = ('Funding_Snippet',annotation_row_id)\n",
    "            #value = [start_snippet_row_id,end_snippet_row_id]\n",
    "            key = ('Funding_Snippet',list_line[0])\n",
    "            value =[list_line[2][5:],list_line[3][5:]]\n",
    "        elif annotation_type == \"SnippetStart\" :\n",
    "            #key = ('SnippetStart','start_snippet_row_id')\n",
    "            #value = [Offset_1,Offset_2]\n",
    "            key = ('SnippetStart',list_line[0])\n",
    "            value =  [int(list_line[2]),int(list_line[3])]\n",
    "        elif annotation_type == \"SnippetEnd\" :\n",
    "            #key = ('SnippetEnd','end_snippet_row_id')\n",
    "            #value = [Offset_1,Offset_2]\n",
    "            key = ('SnippetEnd',list_line[0])\n",
    "            value =  [int(list_line[2]),int(list_line[3])]\n",
    "        elif annotation_type == \"No_FB_Found_In_This_Article\" :\n",
    "            #key = ('No_FB_Found_In_This_Article',None)\n",
    "            #value = None\n",
    "            key = ('No_FB_Found_In_This_Article',None)\n",
    "        elif annotation_type == \"AcronymOf\" :\n",
    "            #This is a bidirectional relationship. \n",
    "            #That is why it returnes two (key, value) pairs \n",
    "            #key = [('AcronymOf',acronym_row_id), ('AcronymOf',organization_row_id)]\n",
    "            #value = [organization_row_id, acronym_row_id]\n",
    "            key = [('AcronymOf',list_line[2][5:]), ('AcronymOf',list_line[3][5:])]\n",
    "            value = [list_line[3][5:], list_line[2][5:]]\n",
    "        return key, value\n",
    "    \n",
    "    def preprocess_snippet_info(self,ann_info):\n",
    "        \"\"\"Get offsets of funding snippets\"\"\"\n",
    "        funding_snippets = [v for k, v in ann_info.items() if k[0] == 'Funding_Snippet']\n",
    "        snippet_idxs = []\n",
    "        for funding_snippet in funding_snippets:\n",
    "            #Get first offset from SnippetStart to get the start point of the paragraph\n",
    "            start_idx = ann_info[('SnippetStart',funding_snippet[0])][0] \n",
    "            #Get second offset from SnippetEnd to get the start point of the paragraph\n",
    "            end_idx = ann_info[('SnippetEnd',funding_snippet[1])][1] \n",
    "            snippet_idxs.append((start_idx,end_idx))\n",
    "        return snippet_idxs\n",
    "    \n",
    "    def get_annotation_information_multiple_folders(self):\n",
    "        \"\"\"Fill ann_dict with information from the .ann files\"\"\"\n",
    "        #Loop over folders in data directory\n",
    "        for folder1 in glob(self.dir_name + \"*\"):\n",
    "            for folder in glob(folder1 + '\\\\*'):\n",
    "                #Loop over annotation files\n",
    "                for file_ in glob(folder +\"\\*.ann\"):\n",
    "                    #Get file id\n",
    "                    file_id = file_.replace(folder,\"\")[1:-4]\n",
    "                    #Initialize a dictionary for the file\n",
    "                    file_dict = dict()\n",
    "                    is_correct = True\n",
    "                    with open(file_, \"r\", encoding=\"utf8\") as f:\n",
    "                        #Process information on each line\n",
    "                        for line in f:\n",
    "                            key = None\n",
    "                            value = None\n",
    "                            try:\n",
    "                                key, value = self.extract_values(line.strip().split())\n",
    "                            except (ValueError, IndexError) as ve:\n",
    "                                print(\"Error on annotation file: \",file_, \"\\nError str: \",ve,\"\\n\\n\")\n",
    "                                #Discard this annotation file\n",
    "                                is_correct = False\n",
    "                                break\n",
    "                            #This if clause can be removed when all fields are used\n",
    "                            if key is not None:\n",
    "                                #Key,value of AcronymOf is a list\n",
    "                                if type(key) == list:\n",
    "                                    for i in range(2):\n",
    "                                        file_dict[key[i]] = value[i]\n",
    "                                else:\n",
    "                                    file_dict[key] = value\n",
    "                        #Add information from annotation file to dictionary if no errors\n",
    "                        if is_correct and len(file_dict) != 0:\n",
    "                            #Preprocess the folder name\n",
    "                            folder_processed = folder.split('\\\\')[-2:]\n",
    "                            folder_processed = folder_processed[0] + '\\\\' + folder_processed[1]\n",
    "                            self.ann_dict[(folder_processed,file_id)]=file_dict\n",
    "                            \n",
    "    def get_annotation_information(self):\n",
    "        \"\"\"Fill ann_dict with information from the .ann files\"\"\"\n",
    "        #Loop over folders in data directory\n",
    "        for folder in glob(self.dir_name + \"*\"):\n",
    "            #Loop over annotation files\n",
    "            for file_ in glob(folder +\"\\*.ann\"):\n",
    "                #Get file id\n",
    "                file_id = file_.replace(folder,\"\")[1:-4]\n",
    "                #Initialize a dictionary for the file\n",
    "                file_dict = dict()\n",
    "                is_correct = True\n",
    "                with open(file_, \"r\", encoding=\"utf8\") as f:\n",
    "                    #Process information on each line\n",
    "                    for line in f:\n",
    "                        key = None\n",
    "                        value = None\n",
    "                        try:\n",
    "                            key, value = self.extract_values(line.strip().split())\n",
    "                        except (ValueError, IndexError) as ve:\n",
    "                            print(\"Error on annotation file: \",file_, \"\\nError str: \",ve,\"\\n\\n\")\n",
    "                            #Discard this annotation file\n",
    "                            is_correct = False\n",
    "                            break\n",
    "                        #This if clause can be removed when all fields are used\n",
    "                        if key is not None:\n",
    "                            #Key,value of AcronymOf is a list\n",
    "                            if type(key) == list:\n",
    "                                for i in range(2):\n",
    "                                    file_dict[key[i]] = value[i]\n",
    "                            else:\n",
    "                                file_dict[key] = value\n",
    "                    #Add information from annotation file to dictionary if no errors\n",
    "                    if is_correct and len(file_dict) != 0:\n",
    "                        folder_processed = folder.split('\\\\')[-1]\n",
    "                        self.ann_dict[(folder_processed,file_id)]=file_dict\n",
    "                                \n",
    "    def get_columns(self):\n",
    "        \"\"\"Construct the columns using the information in ann_dict\"\"\"\n",
    "        #Loop over information extracted on each annotation file\n",
    "        for file_key, ann_info in self.ann_dict.items(): \n",
    "            #Get snippets from the annotation file\n",
    "            try:\n",
    "                snippet_idxs = self.preprocess_snippet_info(ann_info)\n",
    "            except KeyError as ke:\n",
    "                print(\"KeyError on annotation file: \" + file_key[0]+\"\\\\\"+file_key[1]+\".ann\\nError str: \",ke,\"\\n\\n\")\n",
    "                #Skip this iteration as there is an error\n",
    "                continue\n",
    "            #Get organization row ids\n",
    "            org_ids = [(k[1],'org') for k in ann_info.keys() if k[0] == 'Organization']\n",
    "            org_ids = org_ids + [(k[1],'acr') for k in ann_info.keys() if k[0] == 'Organization-Acronym']\n",
    "            #For each organization in the annotation file\n",
    "            for org_id in org_ids:\n",
    "                #Organization name and offsets\n",
    "                org_info = None\n",
    "                #If it is an acronym\n",
    "                if org_id[1] == 'acr':\n",
    "                    org_info = ann_info.get(('Organization-Acronym',org_id[0]))\n",
    "                    self.is_acronym.append(True)\n",
    "                #If it is not an acronym\n",
    "                else:\n",
    "                    org_info = ann_info.get(('Organization',org_id[0]))\n",
    "                    self.is_acronym.append(False)\n",
    "                org_id = org_id[0]\n",
    "                self.org_name.append(org_info[2])\n",
    "                self.mention_offset_start.append(org_info[0])\n",
    "                self.mention_offset_end.append(org_info[1])\n",
    "                #Get related snippet information\n",
    "                found = False\n",
    "                for snippet_idx in snippet_idxs:\n",
    "                    #If this snippet includes this organization's information\n",
    "                    if snippet_idx[0] <= org_info[0] and snippet_idx[1] >= org_info[1] and not found:\n",
    "                        try:\n",
    "                            #extract snippet from txt file\n",
    "                            text_file_path = ''\n",
    "                            if self.multiple_folders:\n",
    "                                text_file_path = self.dir_name + '\\\\' + file_key[0].split('\\\\')[-1]+\"\\\\\"+file_key[1]+\".txt\"\n",
    "                            else:\n",
    "                                text_file_path = self.dir_name+\"\\\\\"+file_key[1]+\".txt\"\n",
    "                            with open(text_file_path, \"r\", encoding=\"utf8\") as f:\n",
    "                                self.org_snippet.append(f.read()[snippet_idx[0]:snippet_idx[1]])\n",
    "                            #get the relative offset\n",
    "                            self.org_offset_start.append(org_info[0]-snippet_idx[0])\n",
    "                            self.org_offset_end.append(org_info[1]-snippet_idx[0])\n",
    "                            found = True\n",
    "                        except FileNotFoundError as e:\n",
    "                            print(\"FileNotFoundError on text file: \" + text_file_path  +'\\nError str: ',e,\"\\n\\n\")\n",
    "                #If there is no snippet information\n",
    "                if not found: \n",
    "                    self.org_snippet.append(None)\n",
    "                    self.org_offset_start.append(None)\n",
    "                    self.org_offset_end.append(None)\n",
    "                #Find entity_id and whether the annotator found this org in taxonomy\n",
    "                #Check if this organization is linked to an entity\n",
    "                ann_notes = ann_info.get(('AnnotatorNotes',org_id),None)\n",
    "                #If it is not linked to an entity\n",
    "                if ann_notes is None:\n",
    "                    #Check if it is linked to an organization/acronym with the \n",
    "                    #AcronymOf relation\n",
    "                    other_id = ann_info.get(('AcronymOf',org_id),None)\n",
    "                    #If it is linked to an organization/acronym \n",
    "                    #append its linking as the linking of organization/acronym\n",
    "                    if other_id is not None:\n",
    "                        self.org_entity_id.append(ann_info.get(('AnnotatorNotes',other_id ),None))\n",
    "                        self.org_found.append(ann_info.get(('Found_In_FundRef',other_id ),None))\n",
    "                    #If not this is a NIL linking\n",
    "                    else:\n",
    "                        self.org_entity_id.append(None)\n",
    "                        self.org_found.append(ann_info.get(('Found_In_FundRef',org_id),None))\n",
    "                #If it is linked to an entity, we do not need to check the\n",
    "                #AcronymOf relationship\n",
    "                else:\n",
    "                    self.org_entity_id.append(ann_info.get(('AnnotatorNotes',org_id),None))\n",
    "                    self.org_found.append(ann_info.get(('Found_In_FundRef',org_id),None))\n",
    "                #Add information on which file and folder this is in\n",
    "                self.org_folder.append(file_key[0])\n",
    "                self.org_filename.append(file_key[1])\n",
    "                \n",
    "    def create_df(self):\n",
    "        \"\"\"Create the dataframe using the columns\"\"\"\n",
    "        #Replace NaN's with None's \n",
    "        #When pandas infers data type to be numeric, it converts None's to NaN's\n",
    "        self.df = DataFrame(list(zip(self.org_folder,self.org_filename,self.org_name, self.org_entity_id, self.is_acronym,\n",
    "                                     self.org_found,self.org_snippet,self.org_offset_start,self.org_offset_end,\n",
    "                                     self.mention_offset_start, self.mention_offset_end)),\n",
    "                            columns =[\"Folder\",\"Filename\",\"Organization\",\"Entity_ID\",\"Is_Acronym\",\"Found_in_FundRef\",\"Snippet\",\n",
    "                            \"Rel_Offset_Start\",\"Rel_Offset_End\", 'Mention_Offset_Start','Mention_Offset_End'])\n",
    "        self.df = self.df.where(notnull(self.df), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\Regression\\02022R_619867865.ann \n",
      "Error str:  invalid literal for int() with base 10: '2810;2811' \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\Regression\\02034R_622431326.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\Regression\\02535R_622459459.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\Regression\\02782R_622758516.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\Regression\\04867R_624632133.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\Regression\\06923R_52914528.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\Regression\\06944R_602147495.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\Regression\\07278R_369457453.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\Regression\\08064R_362656044.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\Regression\\08423R_618804492.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\Regression\\10100R_52908283.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\Regression\\10955R_625823985.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\Regression\\11380R_623409302.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\Regression\\11693R_52414618.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\Regression\\13026R_621683576.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\Regression\\18464R_622941330.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "KeyError on annotation file: Regression\\00553R_372903938.ann\n",
      "Error str:  ('SnippetEnd', 'TVelayutham2') \n",
      "\n",
      "\n",
      "KeyError on annotation file: Regression\\07203R_372697116.ann\n",
      "Error str:  ('SnippetEnd', 'TC11') \n",
      "\n",
      "\n",
      "KeyError on annotation file: Regression\\13807R_370489929.ann\n",
      "Error str:  ('SnippetEnd', 'TSachdev7') \n",
      "\n",
      "\n",
      "Number of samples:  56399\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\KPI\\00260K_362883563.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\KPI\\00807K_610052994.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\KPI\\01870K_624362527.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\KPI\\02288K_2001593884.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\KPI\\02485K_622973820.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\KPI\\03012K_369035977.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\KPI\\03241K_52830064.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\KPI\\05654K_626016435.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\KPI\\07621K_626659469.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\KPI\\09134K_368876583.ann \n",
      "Error str:  invalid literal for int() with base 10: '12777;12819' \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\KPI\\09297K_50972681.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\KPI_1k_gold\\00260K_362883563.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\KPI_1k_gold\\00807K_610052994.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Number of samples:  52948\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\Trainin-Train\\12184T_623705032.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\Trainin-Train\\13616T_619081488.ann \n",
      "Error str:  invalid literal for int() with base 10: '37277;37278' \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\Trainin-Train\\15572T_362205645.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\Trainin-Train\\15774T_358417932.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\Trainin-Train\\16705T_369201784.ann \n",
      "Error str:  invalid literal for int() with base 10: '34155;34156' \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\Trainin-Train\\19006T_605827728.ann \n",
      "Error str:  invalid literal for int() with base 10: '5581;5582' \n",
      "\n",
      "\n",
      "KeyError on annotation file: Trainin-Train\\14850T_354625742.ann\n",
      "Error str:  ('SnippetEnd', 'TMariappan5') \n",
      "\n",
      "\n",
      "Number of samples:  27883\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\Trainin-Test\\06141T_623784283.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\Trainin-Test\\08576T_620737251.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\Trainin-Test\\08627T_625266347.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\Trainin-Test\\09384T_622107348.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "KeyError on annotation file: Trainin-Test\\08444T_620551193.ann\n",
      "Error str:  ('SnippetEnd', 'TAnna58') \n",
      "\n",
      "\n",
      "Number of samples:  14475\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\Trainin-Validation\\00804T_626137068.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\Trainin-Validation\\01370T_621977877.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\Trainin-Validation\\02036T_623618343.ann \n",
      "Error str:  invalid literal for int() with base 10: '14440;14493' \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\GoldSet_2019\\Trainin-Validation\\02384T_622335051.ann \n",
      "Error str:  invalid literal for int() with base 10: '42460;42461' \n",
      "\n",
      "\n",
      "Number of samples:  14250\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\Improvement\\Round_1\\S0743731504001686.ann \n",
      "Error str:  invalid literal for int() with base 10: '5593;5594' \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\Improvement\\Round_10\\0307-293194613.ann \n",
      "Error str:  invalid literal for int() with base 10: '9792;9793' \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\Improvement\\Round_26\\0444-2001814042.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\Improvement\\Round_26\\0895-2001815757.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\Improvement\\Round_29\\0453-627694776.ann \n",
      "Error str:  list index out of range \n",
      "\n",
      "\n",
      "Error on annotation file:  C:\\Users\\aydxng\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\Desktop\\ElsevierData\\Dataset2020July\\Dataset2020July\\Improvement\\Round_29\\0812-2002021871.ann \n",
      "Error str:  invalid literal for int() with base 10: '39283;39284' \n",
      "\n",
      "\n",
      "KeyError on annotation file: Improvement\\Round_10\\0485-372492728.ann\n",
      "Error str:  ('SnippetEnd', 'TJuhi2') \n",
      "\n",
      "\n",
      "KeyError on annotation file: Improvement\\Round_12\\0031-19094465.ann\n",
      "Error str:  ('SnippetStart', 'TJuhi3') \n",
      "\n",
      "\n",
      "KeyError on annotation file: Improvement\\Round_14\\0799-224236493.ann\n",
      "Error str:  ('SnippetEnd', 'TAbhinav3') \n",
      "\n",
      "\n",
      "Number of samples:  24348\n"
     ]
    }
   ],
   "source": [
    "dir_name = \"C:\\\\Users\\\\aydxng\\\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\\\Desktop\\\\ElsevierData\\\\Dataset2020July\\\\Dataset2020July\\\\GoldSet_2019\\\\Regression\"\n",
    "out_filename = \"temp.csv\"\n",
    "multiple_folders = False\n",
    "\n",
    "df_reg = FileReader(dir_name,out_filename, multiple_folders).df\n",
    "df_reg['Folder'] = \"GoldSet_2019\\\\Regression\"\n",
    "print('Number of samples: ',df_reg.shape[0])\n",
    "####################\n",
    "dir_name = \"C:\\\\Users\\\\aydxng\\\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\\\Desktop\\\\ElsevierData\\\\Dataset2020July\\\\Dataset2020July\\\\GoldSet_2019\\\\KPI\"\n",
    "out_filename = \"temp.csv\"\n",
    "multiple_folders = False\n",
    "\n",
    "df_kpi = FileReader(dir_name,out_filename, multiple_folders).df\n",
    "df_kpi['Folder'] = \"GoldSet_2019\\\\KPI\"\n",
    "print('Number of samples: ',df_kpi.shape[0])\n",
    "####################\n",
    "dir_name = \"C:\\\\Users\\\\aydxng\\\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\\\Desktop\\\\ElsevierData\\\\Dataset2020July\\\\Dataset2020July\\\\GoldSet_2019\\\\Trainin-Train\"\n",
    "out_filename = \"temp.csv\"\n",
    "multiple_folders = False\n",
    "\n",
    "df_ttra = FileReader(dir_name,out_filename, multiple_folders).df\n",
    "df_ttra['Folder'] = \"GoldSet_2019\\\\Trainin-Train\"\n",
    "print('Number of samples: ',df_ttra.shape[0])\n",
    "####################\n",
    "dir_name = \"C:\\\\Users\\\\aydxng\\\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\\\Desktop\\\\ElsevierData\\\\Dataset2020July\\\\Dataset2020July\\\\GoldSet_2019\\\\Trainin-Test\"\n",
    "out_filename = \"temp.csv\"\n",
    "multiple_folders = False\n",
    "\n",
    "\n",
    "df_tte = FileReader(dir_name,out_filename, multiple_folders).df\n",
    "df_tte['Folder'] = \"GoldSet_2019\\\\Trainin-Test\"\n",
    "print('Number of samples: ',df_tte.shape[0])\n",
    "####################\n",
    "dir_name = \"C:\\\\Users\\\\aydxng\\\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\\\Desktop\\\\ElsevierData\\\\Dataset2020July\\\\Dataset2020July\\\\GoldSet_2019\\\\Trainin-Validation\"\n",
    "out_filename = \"temp.csv\"\n",
    "multiple_folders = False\n",
    "\n",
    "df_tval = FileReader(dir_name,out_filename, multiple_folders).df\n",
    "df_tval['Folder'] = \"GoldSet_2019\\\\Trainin-Validation\"\n",
    "print('Number of samples: ',df_tval.shape[0])\n",
    "####################\n",
    "dir_name = \"C:\\\\Users\\\\aydxng\\\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\\\Desktop\\\\ElsevierData\\\\Dataset2020July\\\\Dataset2020July\\\\Improvement\"\n",
    "out_filename = \"temp.csv\"\n",
    "multiple_folders = True\n",
    "\n",
    "df_imp = FileReader(dir_name,out_filename, multiple_folders).df\n",
    "print('Number of samples: ',df_imp.shape[0])\n",
    "####################\n",
    "df = pd.concat([df_reg,df_kpi,df_ttra,df_tte,df_tval,df_imp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Folder</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Organization</th>\n",
       "      <th>Entity_ID</th>\n",
       "      <th>Is_Acronym</th>\n",
       "      <th>Found_in_FundRef</th>\n",
       "      <th>Snippet</th>\n",
       "      <th>Rel_Offset_Start</th>\n",
       "      <th>Rel_Offset_End</th>\n",
       "      <th>Mention_Offset_Start</th>\n",
       "      <th>Mention_Offset_End</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GoldSet_2019\\Regression</td>\n",
       "      <td>00000R_618242926</td>\n",
       "      <td>NSF</td>\n",
       "      <td>100000001</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>We thank K. Pierce who assisted with mesocosm ...</td>\n",
       "      <td>184</td>\n",
       "      <td>187</td>\n",
       "      <td>63693</td>\n",
       "      <td>63696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GoldSet_2019\\Regression</td>\n",
       "      <td>00001R_619263419</td>\n",
       "      <td>JSPS</td>\n",
       "      <td>501100001691</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>This study was partially supported by JSPS Gra...</td>\n",
       "      <td>38</td>\n",
       "      <td>42</td>\n",
       "      <td>20746</td>\n",
       "      <td>20750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GoldSet_2019\\Regression</td>\n",
       "      <td>00002R_618805515</td>\n",
       "      <td>ADAPT Centre for Digital Content Technology</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>The authors would like to acknowledge the fund...</td>\n",
       "      <td>119</td>\n",
       "      <td>162</td>\n",
       "      <td>45965</td>\n",
       "      <td>46008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GoldSet_2019\\Regression</td>\n",
       "      <td>00002R_618805515</td>\n",
       "      <td>Irish Research Council</td>\n",
       "      <td>501100002081</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>The authors would like to acknowledge the fund...</td>\n",
       "      <td>71</td>\n",
       "      <td>93</td>\n",
       "      <td>45917</td>\n",
       "      <td>45939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GoldSet_2019\\Regression</td>\n",
       "      <td>00003R_621409062</td>\n",
       "      <td>Ministerio de Ciencia e Innovación</td>\n",
       "      <td>501100004837</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>This work was supported in part by the Ministe...</td>\n",
       "      <td>39</td>\n",
       "      <td>73</td>\n",
       "      <td>2735</td>\n",
       "      <td>2769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Folder          Filename  \\\n",
       "0  GoldSet_2019\\Regression  00000R_618242926   \n",
       "1  GoldSet_2019\\Regression  00001R_619263419   \n",
       "2  GoldSet_2019\\Regression  00002R_618805515   \n",
       "3  GoldSet_2019\\Regression  00002R_618805515   \n",
       "4  GoldSet_2019\\Regression  00003R_621409062   \n",
       "\n",
       "                                  Organization     Entity_ID  Is_Acronym  \\\n",
       "0                                          NSF     100000001        True   \n",
       "1                                         JSPS  501100001691        True   \n",
       "2  ADAPT Centre for Digital Content Technology          None       False   \n",
       "3                       Irish Research Council  501100002081       False   \n",
       "4           Ministerio de Ciencia e Innovación  501100004837       False   \n",
       "\n",
       "  Found_in_FundRef                                            Snippet  \\\n",
       "0             True  We thank K. Pierce who assisted with mesocosm ...   \n",
       "1             True  This study was partially supported by JSPS Gra...   \n",
       "2            False  The authors would like to acknowledge the fund...   \n",
       "3             True  The authors would like to acknowledge the fund...   \n",
       "4             True  This work was supported in part by the Ministe...   \n",
       "\n",
       "  Rel_Offset_Start Rel_Offset_End  Mention_Offset_Start  Mention_Offset_End  \n",
       "0              184            187                 63693               63696  \n",
       "1               38             42                 20746               20750  \n",
       "2              119            162                 45965               46008  \n",
       "3               71             93                 45917               45939  \n",
       "4               39             73                  2735                2769  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filenames in Monitor Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_fnames = []\n",
    "with open(\"C:\\\\Users\\\\aydxng\\\\Documents\\\\ds-fundingbodies-linkingcomponent-masterthesis\\\\Thesis\\\\PrepareBERT_forDatabricks\\\\CleanCode\\\\MONITOR_FNAMES.txt\",'r') as f:\n",
    "    monitor_fnames = [x.strip() for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filenames in Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\aydxng\\\\Documents\\\\ds-fundingbodies-linkingcomponent-masterthesis\\\\Thesis\\\\CreateNERSilverSet_NewPipelineOutput\\\\\"\n",
    "with open(path+'df_sent_divided.pkl','rb') as f:\n",
    "    df_sent= pickle.load(f)\n",
    "validation_fnames = df_sent[df_sent.Dataset=='Validation'].Filename.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filenames in Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fnames = os.listdir(\"C:\\\\Users\\\\aydxng\\\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\\\Desktop\\\\ElsevierData\\\\Dataset2020July\\\\Dataset2020July\\\\GoldSet_2019\\\\Regression\")\n",
    "test_fnames = [x[:-4] for x in test_fnames]\n",
    "test_fnames = list(set(test_fnames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Entities in Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'C:\\\\Users\\\\aydxng\\\\Documents\\\\FilesForExperiment\\\\NewTaxononmy\\\\FundRef_v2020-11_20201112_nowrapper_shadow\\\\FundRef_v2020-11_20201112_nowrapper_shadow.rdf'\n",
    "g = rdflib.Graph()\n",
    "g.load(filename)\n",
    "\n",
    "entity_ids = []\n",
    "for x in g:\n",
    "    #If the element is a \"Concept\", it is an entity.\n",
    "    #We add the entity ID to the list\n",
    "    if x[2] == rdflib.term.URIRef('http://www.w3.org/2004/02/skos/core#Concept'):\n",
    "        entity_ids.append(str(x[0]))\n",
    "        \n",
    "entity_ids =[x.split(\"/\")[-1] for x in entity_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dict of NEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all the extracted mentions. Store them in set to delete duplicate mentions\n",
    "ne_dict = dict()\n",
    "for index, row in df.iterrows():\n",
    "    key = row['Folder'] + \"\\\\\" + row['Filename']\n",
    "    if ne_dict.get(key,None) is None:\n",
    "        ne_dict[key] = set()\n",
    "    ne_dict[key].add((row['Mention_Offset_Start'],row['Mention_Offset_End'],row['Organization'],row['Entity_ID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Folder</th>\n",
       "      <th>Filename</th>\n",
       "      <th>ID</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Start_Idx</th>\n",
       "      <th>End_Idx</th>\n",
       "      <th>Sentence_Tokenized</th>\n",
       "      <th>Token_Spans</th>\n",
       "      <th>Gold_Span_Tags_ORG_IOB</th>\n",
       "      <th>Gold_Span_Tags_GRT_IOB</th>\n",
       "      <th>Gold_Span_Tags_IOB</th>\n",
       "      <th>Pipeline_Span_Tags_ORG_IOB</th>\n",
       "      <th>Pipeline_Span_Tags_GRT_IOB</th>\n",
       "      <th>Pipeline_Span_Tags_IOB</th>\n",
       "      <th>Dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GoldSet_2019\\KPI</td>\n",
       "      <td>00000K_622440654</td>\n",
       "      <td>2</td>\n",
       "      <td>The work was supported by funding from NIH (EB...</td>\n",
       "      <td>46958</td>\n",
       "      <td>47235</td>\n",
       "      <td>[The, work, was, supported, by, funding, from,...</td>\n",
       "      <td>[(46958, 46961), (46962, 46966), (46967, 46970...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B_ORG, O, B_GRT, O, O, O...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B_ORG, O, B_GRT, O, O, O...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GoldSet_2019\\KPI</td>\n",
       "      <td>00001K_613572066</td>\n",
       "      <td>1</td>\n",
       "      <td>We appreciate the generous financial support f...</td>\n",
       "      <td>38478</td>\n",
       "      <td>39070</td>\n",
       "      <td>[We, appreciate, the, generous, financial, sup...</td>\n",
       "      <td>[(38478, 38480), (38481, 38491), (38492, 38495...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, B, I, I, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, B_ORG, I_...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, B, I, I, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, B_ORG, I_...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GoldSet_2019\\KPI</td>\n",
       "      <td>00002K_365094003</td>\n",
       "      <td>0</td>\n",
       "      <td>Universidad de Zaragoza gratefully acknowledge...</td>\n",
       "      <td>29863</td>\n",
       "      <td>30013</td>\n",
       "      <td>[Universidad, de, Zaragoza, gratefully, acknow...</td>\n",
       "      <td>[(29863, 29874), (29875, 29877), (29878, 29886...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B, I, I, I, I, I, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B_ORG, I_ORG, I_OR...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B, I, I, I, I, I, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B_ORG, I_ORG, I_OR...</td>\n",
       "      <td>Validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GoldSet_2019\\KPI</td>\n",
       "      <td>00003K_362108299</td>\n",
       "      <td>0</td>\n",
       "      <td>Q. Wei was partially supported by the National...</td>\n",
       "      <td>32927</td>\n",
       "      <td>33095</td>\n",
       "      <td>[Q, ., Wei, was, partially, supported, by, the...</td>\n",
       "      <td>[(32927, 32928), (32928, 32929), (32930, 32933...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, B, I, I, I, I, I, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, B_ORG, I_ORG, I_ORG, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, B, I, I, I, I, I, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, B_ORG, I_ORG, I_ORG, ...</td>\n",
       "      <td>Validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GoldSet_2019\\KPI</td>\n",
       "      <td>00003K_362108299</td>\n",
       "      <td>0</td>\n",
       "      <td>The research is partially supported by the Hon...</td>\n",
       "      <td>33096</td>\n",
       "      <td>33202</td>\n",
       "      <td>[The, research, is, partially, supported, by, ...</td>\n",
       "      <td>[(33096, 33099), (33100, 33108), (33109, 33111...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, B, I, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B_ORG, O, O, O, B_...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, B, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B_ORG, O, O, O, O,...</td>\n",
       "      <td>Validation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Folder          Filename  ID  \\\n",
       "0  GoldSet_2019\\KPI  00000K_622440654   2   \n",
       "1  GoldSet_2019\\KPI  00001K_613572066   1   \n",
       "2  GoldSet_2019\\KPI  00002K_365094003   0   \n",
       "3  GoldSet_2019\\KPI  00003K_362108299   0   \n",
       "4  GoldSet_2019\\KPI  00003K_362108299   0   \n",
       "\n",
       "                                            Sentence  Start_Idx  End_Idx  \\\n",
       "0  The work was supported by funding from NIH (EB...      46958    47235   \n",
       "1  We appreciate the generous financial support f...      38478    39070   \n",
       "2  Universidad de Zaragoza gratefully acknowledge...      29863    30013   \n",
       "3  Q. Wei was partially supported by the National...      32927    33095   \n",
       "4  The research is partially supported by the Hon...      33096    33202   \n",
       "\n",
       "                                  Sentence_Tokenized  \\\n",
       "0  [The, work, was, supported, by, funding, from,...   \n",
       "1  [We, appreciate, the, generous, financial, sup...   \n",
       "2  [Universidad, de, Zaragoza, gratefully, acknow...   \n",
       "3  [Q, ., Wei, was, partially, supported, by, the...   \n",
       "4  [The, research, is, partially, supported, by, ...   \n",
       "\n",
       "                                         Token_Spans  \\\n",
       "0  [(46958, 46961), (46962, 46966), (46967, 46970...   \n",
       "1  [(38478, 38480), (38481, 38491), (38492, 38495...   \n",
       "2  [(29863, 29874), (29875, 29877), (29878, 29886...   \n",
       "3  [(32927, 32928), (32928, 32929), (32930, 32933...   \n",
       "4  [(33096, 33099), (33100, 33108), (33109, 33111...   \n",
       "\n",
       "                              Gold_Span_Tags_ORG_IOB  \\\n",
       "0  [O, O, O, O, O, O, O, B, O, O, O, O, O, O, O, ...   \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, B, I, I, ...   \n",
       "2  [O, O, O, O, O, O, O, O, O, B, I, I, I, I, I, ...   \n",
       "3  [O, O, O, O, O, O, O, O, B, I, I, I, I, I, O, ...   \n",
       "4  [O, O, O, O, O, O, O, O, O, B, O, O, O, O, O, ...   \n",
       "\n",
       "                              Gold_Span_Tags_GRT_IOB  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, B, O, O, O, O, O, ...   \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, B, I, ...   \n",
       "\n",
       "                                  Gold_Span_Tags_IOB  \\\n",
       "0  [O, O, O, O, O, O, O, B_ORG, O, B_GRT, O, O, O...   \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, B_ORG, I_...   \n",
       "2  [O, O, O, O, O, O, O, O, O, B_ORG, I_ORG, I_OR...   \n",
       "3  [O, O, O, O, O, O, O, O, B_ORG, I_ORG, I_ORG, ...   \n",
       "4  [O, O, O, O, O, O, O, O, O, B_ORG, O, O, O, B_...   \n",
       "\n",
       "                          Pipeline_Span_Tags_ORG_IOB  \\\n",
       "0  [O, O, O, O, O, O, O, B, O, O, O, O, O, O, O, ...   \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, B, I, I, ...   \n",
       "2  [O, O, O, O, O, O, O, O, O, B, I, I, I, I, I, ...   \n",
       "3  [O, O, O, O, O, O, O, O, B, I, I, I, I, I, O, ...   \n",
       "4  [O, O, O, O, O, O, O, O, O, B, O, O, O, O, O, ...   \n",
       "\n",
       "                          Pipeline_Span_Tags_GRT_IOB  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, B, O, O, O, O, O, ...   \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, B, ...   \n",
       "\n",
       "                              Pipeline_Span_Tags_IOB     Dataset  \n",
       "0  [O, O, O, O, O, O, O, B_ORG, O, B_GRT, O, O, O...    Training  \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, B_ORG, I_...    Training  \n",
       "2  [O, O, O, O, O, O, O, O, O, B_ORG, I_ORG, I_OR...  Validation  \n",
       "3  [O, O, O, O, O, O, O, O, B_ORG, I_ORG, I_ORG, ...  Validation  \n",
       "4  [O, O, O, O, O, O, O, O, O, B_ORG, O, O, O, O,...  Validation  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This dataframe has all the processed sentences\n",
    "df_sent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the mentions for training set here\n",
    "training_named_entities = []\n",
    "for key, value in ne_dict.items():\n",
    "    train=False\n",
    "    if \"Regression\" not in key:\n",
    "        #Definitely training set\n",
    "        if \"KPI\" not in key:\n",
    "            train=True\n",
    "        else:\n",
    "            filename = key.split(\"\\\\\")[-1]\n",
    "            #Definitely training set\n",
    "            if filename not in monitor_fnames and filename not in validation_fnames:\n",
    "                train=True\n",
    "    #If this file is in training data\n",
    "    if train:\n",
    "        folder = \"\\\\\".join(key.split(\"\\\\\")[:-1])\n",
    "        filename = key.split(\"\\\\\")[-1]\n",
    "        #Loop over named entities of this file\n",
    "        for ne in value:\n",
    "            #If the link is None or link exists in taxonomy we are interested\n",
    "            if ne[3] is None or ne[3] in entity_ids:\n",
    "                training_named_entities.append((folder,filename,ne[0],ne[1],ne[2],ne[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the mentions for monitor set here\n",
    "monitor_named_entities = []\n",
    "for key, value in ne_dict.items():\n",
    "    #All monitor filenames are from KPI\n",
    "    if \"KPI\" in key:\n",
    "        folder = \"\\\\\".join(key.split(\"\\\\\")[:-1])\n",
    "        filename = key.split(\"\\\\\")[-1]\n",
    "        #Definitely monitor\n",
    "        if filename in monitor_fnames:\n",
    "            #Loop over named entities of this file\n",
    "            for ne in value:\n",
    "                #If the link is None or link exists in taxonomy we are interested\n",
    "                if ne[3] is None or ne[3] in entity_ids:\n",
    "                    monitor_named_entities.append((folder,filename,ne[0],ne[1],ne[2],ne[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the mentions for validation set here\n",
    "valid_named_entities = []\n",
    "for key, value in ne_dict.items():\n",
    "    #All valid filenames are from KPI\n",
    "    if \"KPI\" in key:\n",
    "        folder = \"\\\\\".join(key.split(\"\\\\\")[:-1])\n",
    "        filename = key.split(\"\\\\\")[-1]\n",
    "        #Definitely monitor\n",
    "        if filename in validation_fnames:\n",
    "            #Loop over named entities of this file\n",
    "            for ne in value:\n",
    "                #If the link is None or link exists in taxonomy we are interested\n",
    "                if ne[3] is None or ne[3] in entity_ids:\n",
    "                    valid_named_entities.append((folder,filename,ne[0],ne[1],ne[2],ne[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the lengths and make sure there is no overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103003\n"
     ]
    }
   ],
   "source": [
    "print(len(training_named_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5745\n"
     ]
    }
   ],
   "source": [
    "print(len(monitor_named_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20107\n"
     ]
    }
   ],
   "source": [
    "print(len(valid_named_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set.intersection(set(training_named_entities),set(valid_named_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set.intersection(set(valid_named_entities),set(monitor_named_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set.intersection(set(training_named_entities),set(monitor_named_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These will store the samples in the format we will write them\n",
    "to_write_train = []\n",
    "to_write_monitor = []\n",
    "to_write_valid = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We may need to process more sentences to get the context. However, this is expensive\n",
    "#That is why, first we make a deep copy of this list, then, when we create a training example,\n",
    "#we delete it from this list. This means that we already have the sentence for that training example,\n",
    "#we may need to keep processing sentences for others\n",
    "training_named_entities_copy = [x for x in training_named_entities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(training_named_entities)):\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "    item = training_named_entities[i]\n",
    "    temp = df_sent[(df_sent['Folder']==item[0]) & (df_sent['Filename']==item[1])]\n",
    "    for index, row in temp.iterrows():\n",
    "        if item[2] >= row['Start_Idx'] and item[3] <= row['End_Idx']:\n",
    "            training_named_entities_copy.remove(item)\n",
    "            to_write_train.append({\"mention\":item[4],\n",
    "                                   \"context_left\": row['Sentence'][0:item[2]-row['Start_Idx']],\n",
    "                                   \"context_right\":row['Sentence'][item[3]+1-row['Start_Idx']:],\n",
    "                                   \"label_id\":item[5],\n",
    "                                   \"folder\":item[0],\n",
    "                                   \"filename\":item[1]})\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_named_entities_copy = [x for x in monitor_named_entities]\n",
    "valid_named_entities_copy = [x for x in valid_named_entities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(monitor_named_entities)):\n",
    "    item = monitor_named_entities[i]\n",
    "    temp = df_sent[(df_sent['Folder']==item[0]) & (df_sent['Filename']==item[1])]\n",
    "    for index, row in temp.iterrows():\n",
    "        if item[2] >= row['Start_Idx'] and item[3] <= row['End_Idx']:\n",
    "            monitor_named_entities_copy.remove(item)\n",
    "            to_write_monitor.append({\"mention\":item[4],\n",
    "                                   \"context_left\": row['Sentence'][0:item[2]-row['Start_Idx']],\n",
    "                                   \"context_right\":row['Sentence'][item[3]+1-row['Start_Idx']:],\n",
    "                                   \"label_id\":item[5],\n",
    "                                   \"folder\":item[0],\n",
    "                                   \"filename\":item[1]})\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(valid_named_entities)):\n",
    "    item = valid_named_entities[i]\n",
    "    temp = df_sent[(df_sent['Folder']==item[0]) & (df_sent['Filename']==item[1])]\n",
    "    for index, row in temp.iterrows():\n",
    "        if item[2] >= row['Start_Idx'] and item[3] <= row['End_Idx']:\n",
    "            valid_named_entities_copy.remove(item)\n",
    "            to_write_valid.append({\"mention\":item[4],\n",
    "                                   \"context_left\": row['Sentence'][0:item[2]-row['Start_Idx']],\n",
    "                                   \"context_right\":row['Sentence'][item[3]+1-row['Start_Idx']:],\n",
    "                                   \"label_id\":item[5],\n",
    "                                   \"folder\":item[0],\n",
    "                                   \"filename\":item[1]})\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we check the articles, for which we have named entities but no sentences.\n",
    "#For those, we need to check negative sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_these = set()\n",
    "for item in training_named_entities_copy:\n",
    "    process_these.add((item[0],item[1]))\n",
    "for item in valid_named_entities_copy:\n",
    "    process_these.add((item[0],item[1]))\n",
    "for item in monitor_named_entities_copy:\n",
    "    process_these.add((item[0],item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14199"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(process_these)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "def get_sentences(to_be_processed):\n",
    "    path_to_goldset = \"C:\\\\Users\\\\aydxng\\\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\\\Desktop\\\\ElsevierData\\\\Dataset2020July\\\\Dataset2020July\\\\\"\n",
    "    path_to_pipeline = \"C:\\\\Users\\\\aydxng\\PIPELINEOUTPUT\\\\\"\n",
    "    #Get sentences\n",
    "    sentences = []\n",
    "    #Loop over the fiels again\n",
    "    start = time.time()\n",
    "    for i, item in enumerate(to_be_processed):\n",
    "        folder= item[0]\n",
    "        fname=item[1]\n",
    "        if i%100==0:\n",
    "            print(i,\" \",time.time()-start)\n",
    "        content = None\n",
    "        #Open the original text file\n",
    "        with open(path_to_goldset+folder+\"\\\\\"+fname+\".txt\",'r',encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        index_array = np.ones(len(content))\n",
    "        for i in range(len(content)):\n",
    "            if content[i] == \" \":\n",
    "                index_array[i] = 0\n",
    "        content_no_ws = content.replace(\" \",\"\")\n",
    "        #Open the file\n",
    "        with open(path_to_pipeline+folder+\"\\\\\"+fname+\".txt\",'r',encoding='utf-8') as f:\n",
    "            #Read file line by line\n",
    "            for line in f.readlines():\n",
    "                #Split line by whitespace\n",
    "                line_vec = line.strip().split()\n",
    "                try:\n",
    "                    #If the line contains a sentence\n",
    "                    if 'Sentence' in line_vec[1]:\n",
    "                        #If this is a negative sentence\n",
    "                        if line_vec[2] == 'negative':\n",
    "                            #Get sentence text\n",
    "                            text = line[re.search('negative',line).span()[1]+1:][:-1].replace(\" \", \"\")\n",
    "                            #Get relative span wrt the corresponding Section\n",
    "                            temp_start_idx = content_no_ws.find(text)\n",
    "                            temp_end_idx = temp_start_idx + len(text)\n",
    "                            start_idx = -1\n",
    "                            end_idx = -1\n",
    "                            ctr = 0\n",
    "                            for i in range(len(index_array)):\n",
    "                                if ctr == temp_start_idx:\n",
    "                                    start_idx = i\n",
    "                                if ctr == temp_end_idx-1:\n",
    "                                    end_idx = i+1\n",
    "                                ctr+=index_array[i]\n",
    "                            #Add row\n",
    "                            sentences.append([folder,fname,content[start_idx:end_idx],start_idx,end_idx])\n",
    "                except IndexError:\n",
    "                    pass        \n",
    "    #Create sentences dataframe\n",
    "    df_sent = pd.DataFrame(sentences,columns=['Folder','Filename','Sentence','Start_Idx','End_Idx'])\n",
    "    print(\"Problematic Sentences: \",len(df_sent[(df_sent.Start_Idx==-1)|(df_sent.End_Idx==-1)]),\" out of \",len(df_sent),\"(\",100*(len(df_sent[(df_sent.Start_Idx==-1)|(df_sent.End_Idx==-1)])/len(df_sent)),\"%)\")\n",
    "    #Return both dataframes\n",
    "    return df_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   0.0\n",
      "100   18.08596658706665\n",
      "200   44.339909076690674\n",
      "300   66.60586166381836\n",
      "400   105.14178037643433\n",
      "500   152.25073862075806\n",
      "600   181.0206789970398\n",
      "700   222.1665599346161\n",
      "800   321.6925354003906\n",
      "900   359.78242588043213\n",
      "1000   383.8383822441101\n",
      "1100   416.32737374305725\n",
      "1200   450.1572802066803\n",
      "1300   530.605183839798\n",
      "1400   563.636093378067\n",
      "1500   587.8510899543762\n",
      "1600   620.2520360946655\n",
      "1700   645.5199964046478\n",
      "1800   677.8929092884064\n",
      "1900   756.7547886371613\n",
      "2000   794.3007309436798\n",
      "2100   820.6097304821014\n",
      "2200   862.7506713867188\n",
      "2300   906.5676126480103\n",
      "2400   942.510555267334\n",
      "2500   965.3304917812347\n",
      "2600   985.1574954986572\n",
      "2700   1015.2344539165497\n",
      "2800   1079.5673732757568\n",
      "2900   1101.5893383026123\n",
      "3000   1196.3352122306824\n",
      "3100   1239.3631224632263\n",
      "3200   1260.949096441269\n",
      "3300   1279.8751137256622\n",
      "3400   1311.1990621089935\n",
      "3500   1338.6780326366425\n",
      "3600   1404.6639091968536\n",
      "3700   1434.5448760986328\n",
      "3800   1465.183839559555\n",
      "3900   1490.6778416633606\n",
      "4000   1514.1348159313202\n",
      "4100   1541.5737454891205\n",
      "4200   1581.4206974506378\n",
      "4300   1613.6506576538086\n",
      "4400   1636.0946311950684\n",
      "4500   1662.4196062088013\n",
      "4600   1686.914574623108\n",
      "4700   1726.1025393009186\n",
      "4800   1745.7635524272919\n",
      "4900   1774.8715221881866\n",
      "5000   1805.1384587287903\n",
      "5100   1825.2814359664917\n",
      "5200   1953.3293430805206\n",
      "5300   1980.9163174629211\n",
      "5400   2010.5202527046204\n",
      "5500   2054.2122070789337\n",
      "5600   2628.838104724884\n",
      "5700   2671.470953941345\n",
      "5800   2707.3358702659607\n",
      "5900   2727.9848074913025\n",
      "6000   2744.2797236442566\n",
      "6100   2768.352682828903\n",
      "6200   2814.135537624359\n",
      "6300   2850.7104303836823\n",
      "6400   2879.2883036136627\n",
      "6500   2898.4202785491943\n",
      "6600   2917.0912189483643\n",
      "6700   2932.009175300598\n",
      "6800   2971.9560556411743\n",
      "6900   3015.636888742447\n",
      "7000   3067.2967853546143\n",
      "7100   3100.8696937561035\n",
      "7200   3131.4286127090454\n",
      "7300   3154.325516462326\n",
      "7400   3212.056394338608\n",
      "7500   3244.4012711048126\n",
      "7600   3272.6282329559326\n",
      "7700   3309.3111429214478\n",
      "7800   3332.6090803146362\n",
      "7900   3350.640039205551\n",
      "8000   3411.244892835617\n",
      "8100   3439.4848985671997\n",
      "8200   3483.7647914886475\n",
      "8300   3505.746739387512\n",
      "8400   3525.6266939640045\n",
      "8500   3570.9116179943085\n",
      "8600   3605.2265059947968\n",
      "8700   3628.8894593715668\n",
      "8800   3673.2763633728027\n",
      "8900   3690.50132727623\n",
      "9000   3722.1242616176605\n",
      "9100   3748.964207649231\n",
      "9200   3765.5951721668243\n",
      "9300   3786.0721254348755\n",
      "9400   3823.047051668167\n",
      "9500   3860.6989698410034\n",
      "9600   3896.258895635605\n",
      "9700   3912.1399009227753\n",
      "9800   3927.0078370571136\n",
      "9900   3954.94482588768\n",
      "10000   3986.281724691391\n",
      "10100   4015.1106736660004\n",
      "10200   4044.2106144428253\n",
      "10300   4083.367542743683\n",
      "10400   4101.551508903503\n",
      "10500   4115.150480747223\n",
      "10600   4138.301435470581\n",
      "10700   4154.610407829285\n",
      "10800   4183.503385782242\n",
      "10900   4219.389291524887\n",
      "11000   4264.661247730255\n",
      "11100   4291.75816822052\n",
      "11200   4322.210113286972\n",
      "11300   4350.80206823349\n",
      "11400   4374.893027305603\n",
      "11500   4407.507969617844\n",
      "11600   4455.103889942169\n",
      "11700   4491.173830509186\n",
      "11800   4529.4257690906525\n",
      "11900   4549.107740163803\n",
      "12000   4575.5787007808685\n",
      "12100   4604.118658065796\n",
      "12200   4630.638654708862\n",
      "12300   4684.959538936615\n",
      "12400   4713.200494527817\n",
      "12500   4748.522440433502\n",
      "12600   4780.164392948151\n",
      "12700   4832.350323677063\n",
      "12800   4857.042291641235\n",
      "12900   4898.126277208328\n",
      "13000   4921.303209066391\n",
      "13100   4986.870128154755\n",
      "13200   5047.2660455703735\n",
      "13300   5108.2470026016235\n",
      "13400   5143.956928730011\n",
      "13500   5168.986895084381\n",
      "13600   5194.973862886429\n",
      "13700   5230.706820726395\n",
      "13800   5269.255809545517\n",
      "13900   5290.821747541428\n",
      "14000   5314.939752578735\n",
      "14100   5356.794668674469\n",
      "Problematic Sentences:  116  out of  133675 ( 0.08677763231718721 %)\n"
     ]
    }
   ],
   "source": [
    "df_sent_negative = get_sentences(process_these)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get the context for others as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35903"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_named_entities_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(training_named_entities_copy)):\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "    item = training_named_entities_copy[i]\n",
    "    temp = df_sent_negative[(df_sent_negative['Folder']==item[0]) & (df_sent_negative['Filename']==item[1])]\n",
    "    for index, row in temp.iterrows():\n",
    "        if item[2] >= row['Start_Idx'] and item[3] <= row['End_Idx']:\n",
    "            to_write_train.append({\"mention\":item[4],\n",
    "                                   \"context_left\": row['Sentence'][0:item[2]-row['Start_Idx']],\n",
    "                                   \"context_right\":row['Sentence'][item[3]+1-row['Start_Idx']:],\n",
    "                                   \"label_id\":item[5],\n",
    "                                   \"folder\":item[0],\n",
    "                                   \"filename\":item[1]})\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(monitor_named_entities_copy)):\n",
    "    item = monitor_named_entities_copy[i]\n",
    "    temp = df_sent_negative[(df_sent_negative['Folder']==item[0]) & (df_sent_negative['Filename']==item[1])]\n",
    "    for index, row in temp.iterrows():\n",
    "        if item[2] >= row['Start_Idx'] and item[3] <= row['End_Idx']:\n",
    "            to_write_monitor.append({\"mention\":item[4],\n",
    "                                   \"context_left\": row['Sentence'][0:item[2]-row['Start_Idx']],\n",
    "                                   \"context_right\":row['Sentence'][item[3]+1-row['Start_Idx']:],\n",
    "                                   \"label_id\":item[5],\n",
    "                                   \"folder\":item[0],\n",
    "                                   \"filename\":item[1]})\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(valid_named_entities_copy)):\n",
    "    item = valid_named_entities_copy[i]\n",
    "    temp = df_sent_negative[(df_sent_negative['Folder']==item[0]) & (df_sent_negative['Filename']==item[1])]\n",
    "    for index, row in temp.iterrows():\n",
    "        if item[2] >= row['Start_Idx'] and item[3] <= row['End_Idx']:\n",
    "            to_write_valid.append({\"mention\":item[4],\n",
    "                                   \"context_left\": row['Sentence'][0:item[2]-row['Start_Idx']],\n",
    "                                   \"context_right\":row['Sentence'][item[3]+1-row['Start_Idx']:],\n",
    "                                   \"label_id\":item[5],\n",
    "                                   \"folder\":item[0],\n",
    "                                   \"filename\":item[1]})\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95761\n",
      "5618\n",
      "19765\n"
     ]
    }
   ],
   "source": [
    "print(len(to_write_train))\n",
    "print(len(to_write_monitor))\n",
    "print(len(to_write_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('train_all.jsonl', 'w') as outfile:\n",
    "    for entry in to_write_train:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')\n",
    "with open('monitor_all.jsonl', 'w') as outfile:\n",
    "    for entry in to_write_monitor:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')\n",
    "with open('valid_all.jsonl', 'w') as outfile:\n",
    "    for entry in to_write_valid:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Entity Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "import numpy as np\n",
    "import pickle\n",
    "import rdflib\n",
    "from transformers import  BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'C:\\\\Users\\\\aydxng\\\\Documents\\\\FilesForExperiment\\\\NewTaxononmy\\\\FundRef_v2020-11_20201112_nowrapper_shadow\\\\FundRef_v2020-11_20201112_nowrapper_shadow.rdf'\n",
    "g = rdflib.Graph()\n",
    "g.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_ids = []\n",
    "for x in g:\n",
    "    #If the element is a \"Concept\", it is an entity.\n",
    "    #We add the entity ID to the list\n",
    "    if x[2] == rdflib.term.URIRef('http://www.w3.org/2004/02/skos/core#Concept'):\n",
    "        entity_ids.append(str(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('entities.pkl','rb') as f:\n",
    "    entity_dict=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENT_LABEL_TAG = \"[unused2]\"\n",
    "ENT_COUNTRY_TAG = \"[unused3]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_ids_and_tokens(tokenizer,cand_labels,cand_country,max_cand_length):\n",
    "    cls_token = tokenizer.cls_token\n",
    "    sep_token = tokenizer.sep_token\n",
    "    \n",
    "    cand_tokens = [cls_token]\n",
    "    for label_ in cand_labels:\n",
    "        cand_tokens += tokenizer.tokenize(label_) + [ENT_LABEL_TAG]\n",
    "    cand_tokens = cand_tokens[:-1]\n",
    "    cand_tokens += [ENT_COUNTRY_TAG] + tokenizer.tokenize(cand_country) + [sep_token]\n",
    "    \n",
    "    input_ids = tokenizer.convert_tokens_to_ids(cand_tokens)\n",
    "    padding = [0] * (max_cand_length - len(input_ids))\n",
    "    input_ids += padding\n",
    "    return input_ids, cand_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_one(lst):\n",
    "    sim_scores = []\n",
    "    for i in range(len(lst)):\n",
    "        max_sim =0\n",
    "        for j in range(len(lst)):\n",
    "            if i!=j:\n",
    "                max_sim = max(0,fuzz.token_sort_ratio(lst[i],lst[j]))\n",
    "        sim_scores.append(max_sim)\n",
    "    idx = np.argmax(sim_scores)\n",
    "    new_lst = lst[0:idx] + lst[idx+1:]\n",
    "    return new_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidate_representation(label_idx, tokenizer, max_cand_length):\n",
    "    \n",
    "    cand_labels = entity_dict[str(label_idx)]['Labels']\n",
    "    cand_labels = list(set(cand_labels))\n",
    "    cand_country = entity_dict[str(label_idx)]['Country']\n",
    "    \n",
    "    input_ids, cand_tokens = get_input_ids_and_tokens(tokenizer,cand_labels,cand_country,max_cand_length)\n",
    "    \n",
    "    while len(input_ids) != max_cand_length:\n",
    "        cand_labels = reduce_one(cand_labels)\n",
    "        input_ids, cand_tokens = get_input_ids_and_tokens(tokenizer,cand_labels,cand_country,max_cand_length)\n",
    "    \n",
    "    assert len(input_ids) == max_cand_length\n",
    "    \n",
    "    return {\n",
    "        \"tokens\": cand_tokens,\n",
    "        \"ids\": input_ids,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aydxng\\Anaconda3\\envs\\Python37\\lib\\site-packages\\urllib3\\connectionpool.py:988: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "C:\\Users\\aydxng\\Anaconda3\\envs\\Python37\\lib\\site-packages\\urllib3\\connectionpool.py:988: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dicts = dict()\n",
    "for e_id in entity_ids:\n",
    "    e_id=e_id.split(\"/\")[-1]\n",
    "    all_dicts[e_id] = get_candidate_representation(e_id, tokenizer, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('501100001230',\n",
       " {'tokens': ['[CLS]',\n",
       "   'Macquarie',\n",
       "   'University',\n",
       "   '[unused3]',\n",
       "   'Australia',\n",
       "   '[SEP]'],\n",
       "  'ids': [101,\n",
       "   26828,\n",
       "   1239,\n",
       "   3,\n",
       "   1754,\n",
       "   102,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(all_dicts.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('entities_256.pkl','wb') as f:\n",
    "    pickle.dump(all_dicts,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "128 -> 96\n",
    "256 -> 36\n",
    "512 -> 23"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
