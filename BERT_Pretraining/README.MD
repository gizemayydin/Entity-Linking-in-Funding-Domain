This folder contains the code to perform Task-Adaptive Pretraining [1] on BERT using funding sentences.

The code is a slightly modified version of the one in this repository: https://github.com/allenai/dont-stop-pretraining/blob/master/ADAPTIVE_PRETRAINING.md

[1]  Suchin Gururangan,  Ana Marasovic,  Swabha Swayamdipta,  Kyle Lo,  Iz Beltagy, Doug Downey, and Noah A. Smith. Don't stop pretraining: Adapt language models to domains and tasks.  InProceedings of ACL, 2020.