{"cells":[{"cell_type":"code","source":["%pip install transformers==3.5.1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9697a135-f8f4-4d8e-a61b-2b8deeb2123f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting transformers==3.5.1\n  Using cached transformers-3.5.1-py3-none-any.whl (1.3 MB)\nCollecting filelock\n  Using cached filelock-3.0.12-py3-none-any.whl (7.6 kB)\nRequirement already satisfied: packaging in /local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages (from transformers==3.5.1) (20.4)\nRequirement already satisfied: requests in /local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages (from transformers==3.5.1) (2.24.0)\nRequirement already satisfied: numpy in /local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages (from transformers==3.5.1) (1.19.2)\nRequirement already satisfied: tqdm&gt;=4.27 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages (from transformers==3.5.1) (4.50.2)\nCollecting sentencepiece==0.1.91\n  Using cached sentencepiece-0.1.91-cp38-cp38-manylinux1_x86_64.whl (1.1 MB)\nRequirement already satisfied: regex!=2019.12.17 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages (from transformers==3.5.1) (2020.10.15)\nRequirement already satisfied: protobuf in /local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages (from transformers==3.5.1) (3.13.0)\nProcessing /root/.cache/pip/wheels/d6/17/75/f2ed13c472c4cecc14f003401bb45efadca64cc589d4bf3103/sacremoses-0.0.44-py3-none-any.whl\nCollecting tokenizers==0.9.3\n  Using cached tokenizers-0.9.3-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\nRequirement already satisfied: six in /local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages (from packaging-&gt;transformers==3.5.1) (1.15.0)\nRequirement already satisfied: pyparsing&gt;=2.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages (from packaging-&gt;transformers==3.5.1) (2.4.7)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages (from requests-&gt;transformers==3.5.1) (2.10)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages (from requests-&gt;transformers==3.5.1) (2020.12.5)\nRequirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages (from requests-&gt;transformers==3.5.1) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages (from requests-&gt;transformers==3.5.1) (1.25.11)\nRequirement already satisfied: setuptools in /local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages (from protobuf-&gt;transformers==3.5.1) (50.3.1.post20201107)\nRequirement already satisfied: joblib in /local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages (from sacremoses-&gt;transformers==3.5.1) (0.17.0)\nRequirement already satisfied: click in /local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages (from sacremoses-&gt;transformers==3.5.1) (7.1.2)\nInstalling collected packages: filelock, sentencepiece, sacremoses, tokenizers, transformers\nSuccessfully installed filelock-3.0.12 sacremoses-0.0.44 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\nPython interpreter will be restarted.\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting transformers==3.5.1\n  Using cached transformers-3.5.1-py3-none-any.whl (1.3 MB)\nCollecting filelock\n  Using cached filelock-3.0.12-py3-none-any.whl (7.6 kB)\nRequirement already satisfied: packaging in /local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages (from transformers==3.5.1) (20.4)\nRequirement already satisfied: requests in /local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages (from transformers==3.5.1) (2.24.0)\nRequirement already satisfied: numpy in /local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages (from transformers==3.5.1) (1.19.2)\nRequirement already satisfied: tqdm&gt;=4.27 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages (from transformers==3.5.1) (4.50.2)\nCollecting sentencepiece==0.1.91\n  Using cached sentencepiece-0.1.91-cp38-cp38-manylinux1_x86_64.whl (1.1 MB)\nRequirement already satisfied: regex!=2019.12.17 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages (from transformers==3.5.1) (2020.10.15)\nRequirement already satisfied: protobuf in /local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages (from transformers==3.5.1) (3.13.0)\nProcessing /root/.cache/pip/wheels/d6/17/75/f2ed13c472c4cecc14f003401bb45efadca64cc589d4bf3103/sacremoses-0.0.44-py3-none-any.whl\nCollecting tokenizers==0.9.3\n  Using cached tokenizers-0.9.3-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\nRequirement already satisfied: six in /local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages (from packaging-&gt;transformers==3.5.1) (1.15.0)\nRequirement already satisfied: pyparsing&gt;=2.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages (from packaging-&gt;transformers==3.5.1) (2.4.7)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages (from requests-&gt;transformers==3.5.1) (2.10)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages (from requests-&gt;transformers==3.5.1) (2020.12.5)\nRequirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages (from requests-&gt;transformers==3.5.1) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages (from requests-&gt;transformers==3.5.1) (1.25.11)\nRequirement already satisfied: setuptools in /local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages (from protobuf-&gt;transformers==3.5.1) (50.3.1.post20201107)\nRequirement already satisfied: joblib in /local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages (from sacremoses-&gt;transformers==3.5.1) (0.17.0)\nRequirement already satisfied: click in /local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages (from sacremoses-&gt;transformers==3.5.1) (7.1.2)\nInstalling collected packages: filelock, sentencepiece, sacremoses, tokenizers, transformers\nSuccessfully installed filelock-3.0.12 sacremoses-0.0.44 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\nPython interpreter will be restarted.\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["import glob\n#import logging\nimport os\nimport pickle\nimport random\nimport re\nimport shutil\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\nfrom torch.utils.data.distributed import DistributedSampler\nfrom tqdm import tqdm, trange \n\nfrom transformers import (\n    MODEL_WITH_LM_HEAD_MAPPING,\n    WEIGHTS_NAME,\n    AdamW,\n    AutoConfig,\n    BertTokenizerFast,\n    AutoModelForMaskedLM,\n    PreTrainedModel,\n    PreTrainedTokenizer,\n    get_linear_schedule_with_warmup,\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a56e503-3aa5-44f8-842b-7f65b2b583ba"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["try:\n    from torch.utils.tensorboard import SummaryWriter\nexcept ImportError:\n    from tensorboardX import SummaryWriter\n\n\n#logger = logging.getLogger(__name__)\n\n\nMODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2723f9e9-159c-49b4-b958-e2f11bdff470"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["class TextDataset(Dataset):\n    def __init__(self, tokenizer: PreTrainedTokenizer, args, file_path: str, block_size=512):\n        assert os.path.isfile(file_path)\n\n        block_size = block_size - tokenizer.num_special_tokens_to_add(pair=False)\n\n        directory, filename = os.path.split(file_path)\n        cached_features_file = os.path.join(\n            directory, args.model_type + \"_cached_lm_\" + str(block_size) + \"_\" + filename\n        )\n\n        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n            print(\"Loading features from cached file %s\", cached_features_file)\n            with open(cached_features_file, \"rb\") as handle:\n                self.examples = pickle.load(handle)\n        else:\n            print(\"Creating features from dataset file at %s\", directory)\n            pass\n\n            self.examples = []\n            with open(file_path, encoding=\"utf-8\") as f:\n                text = f.read()\n\n            tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n\n            for i in range(0, len(tokenized_text) - block_size + 1, block_size):  # Truncate in block of block_size\n                self.examples.append(tokenizer.build_inputs_with_special_tokens(tokenized_text[i : i + block_size]))\n            # Note that we are loosing the last truncated example here for the sake of simplicity (no padding)\n            # If your dataset is small, first you should loook for a bigger one :-) and second you\n            # can change this behavior by adding (model specific) padding.\n\n            print(\"Saving features into cached file %s\", cached_features_file)\n            with open(cached_features_file, \"wb\") as handle:\n                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, item):\n        return torch.tensor(self.examples[item], dtype=torch.long)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"33653552-6b3b-4003-94fa-77793d2aef81"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["class LineByLineTextDataset_old(Dataset):\n    def __init__(self, tokenizer: PreTrainedTokenizer, args, file_path: str, block_size=512):\n        assert os.path.isfile(file_path)\n        # Here, we do not cache the features, operating under the assumption\n        # that we will soon use fast multithreaded tokenizers from the\n        # `tokenizers` repo everywhere =)\n        print(\"Creating features from dataset file at %s\", file_path)\n\n        with open(file_path, encoding=\"utf-8\") as f:\n            lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n\n        self.examples = tokenizer.batch_encode_plus(lines, add_special_tokens=True, max_length=block_size)[\"input_ids\"]\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, i):\n        return torch.tensor(self.examples[i], dtype=torch.long)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca01cc3a-34e5-4db1-a5bf-8f305ca69304"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["class LineByLineTextDataset(Dataset):\n    def __init__(self, tokenizer: PreTrainedTokenizer, args, file_path: str, block_size=512):\n        assert os.path.isfile(file_path)\n        # Here, we do not cache the features, operating under the assumption\n        # that we will soon use fast multithreaded tokenizers from the\n        # `tokenizers` repo everywhere =)\n        print(\"Creating features from dataset file at %s\", file_path)\n        self.tokenizer = tokenizer\n        self.block_size = block_size\n        with open(file_path, encoding=\"utf-8\") as f:\n            self.lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n\n        #self.examples = tokenizer.batch_encode_plus(lines, add_special_tokens=True, max_length=block_size)[\"input_ids\"]\n\n    def __len__(self):\n        return len(self.lines)\n\n    def __getitem__(self, i):\n        return torch.tensor(self.tokenizer.batch_encode_plus([self.lines[i]], add_special_tokens=True, max_length=self.block_size)[\"input_ids\"][0], dtype=torch.long)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05ef423c-d6f1-41b6-a672-d34980b55403"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["import pickle\nclass LineByLineTextDataset_new(Dataset):\n    def __init__(self, tokenizer: PreTrainedTokenizer, args, file_path: str, block_size=512):\n        assert os.path.isfile(file_path)\n        # Here, we do not cache the features, operating under the assumption\n        # that we will soon use fast multithreaded tokenizers from the\n        # `tokenizers` repo everywhere =)\n        print(\"Creating features from dataset file at %s\", file_path)\n        self.file_range_list = []\n        self.step = 1000000\n        \n        with open(file_path, encoding=\"utf-8\") as f:\n            self.lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n        \n        for i in range(0,self.lines,step):\n            examples = tokenizer.batch_encode_plus(lines[i:i+step], add_special_tokens=True, max_length=block_size)[\"input_ids\"]\n            self.file_range_list.append((i,i+step))\n            with open(\"/dbfs/mnt/els-nlp-experts1/data/Gizem/examples_\"+str(i)+_+str(i+step),\"wb\") as f:\n              pickle.dump(examples,f)\n\n    def __len__(self):\n        return len(self.lines)\n\n    def __getitem__(self, i):\n        start = None\n        for item in self.file_range_list:\n          if i>= item[0] and i<item[1]:\n            start=item[0]\n        with open(\"/dbfs/mnt/els-nlp-experts1/data/Gizem/examples_\"+str(start)+_+str(start+step),\"rb\") as f:\n          examples = pickle.load(f)\n        relative_i = i - start\n        \n        return torch.tensor(examples[relative_i], dtype=torch.long)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5dbe4a23-efcc-467d-aada-e1df944a61d4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def load_and_cache_examples(args, tokenizer, evaluate=False,f=''):\n    #file_path = args.eval_data_file if evaluate else args.train_data_file\n    file_path = f if evaluate else args.train_data_file\n    if args.line_by_line:\n        return LineByLineTextDataset(tokenizer, args, file_path=file_path, block_size=args.block_size)\n    else:\n        return TextDataset(tokenizer, args, file_path=file_path, block_size=args.block_size)\n\n\ndef set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)\n\n\ndef _sorted_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> List[str]:\n    ordering_and_checkpoint_path = []\n\n    glob_checkpoints = glob.glob(os.path.join(args.output_dir, \"{}-*\".format(checkpoint_prefix)))\n\n    for path in glob_checkpoints:\n        if use_mtime:\n            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n        else:\n            regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\n            if regex_match and regex_match.groups():\n                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n\n    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n    return checkpoints_sorted\n\n\ndef _rotate_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> None:\n    if not args.save_total_limit:\n        return\n    if args.save_total_limit <= 0:\n        return\n\n    # Check if we should delete older checkpoint(s)\n    checkpoints_sorted = _sorted_checkpoints(args, checkpoint_prefix, use_mtime)\n    if len(checkpoints_sorted) <= args.save_total_limit:\n        return\n\n    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n    for checkpoint in checkpoints_to_be_deleted:\n        print(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n        shutil.rmtree(checkpoint)\n\n\ndef mask_tokens(inputs: torch.Tensor, tokenizer: PreTrainedTokenizer, args) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\" Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. \"\"\"\n\n    if tokenizer.mask_token is None:\n        raise ValueError(\n            \"This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the --mlm flag if you want to use this tokenizer.\"\n        )\n\n    labels = inputs.clone()\n    # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n    probability_matrix = torch.full(labels.shape, args.mlm_probability)\n    special_tokens_mask = [\n        tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n    ]\n    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n    if tokenizer._pad_token is not None:\n        padding_mask = labels.eq(tokenizer.pad_token_id)\n        probability_matrix.masked_fill_(padding_mask, value=0.0)\n    masked_indices = torch.bernoulli(probability_matrix).bool()\n    labels[~masked_indices] = -100  # We only compute loss on masked tokens\n\n    # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n\n    # 10% of the time, we replace masked input tokens with random word\n    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n    inputs[indices_random] = random_words[indices_random]\n\n    # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n    return inputs, labels\n\n\ndef train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n    \n\n    \"\"\" Train the model \"\"\"\n    #if args.local_rank in [-1, 0]:\n    #    tb_writer = SummaryWriter()\n\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n\n    def collate(examples: List[torch.Tensor]):\n        if tokenizer._pad_token is None:\n            return pad_sequence(examples, batch_first=True)\n        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(\n        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate\n    )\n\n    if args.max_steps > 0:\n        t_total = args.max_steps\n        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n    else:\n        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n\n    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n    model.resize_token_embeddings(len(tokenizer))\n\n    # Prepare optimizer and schedule (linear warmup and decay)\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n    )\n\n    # Check if saved optimizer or scheduler states exist\n    if (\n        args.model_name_or_path\n        and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n        and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n    ):\n        # Load in optimizer and scheduler states\n        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n\n    # multi-gpu training (should be after apex fp16 initialization)\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n\n    # Distributed training (should be after apex fp16 initialization)\n    if args.local_rank != -1:\n        model = torch.nn.parallel.DistributedDataParallel(\n            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n        )\n\n    # Train!\n    print(\"***** Running training *****\")\n    print(\"  Num examples = %d\", len(train_dataset))\n    print(\"  Num Epochs = %d\", args.num_train_epochs)\n    print(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n    print(\n       \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n        args.train_batch_size\n        * args.gradient_accumulation_steps\n        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n    )\n    print(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n    print(\"  Total optimization steps = %d\", t_total)\n    \n  \n      \n    global_step = 0\n    epochs_trained = 0\n    steps_trained_in_current_epoch = 0\n    # Check if continuing training from a checkpoint\n    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n        try:\n            # set global_step to gobal_step of last saved checkpoint from model path\n            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n            global_step = int(checkpoint_suffix)\n            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n\n            print(\"  Continuing training from checkpoint, will skip to saved global_step\")\n            print(\"  Continuing training from epoch %d\", epochs_trained)\n            print(\"  Continuing training from global step %d\", global_step)\n            print(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n        except ValueError:\n            print(\"  Starting fine-tuning.\")\n\n    tr_loss, logging_loss = 0.0, 0.0\n\n    model.zero_grad()\n    train_iterator = trange(\n        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n    )\n    set_seed(args)  # Added here for reproducibility\n    for epoch in train_iterator:\n        #epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=True)\n\n        if args.local_rank != -1:\n            train_sampler.set_epoch(epoch)\n\n        for step, batch in enumerate(epoch_iterator):\n            # Skip past any already trained steps if resuming training\n            if steps_trained_in_current_epoch > 0:\n                steps_trained_in_current_epoch -= 1\n                continue\n\n            inputs, labels = mask_tokens(batch, tokenizer, args) if args.mlm else (batch, batch)\n            inputs = inputs.to(args.device)\n            labels = labels.to(args.device)\n            model.train()\n            outputs = model(inputs, masked_lm_labels=labels) if args.mlm else model(inputs, labels=labels)\n            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n\n            if args.n_gpu > 1:\n                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                if args.fp16:\n                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n                else:\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n                optimizer.step()\n                scheduler.step()  # Update learning rate schedule\n                model.zero_grad()\n                global_step += 1\n\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n                    # Log metrics\n                    if (\n                        args.local_rank == -1 and args.evaluate_during_training\n                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n                        results = evaluate(args, model, tokenizer,f='/dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt')\n                        for key, value in results.items():\n                            #tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n                            print(\"eval_\"+str(key)+\" \"+str(value)+\" \"+str(global_step))\n                    #tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n                    #tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n                    print(\"lr \"+ str(scheduler.get_lr()[0])+\" \"+str(global_step))\n                    print(\"loss \"+str( (tr_loss - logging_loss) / args.logging_steps)+\" \"+str( global_step))\n                    logging_loss = tr_loss\n\n                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n                    checkpoint_prefix = \"checkpoint\"\n                    # Save model checkpoint\n                    output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n                    os.makedirs(output_dir, exist_ok=True)\n                    model_to_save = (\n                        model.module if hasattr(model, \"module\") else model\n                    )  # Take care of distributed/parallel training\n                    model_to_save.save_pretrained(output_dir)\n                    tokenizer.save_pretrained(output_dir)\n\n                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n                    print(\"Saving model checkpoint to %s\", output_dir)\n\n                    _rotate_checkpoints(args, checkpoint_prefix)\n\n                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n                    print(\"Saving optimizer and scheduler states to %s\", output_dir)\n\n            if args.max_steps > 0 and global_step > args.max_steps:\n                epoch_iterator.close()\n                break\n        if args.max_steps > 0 and global_step > args.max_steps:\n            train_iterator.close()\n            break\n\n    #if args.local_rank in [-1, 0]:\n        #tb_writer.close()\n\n    return global_step, tr_loss / global_step\n\n\ndef evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, prefix=\"\",f='') -> Dict:\n    # Loop to handle MNLI double evaluation (matched, mis-matched)\n    eval_output_dir = args.output_dir\n    eval_dataset = load_and_cache_examples(args, tokenizer, evaluate=True,f=f)\n\n    if args.local_rank in [-1, 0]:\n        os.makedirs(eval_output_dir, exist_ok=True)\n\n    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n    # Note that DistributedSampler samples randomly\n\n    def collate(examples: List[torch.Tensor]):\n        if tokenizer._pad_token is None:\n            return pad_sequence(examples, batch_first=True)\n        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n\n    eval_sampler = SequentialSampler(eval_dataset)\n    eval_dataloader = DataLoader(\n        eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate\n    )\n\n    # multi-gpu evaluate\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n\n    # Eval!\n    #print(\"***** Running evaluation {} *****\".format(prefix))\n    #print(\"  Num examples = %d\", len(eval_dataset))\n    #print(\"  Batch size = %d\", args.eval_batch_size)\n    eval_loss = 0.0\n    nb_eval_steps = 0\n    model.eval()\n\n    for batch in tqdm(eval_dataloader, desc=\"Evaluating\",disable=True):\n        inputs, labels = mask_tokens(batch, tokenizer, args) if args.mlm else (batch, batch)\n        inputs = inputs.to(args.device)\n        labels = labels.to(args.device)\n\n        with torch.no_grad():\n            outputs = model(inputs, masked_lm_labels=labels) if args.mlm else model(inputs, labels=labels)\n            lm_loss = outputs[0]\n            eval_loss += lm_loss.mean().item()\n        nb_eval_steps += 1\n\n    eval_loss = eval_loss / nb_eval_steps\n    perplexity = torch.exp(torch.tensor(eval_loss))\n\n    result = {\"perplexity\": perplexity}\n\n    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n    \n    print(\"Perplexity\",perplexity.item())\n            \n    return result"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48b28307-6f32-4419-913b-cdee89abceac"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["class arg_class:\n    def __init__(self):\n        #Input file\n        self.train_data_file = \"/dbfs/mnt/els-nlp-experts1/data/Gizem/sentences_new.txt\"\n        #The output directory where the model predictions and checkpoints will be written.\n        self.output_dir = \"/dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3\"\n        #The model architecture to be trained or fine-tuned.\n        self.model_type = \"bert-base-cased\"\n        #An optional input evaluation data file to evaluate the perplexity on (a text file).\n        self.eval_data_file = \"/dbfs/mnt/els-nlp-experts1/data/Gizem/validation.txt\"\n        self.line_by_line = True\n        self.tokenizer_name = \"bert-base-cased\"\n        self.model_name_or_path =\"bert-base-cased\"\n        self.mlm = True\n        self.do_eval = True\n        self.do_train = True\n        self.evaluate_during_training = True\n        self.seed = 0\n        \n        #??\n        self.num_train_epochs = 2\n        self.learning_rate = 0.0005\n        self.per_gpu_train_batch_size = 4\n        self.per_gpu_eval_batch_size = 16\n        self.gradient_accumulation_steps = 2048/self.per_gpu_train_batch_size #or 256/self.per_gpu_train_batch_size\n        self.logging_steps = 20 #Change this to be at the end of each epoch\n        self.save_steps = 20 #Change this to be at the end of each epoch\n        #??\n        \n        #I won't touch these\n        self.warmup_steps = 0\n        self.weight_decay = 0.0\n        self.adam_epsilon = 1e-8\n        self.max_grad_norm = 1.0\n        self.server_ip = None\n        self.server_port = None\n        self.mlm_probability = 0.15\n        self.should_continue = False\n        self.config_name = None\n        self.cache_dir = None\n        self.block_size = -1\n        self.max_steps = -1\n        self.save_total_limit = None\n        self.eval_all_checkpoints = False\n        self.no_cuda = False\n        self.overwrite_output_dir = False\n        self.overwrite_cache = False\n        self.fp16 = False\n        self.fp16_opt_level = \"O1\"\n        self.local_rank = -1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6889db67-5193-493e-97ad-5e69ea028563"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["args = arg_class()\n\nif (\n    os.path.exists(args.output_dir)\n    and os.listdir(args.output_dir)\n    and args.do_train\n    and not args.overwrite_output_dir\n    and not args.should_continue\n):\n    raise ValueError(\n        \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n            args.output_dir\n        )\n    )\n\ndevice = torch.device('cuda')\nargs.n_gpu=1   \nargs.device = device"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab366756-af78-463e-88d7-0db1afd04ef7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#dbutils.fs.rm(\"dbfs:/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-980/config.json\")\n#dbutils.fs.rm(\"dbfs:/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-980/optimizer.pt\")\n#dbutils.fs.rm(\"dbfs:/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-980/pytorch_model.bin\")\n#dbutils.fs.rm(\"dbfs:/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-980/scheduler.pt\")\n#dbutils.fs.rm(\"dbfs:/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-980/special_tokens_map.json\")\n#dbutils.fs.rm(\"dbfs:/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-980/tokenizer_config.json\")\n#dbutils.fs.rm(\"dbfs:/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-980/training_args.bin\")\n#dbutils.fs.rm(\"dbfs:/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-980/vocab.txt\")\n#dbutils.fs.rm(\"dbfs:/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-980/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c4b83fb1-c7fe-4c5d-b08e-1e60547d98a3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[49]: True</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[49]: True</div>"]}}],"execution_count":0},{"cell_type":"code","source":["#dbutils.fs.cp(\"dbfs:/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3\",\"dbfs:/FileStore/shared_uploads/g.aydxn@elsevier.com/\",recurse=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf1e4707-d686-45b8-8c96-7f54d6221444"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[52]: True</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[52]: True</div>"]}}],"execution_count":0},{"cell_type":"code","source":["#dbutils.fs.rm(\"dbfs:/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt/eval_results.txt\")\n#dbutils.fs.ls(\"dbfs:/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"02453db7-9902-4962-abdc-026323854165"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Setup logging\n#logging.basicConfig(\n#    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n#    datefmt=\"%m/%d/%Y %H:%M:%S\",\n#    level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n#)\n#logger.warning(\n#    \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n#    args.local_rank,\n#    device,\n#    args.n_gpu,\n#    bool(args.local_rank != -1),\n#    args.fp16,\n#)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd1c0833-3632-40e6-b58e-eba2d50b5c85"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Set seed\nset_seed(args)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7dbf672-aa72-49b9-acb5-82427a14a061"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["config = AutoConfig.from_pretrained(\"bert-base-cased\")\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e4600dd-5bdb-41ca-abb3-f4202deaaa6f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["model = AutoModelForMaskedLM.from_pretrained(\n        args.model_name_or_path,\n        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n        config=config,\n        cache_dir=args.cache_dir,\n        )    \nargs.block_size = tokenizer.max_len\nmodel.to(args.device)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a29d4a24-1c2a-4838-8755-28a0eddb58d1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: [&#39;cls.seq_relationship.weight&#39;, &#39;cls.seq_relationship.bias&#39;]\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n  warnings.warn(\nOut[12]: BertForMaskedLM(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (cls): BertOnlyMLMHead(\n    (predictions): BertLMPredictionHead(\n      (transform): BertPredictionHeadTransform(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (decoder): Linear(in_features=768, out_features=28996, bias=True)\n    )\n  )\n)</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: [&#39;cls.seq_relationship.weight&#39;, &#39;cls.seq_relationship.bias&#39;]\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n  warnings.warn(\nOut[12]: BertForMaskedLM(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (cls): BertOnlyMLMHead(\n    (predictions): BertLMPredictionHead(\n      (transform): BertPredictionHeadTransform(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (decoder): Linear(in_features=768, out_features=28996, bias=True)\n    )\n  )\n)</div>"]}}],"execution_count":0},{"cell_type":"code","source":["result = evaluate(args, model, tokenizer,f='/dbfs/mnt/els-nlp-experts1/data/Gizem/validation.txt')\nresult = evaluate(args, model, tokenizer,f='/dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"382b6737-9e3e-43a6-8695-3153571ebaa4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Creating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/validation.txt\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to &#39;longest_first&#39; truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages/transformers/modeling_bert.py:1150: FutureWarning: The `masked_lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\n  warnings.warn(\nPerplexity 14.76291275024414\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 15.499031066894531\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Creating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/validation.txt\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to &#39;longest_first&#39; truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages/transformers/modeling_bert.py:1150: FutureWarning: The `masked_lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\n  warnings.warn(\nPerplexity 14.76291275024414\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 15.499031066894531\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Training\nif args.do_train:\n    #if args.local_rank not in [-1, 0]:\n    #    torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training process the dataset, and the others will use the cache\n\n    train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False)\n\n    if args.local_rank == 0:\n        torch.distributed.barrier()\n\n    global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n    print(\" global_step = %s, average loss = %s\", global_step, tr_loss)#13830960"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"90bf6928-4dad-48ce-90a2-3a7d69546cff"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Creating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/sentences_new.txt\n***** Running training *****\n  Num examples = %d 13831064\n  Num Epochs = %d 2\n  Instantaneous batch size per GPU = %d 4\n  Total train batch size (w. parallel, distributed &amp; accumulation) = %d 2048.0\n  Gradient Accumulation steps = %d 512.0\n  Total optimization steps = %d 13506.0\n\rEpoch:   0%|          | 0/2 [00:00&lt;?, ?it/s]Creating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 6.022308826446533\neval_perplexity tensor(6.0223) 20\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  warnings.warn(&#34;To get the last learning rate computed by the scheduler, &#34;\nlr 0.0004992595883311121 20\nloss 2.6585879570775433 20\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-20\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n  warnings.warn(SAVE_STATE_WARNING, UserWarning)\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-20\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 5.1747050285339355\neval_perplexity tensor(5.1747) 40\nlr 0.0004985191766622241 40\nloss 1.9056920712522696 40\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-40\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-40\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 4.590297698974609\neval_perplexity tensor(4.5903) 60\nlr 0.0004977787649933363 60\nloss 1.7878313557215733 60\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-60\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-60\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 4.365617752075195\neval_perplexity tensor(4.3656) 80\nlr 0.0004970383533244484 80\nloss 1.7272635620684014 80\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-80\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-80\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 4.116343975067139\neval_perplexity tensor(4.1163) 100\nlr 0.0004962979416555605 100\nloss 1.679799532686593 100\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-100\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-100\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 4.124221324920654\neval_perplexity tensor(4.1242) 120\nlr 0.0004955575299866726 120\nloss 1.6490687134384643 120\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-120\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-120\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.8804993629455566\neval_perplexity tensor(3.8805) 140\nlr 0.0004948171183177846 140\nloss 1.6205465500352148 140\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-140\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-140\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.7801008224487305\neval_perplexity tensor(3.7801) 160\nlr 0.0004940767066488968 160\nloss 1.6001250446541235 160\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-160\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-160\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.7706570625305176\neval_perplexity tensor(3.7707) 180\nlr 0.0004933362949800089 180\nloss 1.584085173108906 180\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-180\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-180\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.702629566192627\neval_perplexity tensor(3.7026) 200\nlr 0.0004925958833111209 200\nloss 1.564383143253508 200\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-200\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-200\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.6886751651763916\neval_perplexity tensor(3.6887) 220\nlr 0.0004918554716422331 220\nloss 1.5533296391755358 220\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-220\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-220\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.5339908599853516\neval_perplexity tensor(3.5340) 240\nlr 0.0004911150599733452 240\nloss 1.5470442477584583 240\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-240\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-240\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.525101661682129\neval_perplexity tensor(3.5251) 260\nlr 0.0004903746483044572 260\nloss 1.525618933030637 260\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-260\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-260\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.4585518836975098\neval_perplexity tensor(3.4586) 280\nlr 0.0004896342366355694 280\nloss 1.5143619508053234 280\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-280\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-280\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.4139349460601807\neval_perplexity tensor(3.4139) 300\nlr 0.0004888938249666815 300\nloss 1.5037241909350996 300\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-300\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-300\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.3182458877563477\neval_perplexity tensor(3.3182) 320\nlr 0.00048815341329779357 320\nloss 1.503355248794105 320\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-320\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-320\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.4191389083862305\neval_perplexity tensor(3.4191) 340\nlr 0.0004874130016289057 340\nloss 1.4935408531848224 340\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-340\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-340\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.368321180343628\neval_perplexity tensor(3.3683) 360\nlr 0.0004866725899600178 360\nloss 1.4861436439668978 360\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-360\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-360\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.2735910415649414\neval_perplexity tensor(3.2736) 380\nlr 0.0004859321782911299 380\nloss 1.47870154804732 380\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-380\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-380\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.2918875217437744\neval_perplexity tensor(3.2919) 400\nlr 0.000485191766622242 400\nloss 1.474701182077115 400\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-400\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-400\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.2557945251464844\neval_perplexity tensor(3.2558) 420\nlr 0.0004844513549533541 420\nloss 1.4677116775092145 420\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-420\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-420\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.2813949584960938\neval_perplexity tensor(3.2814) 440\nlr 0.0004837109432844662 440\nloss 1.4558566991639963 440\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-440\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-440\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.171010971069336\neval_perplexity tensor(3.1710) 460\nlr 0.00048297053161557827 460\nloss 1.455482363313422 460\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-460\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-460\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.2680928707122803\neval_perplexity tensor(3.2681) 480\nlr 0.0004822301199466904 480\nloss 1.441578291501355 480\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-480\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-480\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.1509485244750977\neval_perplexity tensor(3.1509) 500\nlr 0.0004814897082778025 500\nloss 1.4346015616101795 500\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-500\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-500\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.296373128890991\neval_perplexity tensor(3.2964) 520\nlr 0.0004807492966089146 520\nloss 1.4444091452562133 520\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-520\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-520\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.092790365219116\neval_perplexity tensor(3.0928) 540\nlr 0.00048000888494002667 540\nloss 1.4381878231753944 540\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-540\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-540\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.065340518951416\neval_perplexity tensor(3.0653) 560\nlr 0.0004792684732711388 560\nloss 1.4347872672224184 560\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-560\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-560\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.0262997150421143\neval_perplexity tensor(3.0263) 580\nlr 0.0004785280616022509 580\nloss 1.42240316576208 580\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-580\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-580\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.0903923511505127\neval_perplexity tensor(3.0904) 600\nlr 0.00047778764993336296 600\nloss 1.4244642962188663 600\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-600\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-600\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.06850004196167\neval_perplexity tensor(3.0685) 620\nlr 0.0004770472382644751 620\nloss 1.4165247204808111 620\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-620\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-620\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.1529042720794678\neval_perplexity tensor(3.1529) 640\nlr 0.0004763068265955872 640\nloss 1.4113325396341678 640\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-640\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-640\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.2151851654052734\neval_perplexity tensor(3.2152) 660\nlr 0.0004755664149266993 660\nloss 1.4084658051760925 660\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-660\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-660\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.0683581829071045\neval_perplexity tensor(3.0684) 680\nlr 0.0004748260032578113 680\nloss 1.4023325211346673 680\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-680\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-680\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.070781707763672\neval_perplexity tensor(3.0708) 700\nlr 0.0004740855915889234 700\nloss 1.4035033558382566 700\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-700\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-700\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.0270354747772217\neval_perplexity tensor(3.0270) 720\nlr 0.00047334517992003554 720\nloss 1.3978967298367935 720\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-720\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-720\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.090230941772461\neval_perplexity tensor(3.0902) 740\nlr 0.0004726047682511476 740\nloss 1.3967991828521917 740\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-740\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-740\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.0108985900878906\neval_perplexity tensor(3.0109) 760\nlr 0.0004718643565822597 760\nloss 1.394712264361442 760\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-760\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-760\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.030783176422119\neval_perplexity tensor(3.0308) 780\nlr 0.0004711239449133718 780\nloss 1.3840310763007437 780\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-780\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-780\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 2.9835622310638428\neval_perplexity tensor(2.9836) 800\nlr 0.00047038353324448394 800\nloss 1.3821711618627888 800\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-800\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-800\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.0043301582336426\neval_perplexity tensor(3.0043) 820\nlr 0.000469643121575596 820\nloss 1.3755088782301754 820\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-820\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-820\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.0231332778930664\neval_perplexity tensor(3.0231) 840\nlr 0.0004689027099067081 840\nloss 1.3760997196746758 840\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-840\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-840\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 2.993345260620117\neval_perplexity tensor(2.9933) 860\nlr 0.00046816229823782023 860\nloss 1.3822694203459833 860\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-860\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-860\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.0109341144561768\neval_perplexity tensor(3.0109) 880\nlr 0.0004674218865689323 880\nloss 1.383748673532682 880\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-880\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-880\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 2.9409992694854736\neval_perplexity tensor(2.9410) 900\nlr 0.0004666814749000444 900\nloss 1.3766208378860028 900\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-900\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-900\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.0201594829559326\neval_perplexity tensor(3.0202) 920\nlr 0.0004659410632311565 920\nloss 1.3609946916058107 920\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-920\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-920\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 2.9200692176818848\neval_perplexity tensor(2.9201) 940\nlr 0.00046520065156226863 940\nloss 1.3680250783014345 940\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-940\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-940\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.0270936489105225\neval_perplexity tensor(3.0271) 960\nlr 0.0004644602398933807 960\nloss 1.3622084711405478 960\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-960\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-960\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 2.8821661472320557\neval_perplexity tensor(2.8822) 980\nlr 0.0004637198282244928 980\nloss 1.3632703307715928 980\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-980\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-980\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 2.8611717224121094\neval_perplexity tensor(2.8612) 1000\nlr 0.0004629794165556049 1000\nloss 1.3638634202539834 1000\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-1000\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-1000\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Creating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/sentences_new.txt\n***** Running training *****\n  Num examples = %d 13831064\n  Num Epochs = %d 2\n  Instantaneous batch size per GPU = %d 4\n  Total train batch size (w. parallel, distributed &amp; accumulation) = %d 2048.0\n  Gradient Accumulation steps = %d 512.0\n  Total optimization steps = %d 13506.0\n\rEpoch:   0%|          | 0/2 [00:00&lt;?, ?it/s]Creating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 6.022308826446533\neval_perplexity tensor(6.0223) 20\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  warnings.warn(&#34;To get the last learning rate computed by the scheduler, &#34;\nlr 0.0004992595883311121 20\nloss 2.6585879570775433 20\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-20\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-56908ec4-9695-4c14-bae3-16d2a4e42ea7/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n  warnings.warn(SAVE_STATE_WARNING, UserWarning)\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-20\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 5.1747050285339355\neval_perplexity tensor(5.1747) 40\nlr 0.0004985191766622241 40\nloss 1.9056920712522696 40\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-40\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-40\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 4.590297698974609\neval_perplexity tensor(4.5903) 60\nlr 0.0004977787649933363 60\nloss 1.7878313557215733 60\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-60\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-60\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 4.365617752075195\neval_perplexity tensor(4.3656) 80\nlr 0.0004970383533244484 80\nloss 1.7272635620684014 80\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-80\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-80\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 4.116343975067139\neval_perplexity tensor(4.1163) 100\nlr 0.0004962979416555605 100\nloss 1.679799532686593 100\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-100\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-100\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 4.124221324920654\neval_perplexity tensor(4.1242) 120\nlr 0.0004955575299866726 120\nloss 1.6490687134384643 120\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-120\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-120\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.8804993629455566\neval_perplexity tensor(3.8805) 140\nlr 0.0004948171183177846 140\nloss 1.6205465500352148 140\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-140\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-140\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.7801008224487305\neval_perplexity tensor(3.7801) 160\nlr 0.0004940767066488968 160\nloss 1.6001250446541235 160\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-160\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-160\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.7706570625305176\neval_perplexity tensor(3.7707) 180\nlr 0.0004933362949800089 180\nloss 1.584085173108906 180\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-180\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-180\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.702629566192627\neval_perplexity tensor(3.7026) 200\nlr 0.0004925958833111209 200\nloss 1.564383143253508 200\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-200\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-200\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.6886751651763916\neval_perplexity tensor(3.6887) 220\nlr 0.0004918554716422331 220\nloss 1.5533296391755358 220\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-220\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-220\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.5339908599853516\neval_perplexity tensor(3.5340) 240\nlr 0.0004911150599733452 240\nloss 1.5470442477584583 240\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-240\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-240\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.525101661682129\neval_perplexity tensor(3.5251) 260\nlr 0.0004903746483044572 260\nloss 1.525618933030637 260\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-260\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-260\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.4585518836975098\neval_perplexity tensor(3.4586) 280\nlr 0.0004896342366355694 280\nloss 1.5143619508053234 280\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-280\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-280\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.4139349460601807\neval_perplexity tensor(3.4139) 300\nlr 0.0004888938249666815 300\nloss 1.5037241909350996 300\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-300\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-300\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.3182458877563477\neval_perplexity tensor(3.3182) 320\nlr 0.00048815341329779357 320\nloss 1.503355248794105 320\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-320\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-320\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.4191389083862305\neval_perplexity tensor(3.4191) 340\nlr 0.0004874130016289057 340\nloss 1.4935408531848224 340\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-340\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-340\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.368321180343628\neval_perplexity tensor(3.3683) 360\nlr 0.0004866725899600178 360\nloss 1.4861436439668978 360\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-360\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-360\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.2735910415649414\neval_perplexity tensor(3.2736) 380\nlr 0.0004859321782911299 380\nloss 1.47870154804732 380\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-380\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-380\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.2918875217437744\neval_perplexity tensor(3.2919) 400\nlr 0.000485191766622242 400\nloss 1.474701182077115 400\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-400\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-400\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.2557945251464844\neval_perplexity tensor(3.2558) 420\nlr 0.0004844513549533541 420\nloss 1.4677116775092145 420\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-420\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-420\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.2813949584960938\neval_perplexity tensor(3.2814) 440\nlr 0.0004837109432844662 440\nloss 1.4558566991639963 440\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-440\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-440\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.171010971069336\neval_perplexity tensor(3.1710) 460\nlr 0.00048297053161557827 460\nloss 1.455482363313422 460\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-460\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-460\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.2680928707122803\neval_perplexity tensor(3.2681) 480\nlr 0.0004822301199466904 480\nloss 1.441578291501355 480\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-480\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-480\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.1509485244750977\neval_perplexity tensor(3.1509) 500\nlr 0.0004814897082778025 500\nloss 1.4346015616101795 500\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-500\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-500\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.296373128890991\neval_perplexity tensor(3.2964) 520\nlr 0.0004807492966089146 520\nloss 1.4444091452562133 520\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-520\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-520\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.092790365219116\neval_perplexity tensor(3.0928) 540\nlr 0.00048000888494002667 540\nloss 1.4381878231753944 540\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-540\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-540\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.065340518951416\neval_perplexity tensor(3.0653) 560\nlr 0.0004792684732711388 560\nloss 1.4347872672224184 560\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-560\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-560\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.0262997150421143\neval_perplexity tensor(3.0263) 580\nlr 0.0004785280616022509 580\nloss 1.42240316576208 580\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-580\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-580\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.0903923511505127\neval_perplexity tensor(3.0904) 600\nlr 0.00047778764993336296 600\nloss 1.4244642962188663 600\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-600\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-600\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.06850004196167\neval_perplexity tensor(3.0685) 620\nlr 0.0004770472382644751 620\nloss 1.4165247204808111 620\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-620\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-620\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.1529042720794678\neval_perplexity tensor(3.1529) 640\nlr 0.0004763068265955872 640\nloss 1.4113325396341678 640\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-640\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-640\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.2151851654052734\neval_perplexity tensor(3.2152) 660\nlr 0.0004755664149266993 660\nloss 1.4084658051760925 660\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-660\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-660\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.0683581829071045\neval_perplexity tensor(3.0684) 680\nlr 0.0004748260032578113 680\nloss 1.4023325211346673 680\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-680\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-680\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.070781707763672\neval_perplexity tensor(3.0708) 700\nlr 0.0004740855915889234 700\nloss 1.4035033558382566 700\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-700\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-700\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.0270354747772217\neval_perplexity tensor(3.0270) 720\nlr 0.00047334517992003554 720\nloss 1.3978967298367935 720\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-720\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-720\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.090230941772461\neval_perplexity tensor(3.0902) 740\nlr 0.0004726047682511476 740\nloss 1.3967991828521917 740\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-740\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-740\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.0108985900878906\neval_perplexity tensor(3.0109) 760\nlr 0.0004718643565822597 760\nloss 1.394712264361442 760\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-760\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-760\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.030783176422119\neval_perplexity tensor(3.0308) 780\nlr 0.0004711239449133718 780\nloss 1.3840310763007437 780\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-780\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-780\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 2.9835622310638428\neval_perplexity tensor(2.9836) 800\nlr 0.00047038353324448394 800\nloss 1.3821711618627888 800\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-800\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-800\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.0043301582336426\neval_perplexity tensor(3.0043) 820\nlr 0.000469643121575596 820\nloss 1.3755088782301754 820\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-820\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-820\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.0231332778930664\neval_perplexity tensor(3.0231) 840\nlr 0.0004689027099067081 840\nloss 1.3760997196746758 840\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-840\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-840\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 2.993345260620117\neval_perplexity tensor(2.9933) 860\nlr 0.00046816229823782023 860\nloss 1.3822694203459833 860\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-860\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-860\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.0109341144561768\neval_perplexity tensor(3.0109) 880\nlr 0.0004674218865689323 880\nloss 1.383748673532682 880\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-880\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-880\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 2.9409992694854736\neval_perplexity tensor(2.9410) 900\nlr 0.0004666814749000444 900\nloss 1.3766208378860028 900\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-900\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-900\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.0201594829559326\neval_perplexity tensor(3.0202) 920\nlr 0.0004659410632311565 920\nloss 1.3609946916058107 920\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-920\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-920\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 2.9200692176818848\neval_perplexity tensor(2.9201) 940\nlr 0.00046520065156226863 940\nloss 1.3680250783014345 940\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-940\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-940\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 3.0270936489105225\neval_perplexity tensor(3.0271) 960\nlr 0.0004644602398933807 960\nloss 1.3622084711405478 960\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-960\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-960\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 2.8821661472320557\neval_perplexity tensor(2.8822) 980\nlr 0.0004637198282244928 980\nloss 1.3632703307715928 980\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-980\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-980\nCreating features from dataset file at %s /dbfs/mnt/els-nlp-experts1/data/Gizem/monitor.txt\nPerplexity 2.8611717224121094\neval_perplexity tensor(2.8612) 1000\nlr 0.0004629794165556049 1000\nloss 1.3638634202539834 1000\nSaving model checkpoint to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-1000\nSaving optimizer and scheduler states to %s /dbfs/mnt/els-nlp-experts1/data/Gizem/bert-base-cased-tapt3/checkpoint-1000\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Cancelled","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Pretrain BERT MoreSamples","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3243931}},"nbformat":4,"nbformat_minor":0}
