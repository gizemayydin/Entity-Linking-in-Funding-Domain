{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs\n",
    "\n",
    "1. The input to this model are sentences. You can either input them from a file or manually. To input them from a file, write the input sentences line by line in a file called `sentences.txt`. The codeblock below (can be found on second cell as well) reads this file.\n",
    "```python\n",
    "#Input sentences from file\n",
    "file_path = \"sentences.txt\"\n",
    "with open(file_path,'r',encoding='utf-8') as f:\n",
    "         sentences = [[sent] for sent in f.read().splitlines()]\n",
    "```\n",
    "If you would like to input the sentences manually, uncomment the following block from the second cell and comment the codeblock mentioned above.\n",
    "```python\n",
    "#Input sentences manually.\n",
    "sentences = [[\"Sentence 1\"],\n",
    "                [\"Sentence 2\"],\n",
    "                [\"Sentence 3\"],\n",
    "                [\"...\"]]\n",
    "```\n",
    "2. `path` variable should point out to the path that contains all required files:\n",
    "    * `hardneg_m_4.pt`, `hardneg_ctxt_model_4.pt`: Linear scoring layer  and mention encoder for the candidate selector.\n",
    "    * `entity_embeds_4.pkl`: Entity Embeddings computed with the latest entity encoder.\n",
    "    * `entities.pkl`: See `ED For Funding Organizations/Train GBM Reranker.ipynb`\n",
    "    * `link_prob.json`, `commonness.json`, `popularity.json`: See `ED For Funding Organizations/Train GBM Reranker.ipynb`\n",
    "    * `lgbm12.pkl`: The raranker model\n",
    "    * `BERT_SC_NER_Model.pt`: The NER model\n",
    "\n",
    "3. Input batch sizes and the device to be used.\n",
    "\n",
    "# Output\n",
    "\n",
    "The predictions are stored in the dataframe `df_sent_new`. Important columns:\n",
    "* `Sentence`: The input sentence\t\n",
    "* `Sentence_Tokenized`: Tokenized version of the input sentence\n",
    "* `Token_Spans`: List of tuples, length is equal to number of tokens. For each token, the tuple contains start and end character of the token.\n",
    "* `Preds`:\tList of strings, predicted NER tag for each token.\n",
    "* `Probs`:\tList of dictionaries, NER tag probability distribution for each token.\n",
    "* `Entities`: Annotations for each sentence. Each annotation is contained in a dictionary:\n",
    "    * `Mention`: Mention extracted by the NER component.\n",
    "    * `Type`: Type of mention. 'ORG' for organization and 'GRT' for grant.\n",
    "    * `Character_Spans`: Spans of mention in terms of character index.\n",
    "    * `Token_Spans`: Spans of mention in terms of tokens\n",
    "    * `Link_to_Fundref`: Linked FundRef entity ID. None if NIL mention, \"-\" if non-applicable (e.g. for grant mentions)\n",
    "    * `Link_Confidence`: Probability assigned by the reranker, \"-\" if non-applicable (e.g. for grant mentions and NIL mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertModel\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import torch\n",
    "from seqeval.metrics.sequence_labeling import get_entities\n",
    "from NERFunctions import *\n",
    "from EDFunctions import *\n",
    "import random\n",
    "import time\n",
    "from fuzzywuzzy import fuzz\n",
    "import pickle\n",
    "import json\n",
    "import lightgbm as lgb\n",
    "import annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############INPUTS################\n",
    "#path to model\n",
    "path = \"\"\n",
    "\n",
    "#Batch sizes\n",
    "ner_batch_size =16\n",
    "ctxt_batch_size= 32\n",
    "#'cuda' for gpu 'cpu' for cpu\n",
    "device_str = 'cuda'\n",
    "#ED Default threshold\n",
    "threshold = 0.042\n",
    "\n",
    "\n",
    "### INPUT SENTENCES ###\n",
    "#Input sentences from file\n",
    "with open(\"sentences.txt\",'r',encoding='utf-8') as f:\n",
    "    sentences = [[sent] for sent in f.read().splitlines()]\n",
    "#Input sentences manually.\n",
    "#sentences = [[\"Sentence 1\"],\n",
    "#             [\"Sentence 2\"],\n",
    "#             [\"Sentence 3\"],\n",
    "#             [\"...\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############VARIABLES################\n",
    "device = torch.device(device_str)\n",
    "#Path to NER model\n",
    "model = torch.load(path+'BERT_SC_NER_Model.pt')\n",
    "#Load tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
    "#Define tag ids\n",
    "id2tag = {0: 'I-GRT', 1: 'O', 2: 'B-GRT', 3: 'B-ORG', 4: 'I-ORG'}\n",
    "\n",
    "#Load files\n",
    "with open(path+'commonness.json','r',encoding='utf-8') as f:\n",
    "    commonness = json.load(f)\n",
    "with open(path+'link_prob.json','r',encoding='utf-8') as f:\n",
    "    link_probability = json.load(f)\n",
    "with open(path+'popularity.json','r',encoding='utf-8') as f:\n",
    "    popularity = json.load(f)\n",
    "with open(path+'lgbm12.pkl','rb') as f:\n",
    "    model_lgb = pickle.load(f)\n",
    "with open(path+'entity_embeds_4.pkl',\"rb\") as f:\n",
    "    entity_emebeddings=pickle.load(f)\n",
    "with open(path+'entities.pkl','rb') as f:\n",
    "    entity_labels=pickle.load(f)\n",
    "ctxt_model = torch.load(path+'hardneg_ctxt_model_4.pt').to(device_str)\n",
    "m = torch.load(path+'hardneg_m_4.pt').to('cpu')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "ctxt_model.eval()\n",
    "m.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mention Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########PROCESS DATASET##########\n",
    "#Create a dataframe with the input sentences.\n",
    "df_sent = pd.DataFrame(sentences,columns=['Sentence'])\n",
    "df_sent['Start_Idx'] = 0\n",
    "df_sent['End_Idx'] = [len(x) for x in df_sent.Sentence.values]\n",
    "df_sent['ID'] = None\n",
    "\n",
    "#This variable will contain the dataset with long sentences\n",
    "df_sent_withlong = df_sent.copy(deep=True)\n",
    "\n",
    "#Add BERT tokenization to the dataset to prepare input\n",
    "res = tokenize_input_bert(df_sent_withlong,'Sentence',tokenizer)\n",
    "df_sent_withlong['Sentence_Tokenized'] = res[0]\n",
    "df_sent_withlong['Token_Spans'] = res[1]\n",
    "#Token encodings does not have [CLS] an [SEP at the moment]\n",
    "df_sent_withlong['Token_Encoding'] = res[2]\n",
    "\n",
    "\n",
    "#Define max token length (512-2=510)\n",
    "#Split big sentences\n",
    "#df_sent -> big sentences splitted\n",
    "#df_sent_withlong -> original\n",
    "max_len = 510\n",
    "df_sent, too_long_df_sent =split_long_sentences_old(df_sent_withlong,max_len)\n",
    "\n",
    "#Prepare \"df_sent\" for the model\n",
    "max_len = 512\n",
    "df_sent_texts = df_sent['Sentence_Tokenized'].values\n",
    "df_sent_encodings = df_sent['Token_Encoding'].values\n",
    "df_sent_seq_lens = df_sent['Token_Encoding'].apply(lambda x: len(x)+2)\n",
    "#This is to distinguish the wordpieces\n",
    "df_sent_labels = get_labels(df_sent)\n",
    "df_sent_labels = add_and_pad(df_sent_labels,max_len,-1,-1,-1)\n",
    "df_sent_encodings = add_and_pad(df_sent_encodings,max_len,101,102,0)\n",
    "df_sent_attention_mask = [[0 if num==0 else 1 for num in lst]  for lst in df_sent_encodings]\n",
    "\n",
    "#Create datasets\n",
    "sent_dataset = FB_Dataset(df_sent_encodings, df_sent_labels,df_sent_attention_mask,df_sent_seq_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########PREDICT NER##########\n",
    "model.eval()\n",
    "model.to(device)\n",
    "data_loader = DataLoader(sent_dataset, batch_size=ner_batch_size, shuffle=False)\n",
    "print('Getting Predictions...')\n",
    "with torch.no_grad():\n",
    "    preds = np.zeros((0,512,5))\n",
    "    #Loop over minibatches\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        print(i,\"/\",len(data_loader))\n",
    "        #Get the max length in this batch and crop based on that\n",
    "        seq_lens = batch['seq_len']\n",
    "        max_len_for_batch = max(seq_lens.cpu().detach().numpy())\n",
    "        #Get inputs and labels for that batch and crop\n",
    "        input_ids_ = torch.tensor(batch['input_ids'][:,:max_len_for_batch].detach().numpy()).to(device)\n",
    "        attention_mask_ = torch.tensor(batch['attention_mask'][:,:max_len_for_batch].detach().numpy()).to(device)\n",
    "        #Do a forward pass\n",
    "        outputs = model(input_ids_, attention_mask=attention_mask_)\n",
    "        #Save the predictions\n",
    "        these_preds = outputs[0].cpu().detach().numpy()\n",
    "        #Pad the predictions again\n",
    "        new_preds = np.ones((len(input_ids_),512,5)) * -100\n",
    "        new_preds[:,:max_len_for_batch,:] = these_preds\n",
    "        preds = np.concatenate([preds,new_preds],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############PREPROCESS THE OUTPUT################\n",
    "taglist = ['I-GRT','O','B-GRT', 'B-ORG',  'I-ORG']\n",
    "#Get softmax of the logits\n",
    "preds=softmax(preds,axis=2)\n",
    "#Get predicted label index\n",
    "preds_labels = np.argmax(preds,axis=2)\n",
    "#Get the probability of the predicted label\n",
    "preds_probs = []\n",
    "for row in preds:\n",
    "    new_row =[]\n",
    "    for tok in row:\n",
    "        new_row.append(dict(zip(taglist,np.round(tok,4))))\n",
    "    preds_probs.append(new_row)\n",
    "#Get predicted label and discard special tokens\n",
    "preds_tagged = []\n",
    "preds_probs_tagged = []\n",
    "for i in range(len(df_sent_labels)):\n",
    "    lbl = df_sent_labels[i]\n",
    "    preds_ = preds_labels[i]\n",
    "    probs_ = preds_probs[i]\n",
    "    new_preds = []\n",
    "    new_probs = []\n",
    "    for j in range(len(lbl)):\n",
    "        enc = lbl[j]\n",
    "        pred = preds_[j]\n",
    "        prob = probs_[j]\n",
    "        if enc != -1:\n",
    "            new_preds.append(id2tag[pred])\n",
    "            new_probs.append(prob)\n",
    "    preds_tagged.append(new_preds)\n",
    "    preds_probs_tagged.append(new_probs)\n",
    "df_sent['Preds'] = preds_tagged\n",
    "df_sent['Probs'] = preds_probs_tagged\n",
    "\n",
    "\n",
    "#Part of validation without the split sentences\n",
    "df_sent_ok = df_sent[df_sent.index<(len(df_sent_withlong)-len(too_long_df_sent))].copy(deep=True)\n",
    "#Part of validation with the split sentences\n",
    "df_sent_merge = df_sent[df_sent.index>=(len(df_sent_withlong)-len(too_long_df_sent))]\n",
    "\n",
    "#Get the merged predictions for the split sentences\n",
    "preds = df_sent_merge.groupby('ID').Preds.apply(sum)\n",
    "probs = df_sent_merge.groupby('ID').Probs.apply(sum)\n",
    "\n",
    "#Extract the part that we will paste (these are the long sentences)\n",
    "to_be_pasted = df_sent_withlong[df_sent_withlong.index.isin(too_long_df_sent)].copy(deep=True)\n",
    "\n",
    "#Append predictions for the long sentences\n",
    "new_preds = []\n",
    "new_probs = []\n",
    "for index, row in to_be_pasted.iterrows():\n",
    "    new_preds.append(preds[index])\n",
    "    new_probs.append(probs[index])\n",
    "to_be_pasted['Preds'] = new_preds\n",
    "to_be_pasted['Probs'] = new_probs\n",
    "\n",
    "#Construct the new validation set by merging them\n",
    "df_sent_new = pd.concat([df_sent_ok,to_be_pasted])\n",
    "\n",
    "#Make sure we did not miss anything\n",
    "print(df_sent_new.shape[0] == df_sent_withlong.shape[0])\n",
    "\n",
    "df_sent_new.reset_index(drop=True,inplace=True)\n",
    "\n",
    "df_sent_new.drop(['Token_Encoding','ID'],axis=1,inplace=True)\n",
    "\n",
    "#Get the tokenized words (wordpieces alltogether)\n",
    "res = tokenize_with_bert(df_sent_new,tokenizer)\n",
    "df_sent_new['Sentence_Tokenized'] = res[0]\n",
    "df_sent_new['Token_Spans'] = res[1]\n",
    "\n",
    "##### Extract Named Entities with Spans ####\n",
    "entities = []\n",
    "ann_number = 0\n",
    "for index, row in df_sent_new.iterrows():\n",
    "    these_entities = []\n",
    "    extracted= get_entities(row['Preds'])\n",
    "    for item in extracted:\n",
    "        item_dict = dict()\n",
    "        item_dict['Mention'] = row['Sentence'][int(row['Token_Spans'][item[1]][0]):int(row['Token_Spans'][item[2]][1])]\n",
    "        item_dict['Type'] = item[0]\n",
    "        item_dict['Character_Spans'] = (row['Token_Spans'][item[1]][0],row['Token_Spans'][item[2]][1])\n",
    "        item_dict['Token_Spans'] = (item[1],item[2]+1)\n",
    "        item_dict['Annotation_ID'] = ann_number\n",
    "        ann_number +=1\n",
    "        these_entities.append(item_dict)\n",
    "    entities.append(these_entities)\n",
    "df_sent_new['Entities'] = entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sent_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare for ED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_input = []\n",
    "#Loop over inputs\n",
    "for index, row in df_sent_new.iterrows():\n",
    "    #Loop over extracted mentions\n",
    "    for ann in row['Entities']:\n",
    "        #If the mention is an organization, we perform linking.\n",
    "        if ann['Type'] == 'ORG':\n",
    "            ed_sample = dict()\n",
    "            ed_sample['Annotation_ID'] = ann['Annotation_ID']\n",
    "            ed_sample['mention'] = ann['Mention']\n",
    "            ed_sample['context_left'] = row['Sentence'][0:int(ann['Character_Spans'][0])]\n",
    "            ed_sample['context_right'] = row['Sentence'][int(ann['Character_Spans'][1]):]\n",
    "            ed_input.append(ed_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_input[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Candidate Selection with Biencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataset\n",
    "train_data, train_tensor_data = process_mention_data_2(ed_input,tokenizer)\n",
    "\n",
    "train_sampler = SequentialSampler(train_tensor_data)\n",
    "train_dataloader = DataLoader(train_tensor_data, sampler=train_sampler, batch_size=ctxt_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get mention embeddings\n",
    "ctxt_model.eval()\n",
    "print(len(train_dataloader))\n",
    "mention_embeddings = []\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    for step, context_input in enumerate(train_dataloader):\n",
    "        if step%10==0:\n",
    "            print(\"Step: \",step,\" \",time.time()-start)\n",
    "        context_input = context_input[0]\n",
    "        this_batch= context_input.size(0)\n",
    "        ctxt_rep = ctxt_model(context_input.to(device))[0][:,0,:]\n",
    "        for i in range(this_batch):\n",
    "            mention_embeddings.append(ctxt_rep[i].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build annoy index for nearest neighbor search\n",
    "#Param for positive class\n",
    "m_second_param = list(m.parameters())[0][1].detach().numpy()\n",
    "\n",
    "entity_emebeddings_with_m = dict()\n",
    "keys_map = dict()\n",
    "ctr = 0\n",
    "for k,v in entity_emebeddings.items():\n",
    "    entity_emebeddings_with_m[ctr] = np.multiply(m_second_param,v)\n",
    "    keys_map[ctr] = k\n",
    "    ctr+=1\n",
    "    \n",
    "t = annoy.AnnoyIndex(768, 'dot') \n",
    "\n",
    "t.set_seed(0)\n",
    "\n",
    "for k,v in entity_emebeddings_with_m.items():\n",
    "    t.add_item(k, v)\n",
    "t.build(1000, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get top 12 candidates\n",
    "num_cands=12\n",
    "\n",
    "cands = []\n",
    "start = time.time()\n",
    "#Loop over mentions \n",
    "for i in range(len(mention_embeddings)):\n",
    "    if i%100 == 0:\n",
    "        print(i, \" \",time.time()-start)\n",
    "    #Get the mention embedding\n",
    "    this_ment_embed = mention_embeddings[i]\n",
    "    \n",
    "    \n",
    "    #Now we get the top num_hard_negs predictions\n",
    "    res = t.get_nns_by_vector(this_ment_embed, num_cands, search_k=len(entity_emebeddings_with_m), include_distances=True)\n",
    "    #Store entities and scores\n",
    "    #Score = -dot\n",
    "    returned_entities = [keys_map[x] for x in res[0]]\n",
    "    scores = [1- 1/(1 + np.exp(x)) for x in res[1]]\n",
    "    merged = list(zip(scores,returned_entities))\n",
    "    #Sort returned instances\n",
    "    merged.sort(key=lambda tup: tup[0],reverse=True) \n",
    "\n",
    "    cands.append(merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reranking with GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get LGBM preds\n",
    "bert_scores = []\n",
    "fw_scores2 = []\n",
    "unique_id = []\n",
    "entity_ = []\n",
    "commonness_ = []\n",
    "popularity_ = []\n",
    "link_probability_ = []\n",
    "\n",
    "for i in range(len(ed_input)):\n",
    "    candidates = cands[i]\n",
    "    this_mention = ed_input[i]['mention']\n",
    "    for j in range(num_cands):\n",
    "        #Get FW score\n",
    "        this_ent_labels = entity_labels[str(candidates[j][1])]['Labels']\n",
    "        \n",
    "        fw_score2 = 0\n",
    "        for lbl in this_ent_labels:\n",
    "            fw_score2 = max(fw_score2,fuzz.token_sort_ratio(this_mention,lbl)/100)\n",
    "        fw_scores2.append(fw_score2)\n",
    "        \n",
    "        #Get BERT score\n",
    "        bert_scores.append(candidates[j][0])\n",
    "        \n",
    "        commonness_.append(commonness.get(this_mention.lower(),{}).get(str(candidates[j][1]),0.))\n",
    "        popularity_.append(popularity.get(str(candidates[j][1]),0.))\n",
    "        link_probability_.append(link_probability.get(this_mention.lower(),0.))\n",
    "        \n",
    "        \n",
    "        unique_id.append(i)\n",
    "        entity_.append(candidates[j][1])\n",
    "df=pd.DataFrame({'ID':unique_id,'Commonness':commonness_,'BERT':bert_scores,\n",
    "                 'Popularity':popularity_,'Link_Probability':link_probability_,\n",
    "                 'FW2':fw_scores2,'Entity':entity_})\n",
    "preds = model_lgb.predict(df[['Commonness', 'BERT', 'FW2','Popularity' ,'Link_Probability']]) \n",
    "\n",
    "df['Score'] = preds\n",
    "temp = df.copy(deep=True)\n",
    "temp=temp.loc[temp.groupby('ID').Score.idxmax().values][['ID','Score','Entity']]\n",
    "#Get Entities\n",
    "entities = temp.Entity.values\n",
    "#Get scores\n",
    "scores = temp.Score.values\n",
    "#Apply threshold\n",
    "temp['Entity']=get_thresholded_preds(threshold,entities,scores)\n",
    "temp.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_dict = dict()\n",
    "for index, row in temp.iterrows():\n",
    "    annotation_dict[int(ed_input[index]['Annotation_ID'])] = (row['Entity'],row['Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_entities = []\n",
    "for index, row in df_sent_new.iterrows():\n",
    "    this_new_entities = []\n",
    "    for ann in row['Entities']:\n",
    "        res = annotation_dict.get(int(ann['Annotation_ID']),('-','-'))\n",
    "        ann['Link_to_Fundref'] = res[0]\n",
    "        if res[0] == 'None':\n",
    "            ann['Link_Confidence'] = \"-\"\n",
    "        else:\n",
    "            ann['Link_Confidence'] = res[1]\n",
    "        this_new_entities.append(ann)\n",
    "    new_entities.append(this_new_entities)\n",
    "df_sent_new['Entities'] = new_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sent_new.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
