{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs\n",
    "See third cell. Most important inputs are:\n",
    "* `INP_PATH`: A path.\n",
    "    * Resulting models are saved here.\n",
    "    * Old models should be here.\n",
    "    * `entity_pool.pkl` should be here\n",
    "    * `train.jsonl` and `monitor.jsonl` should be here.\n",
    "    * `entity_representations.pkl` should be here.\n",
    "* `base_bert_model_ctxt`: Name of the context model of previous round\n",
    "* `base_bert_model_cand`: Name of the entity model of previous round\n",
    "* `base_bert_model_m`: Name of the linear layer of previous round\n",
    "* `num_random_neg_cands`: Number of random negative candidate entities to sample per mention\n",
    "* `hard_neg_cands_train_path`: Path to hard negatives for training set (output of `Hard Negative Mining.ipynb` for training set) \n",
    "* `hard_neg_cands_monitor_path`: Path to hard negatives for dev set (output of `Hard Negative Mining.ipynb` for dev set)\n",
    "* `ROUND_NUMBER`: Which training round is this? (2, 3, 4)\n",
    "The rest of the inputs are explained in the comments and no change is required.\n",
    "\n",
    "# Outputs\n",
    "Trained models:\n",
    "* \"hardnge_ctxt_model_ROUND_NUMBER.pt\"\n",
    "* \"hardnge_cand_model_ROUND_NUMBER.pt\"\n",
    "* \"hardnge_m_ROUND_NUMBER.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3deaf16c-e86d-4da6-b57d-602d78823eeb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install transformers==3.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1fcc29e0-0ed5-435a-a214-29b3156529f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import get_linear_schedule_with_warmup,BertTokenizerFast, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "59074b5d-b6fd-423c-b0dc-3eee5fa50edd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "INP_PATH = \"\"\n",
    "#Seed\n",
    "seed = 0\n",
    "#Cuda or cpu\n",
    "device = 'cuda'\n",
    "#See above\n",
    "base_bert_model_ctxt = INP_PATH+\"randomneg_ctxt_model_1.pt\"\n",
    "base_bert_model_cand = INP_PATH+\"randomneg_cand_model_1.pt\"\n",
    "base_m = INP_PATH+\"randomneg_m_1.pt\"\n",
    "hard_neg_cands_train_path = INP_PATH+\"train_hardnegs_1.pkl\"\n",
    "hard_neg_cands_valid_path =  INP_PATH+\"dev_hardnegs_1.pkl\"\n",
    "#Max length of mention context. Default: 64\n",
    "max_context_length= 64\n",
    "#Max length of entity representation. Default: 256\n",
    "max_cand_length = 256\n",
    "#Batch size for training. Default:16\n",
    "train_batch_size = 16\n",
    "#Number of epochs. Default:2\n",
    "num_train_epochs=2\n",
    "#Batch size for evaluation. Default:256\n",
    "eval_batch_size=256\n",
    "#Gradient accumulation steps. Effective batch size=train_batch_sizexgrad_acc_steps. Default:4\n",
    "grad_acc_steps=4\n",
    "#Number of random negative candidates to sample per mention\n",
    "num_random_neg_cands=2\n",
    "#Value to normalize the gradients to. Default:1.0\n",
    "grad_norm = 1.0\n",
    "#Training round\n",
    "ROUND_NUMBER = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fname = INP_PATH+\"train.jsonl\"\n",
    "monitor_fname = INP_PATH+\"dev.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fd135806-4ac0-408e-b858-8d0c95790ab1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##FROM THE BLINK REPO##\n",
    "ENT_START_TAG = \"[unused0]\"\n",
    "ENT_END_TAG = \"[unused1]\"\n",
    "\n",
    "def select_field(data, key1, key2=None):\n",
    "    if key2 is None:\n",
    "        return [example[key1] for example in data]\n",
    "    else:\n",
    "        return [example[key1][key2] for example in data]\n",
    "\n",
    "def get_context_representation(\n",
    "    sample,\n",
    "    tokenizer,\n",
    "    max_seq_length,\n",
    "    mention_key=\"mention\",\n",
    "    context_key=\"context\",\n",
    "    ent_start_token=ENT_START_TAG,\n",
    "    ent_end_token=ENT_END_TAG,\n",
    "):\n",
    "    # mention_tokens = [Ms] mention [Me]\n",
    "    mention_tokens = []\n",
    "    if sample[mention_key] and len(sample[mention_key]) > 0:\n",
    "        mention_tokens = tokenizer.tokenize(sample[mention_key])\n",
    "        mention_tokens = [ent_start_token] + mention_tokens + [ent_end_token]\n",
    "\n",
    "    context_left = sample[context_key + \"_left\"]\n",
    "    context_right = sample[context_key + \"_right\"]\n",
    "    context_left = tokenizer.tokenize(context_left)\n",
    "    context_right = tokenizer.tokenize(context_right)\n",
    "\n",
    "    left_quota = (max_seq_length - len(mention_tokens)) // 2 - 1\n",
    "    right_quota = max_seq_length - len(mention_tokens) - left_quota - 2\n",
    "    left_add = len(context_left)\n",
    "    right_add = len(context_right)\n",
    "    if left_add <= left_quota:\n",
    "        if right_add > right_quota:\n",
    "            right_quota += left_quota - left_add\n",
    "    else:\n",
    "        if right_add <= right_quota:\n",
    "            left_quota += right_quota - right_add\n",
    "    \n",
    "    context_tokens = (\n",
    "        context_left[-left_quota:] + mention_tokens + context_right[:right_quota]\n",
    "    )\n",
    "    \n",
    "    # mention_tokens = [CLS] left context [Ms] mention [Me] right context [SEP]\n",
    "    context_tokens = [\"[CLS]\"] + context_tokens + [\"[SEP]\"]\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(context_tokens)\n",
    "    padding = [0] * (max_seq_length - len(input_ids))\n",
    "    input_ids += padding\n",
    "    assert len(input_ids) == max_seq_length\n",
    "\n",
    "    return {\n",
    "        \"tokens\": context_tokens,\n",
    "        \"ids\": input_ids,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_candidate_representation(label_idx):\n",
    "\n",
    "    cand_tokens = entity_dict[str(label_idx)]['tokens']\n",
    "    input_ids = entity_dict[str(label_idx)]['ids']\n",
    "    \n",
    "    return {\n",
    "        \"tokens\": cand_tokens,\n",
    "        \"ids\": input_ids,\n",
    "    }\n",
    "  \n",
    "def to_bert_input(token_idx,dev_name):\n",
    "    \"\"\" token_idx is a 2D tensor int.\n",
    "        return token_idx, segment_idx and mask\n",
    "    \"\"\"\n",
    "    segment_idx = None\n",
    "    mask = token_idx != 0\n",
    "    if dev_name =='cuda':\n",
    "        segment_idx = torch.cuda.LongTensor(token_idx * 0)\n",
    "        mask = torch.cuda.LongTensor(mask.long())\n",
    "    else:\n",
    "        segment_idx = torch.LongTensor(token_idx * 0)\n",
    "        mask = torch.LongTensor(mask.long())    \n",
    "    return token_idx, segment_idx, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "99415d2c-1e8d-4867-8ff2-659b99428c09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_mention_data(\n",
    "    samples,\n",
    "    tokenizer,\n",
    "    max_context_length,\n",
    "    max_cand_length,\n",
    "    mention_key=\"mention\",\n",
    "    context_key=\"context\",\n",
    "    ent_start_token=ENT_START_TAG,\n",
    "    ent_end_token=ENT_END_TAG\n",
    "):\n",
    "    processed_samples = []\n",
    "    iter_ = samples\n",
    "    all_samples = []\n",
    "\n",
    "    for idx, sample in enumerate(iter_):\n",
    "        context_tokens = get_context_representation(\n",
    "            sample,\n",
    "            tokenizer,\n",
    "            max_context_length,\n",
    "            mention_key,\n",
    "            context_key,\n",
    "            ent_start_token,\n",
    "            ent_end_token,\n",
    "        )\n",
    "        \n",
    "        if sample[\"label_id\"] is None:\n",
    "          #NIL mention\n",
    "          pass\n",
    "        else:\n",
    "            label_idx = int(sample[\"label_id\"])\n",
    "            label_tokens = get_candidate_representation(label_idx)\n",
    "            \n",
    "            record = {\n",
    "                \"context\": context_tokens,\n",
    "                \"label\": label_tokens,\n",
    "                \"label_idx\": 1,\n",
    "                \"sample\":sample\n",
    "            }\n",
    "            processed_samples.append(record)\n",
    "            all_samples.append(sample)\n",
    "        \n",
    "        for label_idx in sample[\"negative_cands\"]:\n",
    "            label_tokens = get_candidate_representation(label_idx)\n",
    "            record = {\n",
    "                \"context\": context_tokens,\n",
    "                \"label\": label_tokens,\n",
    "                \"label_idx\": 0,\n",
    "                \"sample\":sample\n",
    "            }\n",
    "            processed_samples.append(record)\n",
    "            all_samples.append(sample)\n",
    "        \n",
    "    context_vecs = torch.tensor(\n",
    "        select_field(processed_samples, \"context\", \"ids\"), dtype=torch.long,\n",
    "    )\n",
    "    cand_vecs = torch.tensor(\n",
    "        select_field(processed_samples, \"label\", \"ids\"), dtype=torch.long,\n",
    "    )\n",
    "    label_idx = torch.tensor(\n",
    "        select_field(processed_samples, \"label_idx\"), dtype=torch.long,\n",
    "    )\n",
    "    data = {\n",
    "        \"context_vecs\": context_vecs,\n",
    "        \"cand_vecs\": cand_vecs,\n",
    "        \"label_idx\": label_idx,\n",
    "        \"sample\":all_samples\n",
    "    }\n",
    "\n",
    "    tensor_data = TensorDataset(context_vecs, cand_vecs, label_idx)\n",
    "    return data, tensor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f7210949-33bc-4492-9a74-65abf8b095ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#keys are string\n",
    "with open(INP_PATH+'entity_representations.pkl','rb') as f:\n",
    "    entity_dict=pickle.load(f)\n",
    "with open(hard_neg_cands_train_path,'rb') as f:\n",
    "    hard_neg_cands_train=pickle.load(f)\n",
    "with open(hard_neg_cands_valid_path,'rb') as f:\n",
    "    hard_neg_cands_valid=pickle.load(f)\n",
    "    \n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d61f7cd0-4299-4ed4-ba36-7bf12393cea8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ctxt_model = torch.load(base_bert_model_ctxt).to(device)\n",
    "cand_model = torch.load(base_bert_model_cand).to(device)\n",
    "m = torch.load(base_m).to(device)\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "929a9bde-f2db-47e1-9cee-30ea1f225512",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load train data \n",
    "train_samples = []\n",
    "with open(train_fname, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        train_samples.append(json.loads(line.strip()))\n",
    "print(len(train_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f44858d9-88af-4c2a-bb39-e7d5f7decf4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open(INP_PATH+'entity_pool.pkl','rb') as f:\n",
    "    entity_pool=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3796412e-2b7b-48fe-86fb-d7a255bf3423",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(train_samples)):\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "    \n",
    "    #We will store the negative samples for this entity here\n",
    "    neg_samples = []\n",
    "    #Get hard negatives\n",
    "    hard_neg_cands = hard_neg_cands_train[i]\n",
    "    #Sample from all entities\n",
    "    e_ids = list(entity_dict.keys())\n",
    "    #Do not sample the correct entity\n",
    "    if train_samples[i]['label_id'] is not None:\n",
    "        for e_id in entity_pool[train_samples[i]['label_id']]:\n",
    "            e_ids.remove(e_id)\n",
    "    #Add hard negatives to the negative samples\n",
    "    neg_samples += hard_neg_cands\n",
    "    \n",
    "    #Remove the hard entity IDs so we do not sample them randomly\n",
    "    for e_id in neg_samples:\n",
    "        e_ids.remove(str(e_id))\n",
    "    #Add random negatives\n",
    "    neg_samples = neg_samples + list(np.random.choice(e_ids,num_random_neg_cands,replace=False))\n",
    "   \n",
    "    neg_samples = [int(x) for x in neg_samples]\n",
    "    train_samples[i]['negative_cands'] = neg_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "694ac014-0a17-4549-a41a-2984df8aed23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_data, train_tensor_data = process_mention_data(\n",
    "    train_samples,\n",
    "    tokenizer,\n",
    "    max_context_length,\n",
    "    max_cand_length\n",
    ")\n",
    "\n",
    "train_sampler = RandomSampler(train_tensor_data)\n",
    "train_dataloader = DataLoader(train_tensor_data, sampler=train_sampler, batch_size=train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e25c748d-4546-4a67-a839-2593c8a3dc84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load eval data\n",
    "valid_samples = []\n",
    "with open(monitor_fname, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        valid_samples.append(json.loads(line.strip()))\n",
    "print(len(valid_samples))\n",
    "\n",
    "for i in range(len(valid_samples)):\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "    \n",
    "    #We will store the negative samples for this entity here\n",
    "    neg_samples = []\n",
    "    #Get hard negatives\n",
    "    hard_neg_cands = hard_neg_cands_valid[i]\n",
    "    #Sample from all entities\n",
    "    e_ids = list(entity_dict.keys())\n",
    "    #Do not sample the correct entity\n",
    "    if valid_samples[i]['label_id'] is not None:\n",
    "        for e_id in entity_pool[valid_samples[i]['label_id']]:\n",
    "            e_ids.remove(e_id)\n",
    "    #Add hard negatives to the negative samples\n",
    "    neg_samples += hard_neg_cands\n",
    "    #Remove the hard entity IDs so we do not sample them randomly\n",
    "    for e_id in neg_samples:\n",
    "        e_ids.remove(str(e_id))\n",
    "    #Add random negatives\n",
    "    neg_samples = neg_samples + list(np.random.choice(e_ids,num_random_neg_cands,replace=False))\n",
    "   \n",
    "    neg_samples = [int(x) for x in neg_samples]\n",
    "    valid_samples[i]['negative_cands'] = neg_samples\n",
    "\n",
    "valid_data, valid_tensor_data = process_mention_data(\n",
    "    valid_samples,\n",
    "    tokenizer,\n",
    "    max_context_length,\n",
    "    max_cand_length\n",
    ")\n",
    "valid_sampler = SequentialSampler(valid_tensor_data)\n",
    "valid_dataloader = DataLoader(valid_tensor_data, sampler=valid_sampler, batch_size=eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c8f00d6d-40d4-46ec-83a0-b10a11c56adb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "optim_cand = torch.optim.AdamW(cand_model.parameters(), lr=2e-5) \n",
    "scheduler_cand = get_linear_schedule_with_warmup(optim_cand, \n",
    "                                                 num_warmup_steps = 0, \n",
    "                                                 num_training_steps = len(train_dataloader) // grad_acc_steps * num_train_epochs)\n",
    "optim_ctxt = torch.optim.AdamW(ctxt_model.parameters(), lr=2e-5) \n",
    "scheduler_ctxt = get_linear_schedule_with_warmup(optim_ctxt, \n",
    "                                                 num_warmup_steps = 0, \n",
    "                                                 num_training_steps = len(train_dataloader) // grad_acc_steps * num_train_epochs)\n",
    "optim_m = torch.optim.AdamW(m.parameters(), lr=2e-5) \n",
    "scheduler_m = get_linear_schedule_with_warmup(optim_m, \n",
    "                                                 num_warmup_steps = 0, \n",
    "                                                 num_training_steps = len(train_dataloader) // grad_acc_steps * num_train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e328ca16-7aa3-4f50-bc5d-035c80429190",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ctxt_model.eval()\n",
    "cand_model.eval()\n",
    "m.eval()\n",
    "all_loss=0\n",
    "print(\"Number of steps: \",len(valid_dataloader))\n",
    "with torch.no_grad():\n",
    "    num_correct = 0\n",
    "    num_all = 0\n",
    "    for step, batch in enumerate(valid_dataloader):\n",
    "\n",
    "        context_input, candidate_input, e_ids  = batch\n",
    "        longest_cand = torch.max(torch.argmin(candidate_input,dim=1))\n",
    "        candidate_input = candidate_input[:,:longest_cand]\n",
    "        \n",
    "        if step%10==0:\n",
    "            print(\"Step:\",step,\" longest cand \",longest_cand)\n",
    "            \n",
    "        context_token_idx, context_segment_idx, context_mask = to_bert_input(context_input.to(device),device)\n",
    "        candidate_token_idx, candidate_segment_idx, candidate_mask = to_bert_input(candidate_input.to(device),device)\n",
    "        \n",
    "        context_rep = ctxt_model(context_token_idx, context_segment_idx, context_mask)[0][:,0,:]\n",
    "        cand_rep = cand_model(candidate_token_idx, candidate_segment_idx, candidate_mask)[0][:,0,:]\n",
    "        \n",
    "        scores = context_rep.mul(cand_rep)\n",
    "        scores = m(scores)\n",
    "        \n",
    "        loss = torch.nn.functional.cross_entropy(scores, e_ids.to(device))#,weight=torch.tensor(class_weights).to(device))\n",
    "        all_loss+=loss\n",
    "        outputs = np.argmax(scores.cpu().detach(), axis=1)\n",
    "        outputs = np.sum(outputs.numpy() == e_ids.numpy())\n",
    "        num_correct += outputs\n",
    "        num_all += context_rep.size(0)\n",
    "all_loss/=len(valid_dataloader)\n",
    "print(\"Val_Loss: \",all_loss)\n",
    "print(\"Val_Acc: \",num_correct/num_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "28c5cb69-9d8e-4fe5-98d7-660abeccc443",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ctxt_model.train()\n",
    "cand_model.train()\n",
    "m.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fb0a1c6e-7c87-464a-8d5c-b5c496a109c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print('Number of steps per epoch: ',len(train_dataloader))\n",
    "print('Number of steps with accumulation: ',len(train_dataloader)//grad_acc_steps)\n",
    "\n",
    "#Reset Gradients\n",
    "optim_cand.zero_grad()\n",
    "optim_ctxt.zero_grad()\n",
    "optim_m.zero_grad()\n",
    "start=time.time()\n",
    "#Loop over epocs\n",
    "for epoch in range(num_train_epochs):\n",
    "    print(\"Epoch \",epoch)\n",
    "    #Store average training loss here\n",
    "    avg_loss = []\n",
    "    #Loop over minibatches\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        #Get the batch\n",
    "        context_input, candidate_input, e_ids  = batch\n",
    "        \n",
    "        longest_cand = torch.max(torch.argmin(candidate_input,dim=1))\n",
    "        candidate_input = candidate_input[:,:longest_cand]\n",
    "        \n",
    "        context_token_idx, context_segment_idx, context_mask = to_bert_input(context_input.to(device),device)\n",
    "        candidate_token_idx, candidate_segment_idx, candidate_mask = to_bert_input(candidate_input.to(device),device)\n",
    "        #Get representations concerning the cls token\n",
    "        context_rep = ctxt_model(context_token_idx, context_segment_idx, context_mask)[0][:,0,:]\n",
    "        cand_rep = cand_model(candidate_token_idx, candidate_segment_idx, candidate_mask)[0][:,0,:]\n",
    "        \n",
    "        #Calculate scores\n",
    "        scores = context_rep.mul(cand_rep)\n",
    "        scores = m(scores)\n",
    "        \n",
    "        #Calculate loss for storing\n",
    "        loss = torch.nn.functional.cross_entropy(scores, e_ids.to(device))#,weight=torch.tensor(class_weights).to(device))\n",
    "        avg_loss.append(loss.item())\n",
    "        \n",
    "        \n",
    "        #Divide loss by grad_acc_steps for backprop\n",
    "        loss = loss/grad_acc_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        #Do an update if you have accumulated enough\n",
    "        if (step+1)%grad_acc_steps==0:\n",
    "            if (step+1)%1000==0:\n",
    "                print(\"\\tStep: \",step+1,\" Loss: \",avg_loss[-1],\" Longest Cand: \",longest_cand,\" \",time.time()-start)\n",
    "            #Normalize gradients\n",
    "            torch.nn.utils.clip_grad_norm_(ctxt_model.parameters(), grad_norm)\n",
    "            torch.nn.utils.clip_grad_norm_(cand_model.parameters(), grad_norm)\n",
    "            torch.nn.utils.clip_grad_norm_(m.parameters(), grad_norm)\n",
    "            #Step the optimizer and scheduler\n",
    "            #Reset gradients\n",
    "            optim_cand.step()\n",
    "            scheduler_cand.step()\n",
    "            optim_cand.zero_grad()\n",
    "            optim_ctxt.step()\n",
    "            optim_ctxt.zero_grad()\n",
    "            scheduler_ctxt.step()\n",
    "            optim_m.step()\n",
    "            scheduler_m.step()\n",
    "            optim_m.zero_grad()\n",
    "            \n",
    "    #Reset gradients at the end of epoch    \n",
    "    optim_cand.zero_grad()\n",
    "    optim_ctxt.zero_grad()\n",
    "    optim_m.zero_grad()\n",
    "    #Put model to eval mode\n",
    "    ctxt_model.eval()\n",
    "    cand_model.eval()\n",
    "    m.eval()\n",
    "    #This will store validation loss\n",
    "    all_loss=0\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        num_all = 0\n",
    "        for step, batch in enumerate(valid_dataloader):\n",
    "            context_input, candidate_input, e_ids  = batch\n",
    "            \n",
    "            longest_cand = torch.max(torch.argmin(candidate_input,dim=1))\n",
    "            candidate_input = candidate_input[:,:longest_cand]\n",
    "            \n",
    "            context_token_idx, context_segment_idx, context_mask = to_bert_input(context_input.to(device),device)\n",
    "            candidate_token_idx, candidate_segment_idx, candidate_mask = to_bert_input(candidate_input.to(device),device)\n",
    "            context_rep = ctxt_model(context_token_idx, context_segment_idx, context_mask)[0][:,0,:]\n",
    "            cand_rep = cand_model(candidate_token_idx, candidate_segment_idx, candidate_mask)[0][:,0,:]\n",
    "            scores = context_rep.mul(cand_rep)\n",
    "            scores = m(scores)\n",
    "            loss = torch.nn.functional.cross_entropy(scores, e_ids.to(device))#,weight=torch.tensor(class_weights).to(device))\n",
    "            all_loss+=loss\n",
    "            outputs = np.argmax(scores.cpu().detach(), axis=1)\n",
    "            outputs = np.sum(outputs.numpy() == e_ids.numpy())\n",
    "            num_correct += outputs\n",
    "            num_all += context_rep.size(0)\n",
    "    all_loss/=len(valid_dataloader)\n",
    "    print(\"Val_Loss: \",all_loss)\n",
    "    print(\"Val_Acc: \",num_correct/num_all)\n",
    "    print(\"Train_loss\",np.mean(avg_loss))\n",
    "    ctxt_model.train()\n",
    "    cand_model.train()\n",
    "    m.train()\n",
    "torch.save(ctxt_model,INP_PATH+\"hardneg_ctxt_model_\"+str(ROUND_NUMBER)+\".pt\")\n",
    "torch.save(cand_model,INP_PATH+\"hardneg_cand_model_\"+str(ROUND_NUMBER)+\".pt\")\n",
    "torch.save(m,INP_PATH+\"hardneg_m_\"+str(ROUND_NUMBER)+\".pt\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "KEEP_BiEncoderHardNeg_Research_2ep",
   "notebookOrigID": 3356870,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
