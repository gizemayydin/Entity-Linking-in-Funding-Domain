{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs\n",
    "* `embeddings_folder`: Path to the file containing the entity embeddings that will be used.\n",
    "* `input_batch_size`: Batch size for context model inference.\n",
    "* `model_path`: Path to the context model that will be used.\n",
    "* `model_path_m`: Path to the linear layer that will be used.\n",
    "* `device`: Whether GPU or CPU will be used.\n",
    "* `train_fname`: Path to `train.jsonl` or `dev.jsonl`\n",
    "* `path_to_entity_pool`: Path to `entity_pool.pkl`\n",
    "* `PRED_FNAME`: Filename of the pickle file that will contain the top predictions and scores. \n",
    "\n",
    "# Outputs\n",
    "Predictions written to `PRED_FNAME`. The store variable is a list which has the same length with the number of samples. For each samples, top 30 (can be changed from last cell) entities are returned. That is why, the list for each sample contains 30 tuples, where the first element is the score assigned by model and the second element is the entity ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import  BertTokenizerFast, BertModel\n",
    "import annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_folder = 'entity_embeds_4.pkl'\n",
    "input_batch_size = 256\n",
    "model_path = 'hardneg_context_model_4.pt'\n",
    "model_path_m = 'hardneg_m_4.pt'\n",
    "seed = 0\n",
    "device='cuda'\n",
    "train_fname = \"train.jsonl\"\n",
    "PRED_FNAME = \"train_preds.pkl\"\n",
    "path_to_entity_pool = \"entity_pool.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_to_entity_pool,\"rb\") as f:\n",
    "    entity_pool = pickle.load(f)\n",
    "    \n",
    "with open(embeddings_folder,\"rb\") as f:\n",
    "    entity_emebeddings=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENT_START_TAG = \"[unused0]\"\n",
    "ENT_END_TAG = \"[unused1]\"\n",
    "def get_context_representation(\n",
    "    sample,\n",
    "    tokenizer,\n",
    "    max_seq_length,\n",
    "    mention_key=\"mention\",\n",
    "    context_key=\"context\",\n",
    "    ent_start_token=ENT_START_TAG,\n",
    "    ent_end_token=ENT_END_TAG,\n",
    "):\n",
    "    # mention_tokens = [Ms] mention [Me]\n",
    "    mention_tokens = []\n",
    "    if sample[mention_key] and len(sample[mention_key]) > 0:\n",
    "        mention_tokens = tokenizer.tokenize(sample[mention_key])\n",
    "        mention_tokens = [ent_start_token] + mention_tokens + [ent_end_token]\n",
    "\n",
    "    context_left = sample[context_key + \"_left\"]\n",
    "    context_right = sample[context_key + \"_right\"]\n",
    "    context_left = tokenizer.tokenize(context_left)\n",
    "    context_right = tokenizer.tokenize(context_right)\n",
    "\n",
    "    left_quota = (max_seq_length - len(mention_tokens)) // 2 - 1\n",
    "    right_quota = max_seq_length - len(mention_tokens) - left_quota - 2\n",
    "    left_add = len(context_left)\n",
    "    right_add = len(context_right)\n",
    "    if left_add <= left_quota:\n",
    "        if right_add > right_quota:\n",
    "            right_quota += left_quota - left_add\n",
    "    else:\n",
    "        if right_add <= right_quota:\n",
    "            left_quota += right_quota - right_add\n",
    "    \n",
    "    context_tokens = (\n",
    "        context_left[-left_quota:] + mention_tokens + context_right[:right_quota]\n",
    "    )\n",
    "    \n",
    "    # mention_tokens = [CLS] left context [Ms] mention [Me] right context [SEP]\n",
    "    context_tokens = [\"[CLS]\"] + context_tokens + [\"[SEP]\"]\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(context_tokens)\n",
    "    padding = [0] * (max_seq_length - len(input_ids))\n",
    "    input_ids += padding\n",
    "    assert len(input_ids) == max_seq_length\n",
    "\n",
    "    return {\n",
    "        \"tokens\": context_tokens,\n",
    "        \"ids\": input_ids,\n",
    "    }\n",
    "\n",
    "\n",
    "def select_field(data, key1, key2=None):\n",
    "    if key2 is None:\n",
    "        return [example[key1] for example in data]\n",
    "    else:\n",
    "        return [example[key1][key2] for example in data]\n",
    "def process_mention_data_2(samples,tokenizer):\n",
    "    \n",
    "    max_context_length=64\n",
    "    mention_key=\"mention\"\n",
    "    context_key=\"context\"\n",
    "    ent_start_token=\"[unused0]\"\n",
    "    ent_end_token=\"[unused1]\"\n",
    "    \n",
    "    processed_samples = []\n",
    "    all_samples = []\n",
    "    iter_ = samples\n",
    "\n",
    "    for idx, sample in enumerate(iter_):\n",
    "        context_tokens = get_context_representation(sample,tokenizer,max_context_length,mention_key,context_key,ent_start_token,ent_end_token)\n",
    "                        \n",
    "        record = {\"context\": context_tokens}\n",
    "            \n",
    "        processed_samples.append(record)\n",
    "        all_samples.append(sample)\n",
    "        \n",
    "    context_vecs = torch.tensor(\n",
    "        select_field(processed_samples, \"context\", \"ids\"), dtype=torch.long,\n",
    "    )\n",
    "    data = {\n",
    "        \"context_vecs\": context_vecs,\n",
    "        \"sample\":all_samples\n",
    "    }\n",
    "\n",
    "    tensor_data = TensorDataset(context_vecs)\n",
    "    return data, tensor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctxt_model = torch.load(model_path).to(device)\n",
    "m = torch.load(model_path_m).to('cpu')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "ctxt_model.eval()\n",
    "m.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data \n",
    "train_samples = []\n",
    "with open(train_fname, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        train_samples.append(json.loads(line.strip()))\n",
    "print(len(train_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_entities = [x['label_id'] for x in train_samples]\n",
    "\n",
    "train_data, train_tensor_data = process_mention_data_2(train_samples,tokenizer)\n",
    "\n",
    "train_sampler = SequentialSampler(train_tensor_data)\n",
    "train_dataloader = DataLoader(train_tensor_data, sampler=train_sampler, batch_size=input_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctxt_model.eval()\n",
    "print(len(train_dataloader))\n",
    "mention_embeddings = []\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    for step, context_input in enumerate(train_dataloader):\n",
    "        if step%10==0:\n",
    "            print(\"Step: \",step,\" \",time.time()-start)\n",
    "        context_input = context_input[0]\n",
    "        this_batch= context_input.size(0)\n",
    "        ctxt_rep = ctxt_model(context_input.to(device))[0][:,0,:]\n",
    "        for i in range(this_batch):\n",
    "            mention_embeddings.append(ctxt_rep[i].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Param for positive class\n",
    "m_second_param = list(m.parameters())[0][1].detach().numpy()\n",
    "\n",
    "entity_emebeddings_with_m = dict()\n",
    "keys_map = dict()\n",
    "ctr = 0\n",
    "for k,v in entity_emebeddings.items():\n",
    "    entity_emebeddings_with_m[ctr] = np.multiply(m_second_param,v)\n",
    "    keys_map[ctr] = k\n",
    "    ctr+=1\n",
    "    \n",
    "t = annoy.AnnoyIndex(768, 'dot') \n",
    "\n",
    "t.set_seed(0)\n",
    "\n",
    "for k,v in entity_emebeddings_with_m.items():\n",
    "    t.add_item(k, v)\n",
    "t.build(1000, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_hard_negs=30\n",
    "\n",
    "cands = []\n",
    "start = time.time()\n",
    "#Loop over mentions \n",
    "for i in range(len(mention_embeddings)):\n",
    "    if i%100 == 0:\n",
    "        print(i, \" \",time.time()-start)\n",
    "    #Get the mention embedding\n",
    "    this_ment_embed = mention_embeddings[i]\n",
    "    \n",
    "    \n",
    "    #Now we get the top num_hard_negs predictions\n",
    "    res = t.get_nns_by_vector(this_ment_embed, num_hard_negs, search_k=len(entity_emebeddings_with_m), include_distances=True)\n",
    "    #Store entities and scores\n",
    "    #Score = -dot\n",
    "    returned_entities = [keys_map[x] for x in res[0]]\n",
    "    scores = [1- 1/(1 + np.exp(x)) for x in res[1]]\n",
    "    merged = list(zip(scores,returned_entities))\n",
    "    #Sort returned instances\n",
    "    merged.sort(key=lambda tup: tup[0],reverse=True) \n",
    "\n",
    "    cands.append(merged)\n",
    "    \n",
    "with open(PRED_FNAME,'wb') as f:\n",
    "    pickle.dump(cands,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
