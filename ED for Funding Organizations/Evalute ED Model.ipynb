{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs\n",
    "\n",
    "* `path_to_entity_pool`: Path to `entity_pool.pkl` (See `Hard Negative Mining.ipynb`)\n",
    "* `correct`: Correct entities per sample\n",
    "* `files`: Filename per sample\n",
    "* `pred`: Predicted entity per sample\n",
    "* `scores`: Score of the predicted entity per sample\n",
    "* `threshold`: NIL mention detection threshold\n",
    "\n",
    "Scores are printed throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List\n",
    "correct = [] \n",
    "#List\n",
    "files = []\n",
    "#List\n",
    "pred = []\n",
    "#List\n",
    "scores= [] \n",
    "#Flaot between [0,1]\n",
    "threshold = \n",
    "path_to_entity_pool = \"entity_pool.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_to_entity_pool,\"rb\") as f:\n",
    "    entity_pool = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thresholded_preds(th,pred,scores):\n",
    "    thresholded_preds = []\n",
    "    for i in range(len(pred)):\n",
    "        if scores[i]>=th:\n",
    "            thresholded_preds.append(pred[i])\n",
    "        else:\n",
    "            thresholded_preds.append(None)\n",
    "    return thresholded_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr = 0\n",
    "thresholded_preds =  get_thresholded_preds(th,pred,scores)\n",
    "for i in range(len(correct)):\n",
    "    if thresholded_preds[i] is None and correct[i] is None:\n",
    "        ctr+=1\n",
    "    elif correct[i] is not None and str(thresholded_preds[i]) in entity_pool[correct[i]]:\n",
    "        ctr+=1\n",
    "print('Micro Average Accuracy: ',np.round((100*ctr)/len(correct),2),'%\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group predictions by files for macro calc\n",
    "grouped_preds = dict()\n",
    "grouped_scores = dict()\n",
    "grouped_correct = dict()\n",
    "for i in range(len(files)):\n",
    "    try:\n",
    "        grouped_preds[files[i]].append(pred[i])\n",
    "        grouped_scores[files[i]].append(scores[i])\n",
    "        grouped_correct[files[i]].append(correct[i])\n",
    "    except KeyError:\n",
    "        grouped_preds[files[i]] = [pred[i]]\n",
    "        grouped_scores[files[i]] = [scores[i]]\n",
    "        grouped_correct[files[i]] = [correct[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate macro avg acc \n",
    "all_accuracies = []\n",
    "for k in grouped_preds.keys():\n",
    "    this_preds = grouped_preds[k]\n",
    "    this_scores = grouped_scores[k]\n",
    "    this_correct = grouped_correct[k]\n",
    "    ctr = 0\n",
    "    thresholded_preds =  get_thresholded_preds(th,this_preds,this_scores)\n",
    "    for i in range(len(this_correct)):\n",
    "        if thresholded_preds[i] is None and this_correct[i] is None:\n",
    "            ctr+=1\n",
    "        elif this_correct[i] is not None and  str(thresholded_preds[i]) in entity_pool[this_correct[i]]:\n",
    "            ctr+=1\n",
    "    all_accuracies.append(ctr/len(this_correct))\n",
    "print('Macro Average Accuracy: ',np.round((100*(np.mean(all_accuracies))),2),'%\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate EE precision\n",
    "\n",
    "ee_precs = []\n",
    "\n",
    "total_nil_predictions = 0\n",
    "correct_nil_predictions = 0\n",
    "thresholded_preds =  get_thresholded_preds(th,pred,scores)\n",
    "for i in range(len(thresholded_preds)):\n",
    "    if thresholded_preds[i] == None:\n",
    "        total_nil_predictions+=1\n",
    "        if correct[i] == None:\n",
    "            correct_nil_predictions+=1\n",
    "try:\n",
    "    print('EE Precision: ',np.round((100*correct_nil_predictions)/total_nil_predictions,2),'%\\n')\n",
    "    ee_precs.append(np.round((100*correct_nil_predictions)/total_nil_predictions,2))\n",
    "except ZeroDivisionError:\n",
    "    print('EE Precision: 0','%\\n')\n",
    "    ee_precs.append(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate EE recall\n",
    "ee_recs = []\n",
    "\n",
    "total_nil_mentions = 0\n",
    "correct_nil_mentions = 0\n",
    "thresholded_preds =  get_thresholded_preds(th,pred,scores)\n",
    "for i in range(len(correct)):\n",
    "    if correct[i] == None:\n",
    "        total_nil_mentions+=1\n",
    "        if thresholded_preds[i] == None:\n",
    "            correct_nil_mentions+=1\n",
    "print('EE Recall: ',np.round((100*correct_nil_mentions)/total_nil_mentions,2),'%\\n')\n",
    "ee_recs.append(np.round((100*correct_nil_mentions)/total_nil_mentions,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate InKB Precision\n",
    "inkb_precs = []\n",
    "\n",
    "total_mentions = 0\n",
    "correct_mentions = 0\n",
    "thresholded_preds =  get_thresholded_preds(th,pred,scores)\n",
    "for i in range(len(thresholded_preds)):\n",
    "    if thresholded_preds[i] is not None:\n",
    "        total_mentions+=1\n",
    "        if correct[i] is not None:\n",
    "            if  str(thresholded_preds[i]) in entity_pool[correct[i]]:\n",
    "                correct_mentions+=1\n",
    "print('InKB Precision: ',np.round((100*correct_mentions)/total_mentions,2),'%\\n')\n",
    "inkb_precs.append(np.round((100*correct_mentions)/total_mentions,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate InKB Recall\n",
    "inkb_recs = []\n",
    "\n",
    "total_mentions = 0\n",
    "correct_mentions = 0\n",
    "thresholded_preds =  get_thresholded_preds(th,pred,scores)\n",
    "for i in range(len(correct)):\n",
    "    if correct[i] is not None:\n",
    "        total_mentions+=1\n",
    "        if  str(thresholded_preds[i]) in entity_pool[correct[i]]:\n",
    "            correct_mentions+=1\n",
    "print('InKB Recall: ',np.round((100*correct_mentions)/total_mentions,2),'%\\n')\n",
    "inkb_recs.append(np.round((100*correct_mentions)/total_mentions,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1(p,r):\n",
    "    return (2*p*r)/(p+r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"EE F1: \",np.round(get_f1(ee_precs[0],ee_recs[0]),2))\n",
    "print(\"InKB F1: \",np.round(get_f1(inkb_precs[0],inkb_recs[0]),2))\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
