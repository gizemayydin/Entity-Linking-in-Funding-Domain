{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs\n",
    "* `inputs`: List of samples. Each sample should be a dictionary with fields `\"mention\"`, `\"context_left\"` and `\"context_right\"`. See notebook `BiEncoder RandomNegative Training.ipynb` for explanations of these fields.\n",
    "* `path`: Path to files needed to run this notebook: \n",
    "    * `commonness.json` (See `Train GBM Reranker.ipynb`)\n",
    "    * `link_prob.json` (See `Train GBM Reranker.ipynb`)\n",
    "    * `popularity.json` (See `Train GBM Reranker.ipynb`)\n",
    "    * `lgbm12.pkl` (Obtained from `Train GBM Reranker.ipynb`)\n",
    "    * `entities.pkl` (See `Train GBM Reranker.ipynb`)\n",
    "    * `entity_embeds.pkl`  (Run `Compute Entity Embeddings.ipynb` with the latest candidate entity encoder model)\n",
    "    * `hardneg_context_model_4.pt` (Latest mention encoder model)\n",
    "    * `hardneg_m_4.pt` (Latest linear scoring layer)\n",
    "* `threshold`: NIL mention detection threshold. Obtained from `Train GBM Reranker.ipynb`\n",
    "* `device`: Device to run the context model, \"cuda\" or \"cpu\"\n",
    "* `input_batch_size`: batch size for the context model\n",
    "\n",
    "# Output\n",
    "\n",
    "The predictions are shown in a dataframe at the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "import pickle\n",
    "import json\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import  BertTokenizerFast, BertModel\n",
    "import lightgbm as lgb\n",
    "import annoy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "inputs = [   ]\n",
    "#Path containing necessary files\n",
    "path = \"\"\n",
    "#NIL mention detection threshold\n",
    "threshold = 0.5\n",
    "#Device and batch size to use context model.\n",
    "device = \"cuda\"\n",
    "input_batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "ENT_START_TAG = \"[unused0]\"\n",
    "ENT_END_TAG = \"[unused1]\"\n",
    "def get_context_representation(\n",
    "    sample,\n",
    "    tokenizer,\n",
    "    max_seq_length,\n",
    "    mention_key=\"mention\",\n",
    "    context_key=\"context\",\n",
    "    ent_start_token=ENT_START_TAG,\n",
    "    ent_end_token=ENT_END_TAG,\n",
    "):\n",
    "    # mention_tokens = [Ms] mention [Me]\n",
    "    mention_tokens = []\n",
    "    if sample[mention_key] and len(sample[mention_key]) > 0:\n",
    "        mention_tokens = tokenizer.tokenize(sample[mention_key])\n",
    "        mention_tokens = [ent_start_token] + mention_tokens + [ent_end_token]\n",
    "\n",
    "    context_left = sample[context_key + \"_left\"]\n",
    "    context_right = sample[context_key + \"_right\"]\n",
    "    context_left = tokenizer.tokenize(context_left)\n",
    "    context_right = tokenizer.tokenize(context_right)\n",
    "\n",
    "    left_quota = (max_seq_length - len(mention_tokens)) // 2 - 1\n",
    "    right_quota = max_seq_length - len(mention_tokens) - left_quota - 2\n",
    "    left_add = len(context_left)\n",
    "    right_add = len(context_right)\n",
    "    if left_add <= left_quota:\n",
    "        if right_add > right_quota:\n",
    "            right_quota += left_quota - left_add\n",
    "    else:\n",
    "        if right_add <= right_quota:\n",
    "            left_quota += right_quota - right_add\n",
    "    \n",
    "    context_tokens = (\n",
    "        context_left[-left_quota:] + mention_tokens + context_right[:right_quota]\n",
    "    )\n",
    "    \n",
    "    # mention_tokens = [CLS] left context [Ms] mention [Me] right context [SEP]\n",
    "    context_tokens = [\"[CLS]\"] + context_tokens + [\"[SEP]\"]\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(context_tokens)\n",
    "    padding = [0] * (max_seq_length - len(input_ids))\n",
    "    input_ids += padding\n",
    "    assert len(input_ids) == max_seq_length\n",
    "\n",
    "    return {\n",
    "        \"tokens\": context_tokens,\n",
    "        \"ids\": input_ids,\n",
    "    }\n",
    "\n",
    "\n",
    "def select_field(data, key1, key2=None):\n",
    "    if key2 is None:\n",
    "        return [example[key1] for example in data]\n",
    "    else:\n",
    "        return [example[key1][key2] for example in data]\n",
    "def process_mention_data_2(samples,tokenizer):\n",
    "    \n",
    "    max_context_length=64\n",
    "    mention_key=\"mention\"\n",
    "    context_key=\"context\"\n",
    "    ent_start_token=\"[unused0]\"\n",
    "    ent_end_token=\"[unused1]\"\n",
    "    \n",
    "    processed_samples = []\n",
    "    all_samples = []\n",
    "    iter_ = samples\n",
    "\n",
    "    for idx, sample in enumerate(iter_):\n",
    "        context_tokens = get_context_representation(sample,tokenizer,max_context_length,mention_key,context_key,ent_start_token,ent_end_token)\n",
    "                        \n",
    "        record = {\"context\": context_tokens}\n",
    "            \n",
    "        processed_samples.append(record)\n",
    "        all_samples.append(sample)\n",
    "        \n",
    "    context_vecs = torch.tensor(\n",
    "        select_field(processed_samples, \"context\", \"ids\"), dtype=torch.long,\n",
    "    )\n",
    "    data = {\n",
    "        \"context_vecs\": context_vecs,\n",
    "        \"sample\":all_samples\n",
    "    }\n",
    "\n",
    "    tensor_data = TensorDataset(context_vecs)\n",
    "    return data, tensor_data\n",
    "def get_thresholded_preds(th,pred,scores):\n",
    "    thresholded_preds = []\n",
    "    for i in range(len(pred)):\n",
    "        if scores[i]>=th:\n",
    "            thresholded_preds.append(str(pred[i]))\n",
    "        else:\n",
    "            thresholded_preds.append('None')\n",
    "    return thresholded_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load files\n",
    "with open(path+'commonness.json','r',encoding='utf-8') as f:\n",
    "    commonness = json.load(f)\n",
    "with open(path+'link_prob.json','r',encoding='utf-8') as f:\n",
    "    link_probability = json.load(f)\n",
    "with open(path+'popularity.json','r',encoding='utf-8') as f:\n",
    "    popularity = json.load(f)\n",
    "with open(path+'lgbm12.pkl','rb') as f:\n",
    "    model_lgb = pickle.load(f)\n",
    "with open(path+'entity_embeds_4.pkl',\"rb\") as f:\n",
    "    entity_emebeddings=pickle.load(f)\n",
    "with open(path+'entities.pkl','rb') as f:\n",
    "    entity_labels=pickle.load(f)\n",
    "ctxt_model = torch.load(path+'hardneg_ctxt_model_4.pt').to(device)\n",
    "m = torch.load(path+'hardneg_m_4.pt').to('cpu')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "ctxt_model.eval()\n",
    "m.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataset\n",
    "train_data, train_tensor_data = process_mention_data_2(inputs,tokenizer)\n",
    "\n",
    "train_sampler = SequentialSampler(train_tensor_data)\n",
    "train_dataloader = DataLoader(train_tensor_data, sampler=train_sampler, batch_size=input_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get mention embeddings\n",
    "ctxt_model.eval()\n",
    "print(len(train_dataloader))\n",
    "mention_embeddings = []\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    for step, context_input in enumerate(train_dataloader):\n",
    "        if step%10==0:\n",
    "            print(\"Step: \",step,\" \",time.time()-start)\n",
    "        context_input = context_input[0]\n",
    "        this_batch= context_input.size(0)\n",
    "        ctxt_rep = ctxt_model(context_input.to(device))[0][:,0,:]\n",
    "        for i in range(this_batch):\n",
    "            mention_embeddings.append(ctxt_rep[i].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build annoy index for nearest neighbor search\n",
    "#Param for positive class\n",
    "m_second_param = list(m.parameters())[0][1].detach().numpy()\n",
    "\n",
    "entity_emebeddings_with_m = dict()\n",
    "keys_map = dict()\n",
    "ctr = 0\n",
    "for k,v in entity_emebeddings.items():\n",
    "    entity_emebeddings_with_m[ctr] = np.multiply(m_second_param,v)\n",
    "    keys_map[ctr] = k\n",
    "    ctr+=1\n",
    "    \n",
    "t = annoy.AnnoyIndex(768, 'dot') \n",
    "\n",
    "t.set_seed(0)\n",
    "\n",
    "for k,v in entity_emebeddings_with_m.items():\n",
    "    t.add_item(k, v)\n",
    "t.build(1000, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get top 12 candidates\n",
    "num_cands=12\n",
    "\n",
    "cands = []\n",
    "start = time.time()\n",
    "#Loop over mentions \n",
    "for i in range(len(mention_embeddings)):\n",
    "    if i%100 == 0:\n",
    "        print(i, \" \",time.time()-start)\n",
    "    #Get the mention embedding\n",
    "    this_ment_embed = mention_embeddings[i]\n",
    "    \n",
    "    \n",
    "    #Now we get the top num_hard_negs predictions\n",
    "    res = t.get_nns_by_vector(this_ment_embed, num_cands, search_k=len(entity_emebeddings_with_m), include_distances=True)\n",
    "    #Store entities and scores\n",
    "    #Score = -dot\n",
    "    returned_entities = [keys_map[x] for x in res[0]]\n",
    "    scores = [1- 1/(1 + np.exp(x)) for x in res[1]]\n",
    "    merged = list(zip(scores,returned_entities))\n",
    "    #Sort returned instances\n",
    "    merged.sort(key=lambda tup: tup[0],reverse=True) \n",
    "\n",
    "    cands.append(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get LGBM preds\n",
    "bert_scores = []\n",
    "fw_scores2 = []\n",
    "unique_id = []\n",
    "entity_ = []\n",
    "commonness_ = []\n",
    "popularity_ = []\n",
    "link_probability_ = []\n",
    "\n",
    "for i in range(len(inputs)):\n",
    "    candidates = cands[i]\n",
    "    this_mention = inputs[i]['mention']\n",
    "    for j in range(num_cands):\n",
    "        #Get FW score\n",
    "        this_ent_labels = entity_labels[str(candidates[j][1])]['Labels']\n",
    "        \n",
    "        fw_score2 = 0\n",
    "        for lbl in this_ent_labels:\n",
    "            fw_score2 = max(fw_score2,fuzz.token_sort_ratio(this_mention,lbl)/100)\n",
    "        fw_scores2.append(fw_score2)\n",
    "        \n",
    "        #Get BERT score\n",
    "        bert_scores.append(candidates[j][0])\n",
    "        \n",
    "        commonness_.append(commonness.get(this_mention.lower(),{}).get(str(candidates[j][1]),0.))\n",
    "        popularity_.append(popularity.get(str(candidates[j][1]),0.))\n",
    "        link_probability_.append(link_probability.get(this_mention.lower(),0.))\n",
    "        \n",
    "        \n",
    "        unique_id.append(i)\n",
    "        entity_.append(candidates[j][1])\n",
    "df=pd.DataFrame({'ID':unique_id,'Commonness':commonness_,'BERT':bert_scores,\n",
    "                 'Popularity':popularity_,'Link_Probability':link_probability_,\n",
    "                 'FW2':fw_scores2,'Entity':entity_})\n",
    "preds = model_lgb.predict(df[['Commonness', 'BERT', 'FW2','Popularity' ,'Link_Probability']]) \n",
    "\n",
    "df['Score'] = preds\n",
    "temp = df.copy(deep=True)\n",
    "temp=temp.loc[temp.groupby('ID').Score.idxmax().values][['ID','Score','Entity']]\n",
    "#Get Entities\n",
    "entities = temp.Entity.values\n",
    "#Get scores\n",
    "scores = temp.Score.values\n",
    "#Apply threshold\n",
    "temp['Entity']=get_thresholded_preds(threshold,entities,scores)\n",
    "#Add mention and context\n",
    "ment = []\n",
    "ctxleft = []\n",
    "ctxright = []\n",
    "for item in inputs:\n",
    "    ment.append(item['mention'])\n",
    "    ctxleft.append(item['context_left'])\n",
    "    ctxright.append(item['context_right'])\n",
    "temp['Mention']=ment\n",
    "temp['Left_Context']=ctxleft\n",
    "temp['Right_Context']=ctxright\n",
    "temp.drop(['ID'],axis=1,inplace=True)\n",
    "temp.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the results\n",
    "temp.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
