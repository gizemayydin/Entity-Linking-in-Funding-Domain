{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs\n",
    "See third cell. Most important inputs are:\n",
    "* `INP_PATH`: This path should contain 3 files:\n",
    "    * The training and dev data. The first must be named `train.jsonl` and the second must be named `dev.jsonl`. \n",
    "        * Both datasets should be in JSON Lines format, where each line is a json object representing a training/dev sample.\n",
    "        * Each json object must have these keys:\n",
    "            * \"mention\": The mention string\n",
    "            * \"context_left\": The left part of the sentence up until the mention\n",
    "            * \"context_right\": The right part of the sentence after the mention\n",
    "            * \"label_id\": The ID of the correct entity (string). Put `null` for NIL mention.\n",
    "            * \"folder\": In which folder the document is contained in. Helps with the cases where two different documents have the same name in different folders.\n",
    "            * \"filename\": The document name.\n",
    "    * A pickle file that should be named `entity_representations.pkl`. This pickle file should contain a dictionary where each key represents a unique entity. \n",
    "        * The values should be dictionaries themselves. For the key `'tokens'` put the tokenized version of entity representation, and for the key `'ids'`, put the token IDs for each wordpiece. Pad both values to the maximum candidate length.\n",
    "        * It will be unpickled as: \n",
    "        ```python\n",
    "        with open(INP_PATH+'entity_representations.pkl','rb') as f:\n",
    "            entity_dict=pickle.load(f)\n",
    "        ```\n",
    "    * Resulting models are saved to this path as well.    \n",
    "* `base_bert_model`: Either path to BERT Scopus or a string specifying which model to use (such as \"bert-base-uncased\")\n",
    "\n",
    "The rest of the inputs are explained in the comments and no change is required.\n",
    "\n",
    "# Outputs\n",
    "Trained models:\n",
    "* \"randomneg_ctxt_model.pt\"\n",
    "* \"randomneg_cand_model.pt\"\n",
    "* \"randomneg_m.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credits\n",
    "\n",
    "Some parts of the code are obtained from the [BLINK](https://github.com/facebookresearch/BLINK/tree/master/blink) repository, especially for data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1fcc29e0-0ed5-435a-a214-29b3156529f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import get_linear_schedule_with_warmup,BertTokenizerFast, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "59074b5d-b6fd-423c-b0dc-3eee5fa50edd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "INP_PATH = \"\"\n",
    "#Seed\n",
    "seed = 0\n",
    "#Cuda or cpu\n",
    "device = 'cuda'\n",
    "#Path to BERT Scopus or enter 'bert-base-cased'\n",
    "base_bert_model = 'bert-base-cased'\n",
    "#Max length of mention context. Default: 64\n",
    "max_context_length= 64\n",
    "#Max length of entity representation. Default: 256\n",
    "max_cand_length = 256\n",
    "#Batch size for training. Default:16\n",
    "train_batch_size = 16\n",
    "#Number of epochs. Default:2\n",
    "num_train_epochs=2\n",
    "#Batch size for evaluation. Default:256\n",
    "eval_batch_size= 256\n",
    "#Gradient accumulation steps. Effective batch size=train_batch_sizexgrad_acc_steps. Default:4\n",
    "grad_acc_steps= 4\n",
    "#Class weights. Default:[0.25,0.75]\n",
    "class_weights = [0.25,0.75]\n",
    "#Number of negative candidates per mention. Default:3\n",
    "num_neg_cands=3\n",
    "#Value to normalize the gradients to. Default:1.0\n",
    "grad_norm = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fname = INP_PATH+\"train.jsonl\"\n",
    "monitor_fname = INP_PATH+\"dev.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fd135806-4ac0-408e-b858-8d0c95790ab1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##FROM THE BLINK REPO##\n",
    "ENT_START_TAG = \"[unused0]\"\n",
    "ENT_END_TAG = \"[unused1]\"\n",
    "\n",
    "def select_field(data, key1, key2=None):\n",
    "    if key2 is None:\n",
    "        return [example[key1] for example in data]\n",
    "    else:\n",
    "        return [example[key1][key2] for example in data]\n",
    "\n",
    "def get_context_representation(\n",
    "    sample,\n",
    "    tokenizer,\n",
    "    max_seq_length,\n",
    "    mention_key=\"mention\",\n",
    "    context_key=\"context\",\n",
    "    ent_start_token=ENT_START_TAG,\n",
    "    ent_end_token=ENT_END_TAG,\n",
    "):\n",
    "    # mention_tokens = [Ms] mention [Me]\n",
    "    mention_tokens = []\n",
    "    if sample[mention_key] and len(sample[mention_key]) > 0:\n",
    "        mention_tokens = tokenizer.tokenize(sample[mention_key])\n",
    "        mention_tokens = [ent_start_token] + mention_tokens + [ent_end_token]\n",
    "\n",
    "    context_left = sample[context_key + \"_left\"]\n",
    "    context_right = sample[context_key + \"_right\"]\n",
    "    context_left = tokenizer.tokenize(context_left)\n",
    "    context_right = tokenizer.tokenize(context_right)\n",
    "\n",
    "    left_quota = (max_seq_length - len(mention_tokens)) // 2 - 1\n",
    "    right_quota = max_seq_length - len(mention_tokens) - left_quota - 2\n",
    "    left_add = len(context_left)\n",
    "    right_add = len(context_right)\n",
    "    if left_add <= left_quota:\n",
    "        if right_add > right_quota:\n",
    "            right_quota += left_quota - left_add\n",
    "    else:\n",
    "        if right_add <= right_quota:\n",
    "            left_quota += right_quota - right_add\n",
    "    \n",
    "    context_tokens = (\n",
    "        context_left[-left_quota:] + mention_tokens + context_right[:right_quota]\n",
    "    )\n",
    "    \n",
    "    # mention_tokens = [CLS] left context [Ms] mention [Me] right context [SEP]\n",
    "    context_tokens = [\"[CLS]\"] + context_tokens + [\"[SEP]\"]\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(context_tokens)\n",
    "    padding = [0] * (max_seq_length - len(input_ids))\n",
    "    input_ids += padding\n",
    "    assert len(input_ids) == max_seq_length\n",
    "\n",
    "    return {\n",
    "        \"tokens\": context_tokens,\n",
    "        \"ids\": input_ids,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_candidate_representation(label_idx):\n",
    "\n",
    "    cand_tokens = entity_dict[str(label_idx)]['tokens']\n",
    "    input_ids = entity_dict[str(label_idx)]['ids']\n",
    "    \n",
    "    return {\n",
    "        \"tokens\": cand_tokens,\n",
    "        \"ids\": input_ids,\n",
    "    }\n",
    "  \n",
    "def to_bert_input(token_idx,dev_name):\n",
    "    \"\"\" token_idx is a 2D tensor int.\n",
    "        return token_idx, segment_idx and mask\n",
    "    \"\"\"\n",
    "    segment_idx = None\n",
    "    mask = token_idx != 0\n",
    "    if dev_name =='cuda':\n",
    "        segment_idx = torch.cuda.LongTensor(token_idx * 0)\n",
    "        mask = torch.cuda.LongTensor(mask.long())\n",
    "    else:\n",
    "        segment_idx = torch.LongTensor(token_idx * 0)\n",
    "        mask = torch.LongTensor(mask.long())    \n",
    "    return token_idx, segment_idx, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "99415d2c-1e8d-4867-8ff2-659b99428c09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##FROM THE BLINK REPO##\n",
    "def process_mention_data(\n",
    "    samples,\n",
    "    tokenizer,\n",
    "    max_context_length,\n",
    "    max_cand_length,\n",
    "    mention_key=\"mention\",\n",
    "    context_key=\"context\",\n",
    "    ent_start_token=ENT_START_TAG,\n",
    "    ent_end_token=ENT_END_TAG\n",
    "):\n",
    "    processed_samples = []\n",
    "    iter_ = samples\n",
    "    all_samples = []\n",
    "    \n",
    "    #Loop over samples\n",
    "    for idx, sample in enumerate(iter_):\n",
    "        #Compute mention and context representation\n",
    "        context_tokens = get_context_representation(\n",
    "            sample,\n",
    "            tokenizer,\n",
    "            max_context_length,\n",
    "            mention_key,\n",
    "            context_key,\n",
    "            ent_start_token,\n",
    "            ent_end_token,\n",
    "        )\n",
    "        \n",
    "        #For non-NIL mentions, add a positive sample to the dataset\n",
    "        if len(sample[\"negative_cands\"]) != num_neg_cands:\n",
    "          #NIL mention\n",
    "          pass\n",
    "        else:\n",
    "            #Get the entity id\n",
    "            label_idx = int(sample[\"label_id\"])\n",
    "            #Get entity representation\n",
    "            label_tokens = get_candidate_representation(label_idx)\n",
    "          \n",
    "            #Add data as positive class\n",
    "            record = {\n",
    "                \"context\": context_tokens,\n",
    "                \"label\": label_tokens,\n",
    "                \"label_idx\": 1,\n",
    "                \"sample\":sample\n",
    "            }\n",
    "            processed_samples.append(record)\n",
    "            all_samples.append(sample)\n",
    "        \n",
    "        #Add negative samples based on the negative candidate entities of that mention\n",
    "        for label_idx in sample[\"negative_cands\"]:\n",
    "            label_tokens = get_candidate_representation(label_idx)\n",
    "            record = {\n",
    "                \"context\": context_tokens,\n",
    "                \"label\": label_tokens,\n",
    "                \"label_idx\": 0,\n",
    "                \"sample\":sample\n",
    "            }\n",
    "            processed_samples.append(record)\n",
    "            all_samples.append(sample)\n",
    "        \n",
    "    #Convert inputs to torch tensors\n",
    "    context_vecs = torch.tensor(\n",
    "        select_field(processed_samples, \"context\", \"ids\"), dtype=torch.long,\n",
    "    )\n",
    "    cand_vecs = torch.tensor(\n",
    "        select_field(processed_samples, \"label\", \"ids\"), dtype=torch.long,\n",
    "    )\n",
    "    label_idx = torch.tensor(\n",
    "        select_field(processed_samples, \"label_idx\"), dtype=torch.long,\n",
    "    )\n",
    "    data = {\n",
    "        \"context_vecs\": context_vecs,\n",
    "        \"cand_vecs\": cand_vecs,\n",
    "        \"label_idx\": label_idx,\n",
    "        \"sample\":all_samples\n",
    "    }\n",
    "    \n",
    "    #Create tensor dataset and return\n",
    "    tensor_data = TensorDataset(context_vecs, cand_vecs, label_idx)\n",
    "    return data, tensor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f7210949-33bc-4492-9a74-65abf8b095ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open(INP_PATH+'entity_representations.pkl','rb') as f:\n",
    "    entity_dict=pickle.load(f)\n",
    "\n",
    "#Seed everything\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d61f7cd0-4299-4ed4-ba36-7bf12393cea8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Initialzie mention and entity models\n",
    "ctxt_model = BertModel.from_pretrained(base_bert_model).to(device)\n",
    "cand_model = BertModel.from_pretrained(base_bert_model).to(device)\n",
    "#Initialize the linear layer\n",
    "m = torch.nn.Linear(768, 2,bias=False).to(device)\n",
    "#Get tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "929a9bde-f2db-47e1-9cee-30ea1f225512",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load train data \n",
    "train_samples = []\n",
    "with open(train_fname, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        train_samples.append(json.loads(line.strip()))\n",
    "print(len(train_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3796412e-2b7b-48fe-86fb-d7a255bf3423",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Sample random negative candidates\n",
    "for i in range(len(train_samples)):\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "    this_neg_cand = num_neg_cands\n",
    "    e_ids = list(entity_dict.keys())\n",
    "    neg_samples = None\n",
    "    #Do not sample the correct enttiy as negative\n",
    "    if train_samples[i]['label_id'] is not None:\n",
    "        e_ids.remove(train_samples[i]['label_id'])\n",
    "        neg_samples = np.random.choice(e_ids,num_neg_cands,replace=False)\n",
    "    else:\n",
    "        #NIL mention\n",
    "        neg_samples = np.random.choice(e_ids,num_neg_cands+1,replace=False)\n",
    "    neg_samples = [int(x) for x in neg_samples]\n",
    "    train_samples[i]['negative_cands'] = neg_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "694ac014-0a17-4549-a41a-2984df8aed23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Get the training dataset\n",
    "train_data, train_tensor_data = process_mention_data(\n",
    "    train_samples,\n",
    "    tokenizer,\n",
    "    max_context_length,\n",
    "    max_cand_length\n",
    ")\n",
    "\n",
    "train_sampler = RandomSampler(train_tensor_data)\n",
    "train_dataloader = DataLoader(train_tensor_data, sampler=train_sampler, batch_size=train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e25c748d-4546-4a67-a839-2593c8a3dc84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load eval data\n",
    "valid_samples = []\n",
    "with open(monitor_fname, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        valid_samples.append(json.loads(line.strip()))\n",
    "print(len(valid_samples))\n",
    "for i in range(len(valid_samples)):\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "    this_neg_cand = num_neg_cands\n",
    "    e_ids = list(entity_dict.keys())\n",
    "    neg_samples = None\n",
    "    if valid_samples[i]['label_id'] is not None:\n",
    "        e_ids.remove(valid_samples[i]['label_id'])\n",
    "        neg_samples = np.random.choice(e_ids,num_neg_cands,replace=False)\n",
    "    else:\n",
    "        #NIL mention\n",
    "        neg_samples = np.random.choice(e_ids,num_neg_cands+1,replace=False)\n",
    "    neg_samples = [int(x) for x in neg_samples]\n",
    "    valid_samples[i]['negative_cands'] = neg_samples\n",
    "\n",
    "valid_data, valid_tensor_data = process_mention_data(\n",
    "    valid_samples,\n",
    "    tokenizer,\n",
    "    max_context_length,\n",
    "    max_cand_length\n",
    ")\n",
    "valid_sampler = SequentialSampler(valid_tensor_data)\n",
    "valid_dataloader = DataLoader(valid_tensor_data, sampler=valid_sampler, batch_size=eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c8f00d6d-40d4-46ec-83a0-b10a11c56adb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Initialize optimizers and schedulers\n",
    "optim_cand = torch.optim.AdamW(cand_model.parameters(), lr=2e-5) \n",
    "scheduler_cand = get_linear_schedule_with_warmup(optim_cand, \n",
    "                                                 num_warmup_steps = 0, \n",
    "                                                 num_training_steps = len(train_dataloader) // grad_acc_steps * num_train_epochs)\n",
    "optim_ctxt = torch.optim.AdamW(ctxt_model.parameters(), lr=2e-5) \n",
    "scheduler_ctxt = get_linear_schedule_with_warmup(optim_ctxt, \n",
    "                                                 num_warmup_steps = 0, \n",
    "                                                 num_training_steps = len(train_dataloader) // grad_acc_steps * num_train_epochs)\n",
    "optim_m = torch.optim.AdamW(m.parameters(), lr=2e-5) \n",
    "scheduler_m = get_linear_schedule_with_warmup(optim_m, \n",
    "                                                 num_warmup_steps = 0, \n",
    "                                                 num_training_steps = len(train_dataloader) // grad_acc_steps * num_train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e328ca16-7aa3-4f50-bc5d-035c80429190",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#First evalaute on monitor\n",
    "ctxt_model.eval()\n",
    "cand_model.eval()\n",
    "m.eval()\n",
    "all_loss=0\n",
    "print(\"Number of steps: \",len(valid_dataloader))\n",
    "with torch.no_grad():\n",
    "    num_correct = 0\n",
    "    num_all = 0\n",
    "    for step, batch in enumerate(valid_dataloader):\n",
    "        \n",
    "        #Get batch and clip based on longest item\n",
    "        context_input, candidate_input, e_ids  = batch\n",
    "        longest_cand = torch.max(torch.argmin(candidate_input,dim=1))\n",
    "        candidate_input = candidate_input[:,:longest_cand]\n",
    "        \n",
    "        if step%10==0:\n",
    "            print(\"Step:\",step,\" longest cand \",longest_cand)\n",
    "            \n",
    "        #Get the scores of each mention-entity pair\n",
    "        context_token_idx, context_segment_idx, context_mask = to_bert_input(context_input.to(device),device)\n",
    "        candidate_token_idx, candidate_segment_idx, candidate_mask = to_bert_input(candidate_input.to(device),device)\n",
    "        \n",
    "        context_rep = ctxt_model(context_token_idx, context_segment_idx, context_mask)[0][:,0,:]\n",
    "        cand_rep = cand_model(candidate_token_idx, candidate_segment_idx, candidate_mask)[0][:,0,:]\n",
    "        \n",
    "        scores = context_rep.mul(cand_rep)\n",
    "        scores = m(scores)\n",
    "        \n",
    "        #Calculate loss\n",
    "        loss = torch.nn.functional.cross_entropy(scores, e_ids.to(device),weight=torch.tensor(class_weights).to(device))\n",
    "        all_loss+=loss\n",
    "        #Calculate score\n",
    "        outputs = np.argmax(scores.cpu().detach(), axis=1)\n",
    "        outputs = np.sum(outputs.numpy() == e_ids.numpy())\n",
    "        num_correct += outputs\n",
    "        num_all += context_rep.size(0)\n",
    "all_loss/=len(valid_dataloader)\n",
    "print(\"Val_Loss: \",all_loss)\n",
    "print(\"Val_Acc: \",num_correct/num_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "28c5cb69-9d8e-4fe5-98d7-660abeccc443",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Put everything in training mode\n",
    "ctxt_model.train()\n",
    "cand_model.train()\n",
    "m.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fb0a1c6e-7c87-464a-8d5c-b5c496a109c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print('Number of steps per epoch: ',len(train_dataloader))\n",
    "print('Number of steps with accumulation: ',len(train_dataloader)//grad_acc_steps)\n",
    "\n",
    "#Reset Gradients\n",
    "optim_cand.zero_grad()\n",
    "optim_ctxt.zero_grad()\n",
    "optim_m.zero_grad()\n",
    "start=time.time()\n",
    "#Loop over epocs\n",
    "for epoch in range(num_train_epochs):\n",
    "    print(\"Epoch \",epoch)\n",
    "    #Store average training loss here\n",
    "    avg_loss = []\n",
    "    #Loop over minibatches\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        #Get the batch\n",
    "        context_input, candidate_input, e_ids  = batch\n",
    "        \n",
    "        longest_cand = torch.max(torch.argmin(candidate_input,dim=1))\n",
    "        candidate_input = candidate_input[:,:longest_cand]\n",
    "        \n",
    "        context_token_idx, context_segment_idx, context_mask = to_bert_input(context_input.to(device),device)\n",
    "        candidate_token_idx, candidate_segment_idx, candidate_mask = to_bert_input(candidate_input.to(device),device)\n",
    "        #Get representations concerning the cls token\n",
    "        context_rep = ctxt_model(context_token_idx, context_segment_idx, context_mask)[0][:,0,:]\n",
    "        cand_rep = cand_model(candidate_token_idx, candidate_segment_idx, candidate_mask)[0][:,0,:]\n",
    "        \n",
    "        #Calculate scores\n",
    "        scores = context_rep.mul(cand_rep)\n",
    "        scores = m(scores)\n",
    "        \n",
    "        #Calculate loss for storing\n",
    "        loss = torch.nn.functional.cross_entropy(scores, e_ids.to(device),weight=torch.tensor(class_weights).to(device))\n",
    "        avg_loss.append(loss.item())\n",
    "        \n",
    "        \n",
    "        #Divide loss by grad_acc_steps for backprop\n",
    "        loss = loss/grad_acc_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        if (step+1)%2000==0:\n",
    "            print(\"\\tStep: \",step+1,\" Loss: \",avg_loss[-1],\" Longest Cand: \",longest_cand,\" \",time.time()-start)\n",
    "        \n",
    "        #Do an update if you have accumulated enough\n",
    "        if (step+1)%grad_acc_steps==0:\n",
    "            #Normalize gradients\n",
    "            torch.nn.utils.clip_grad_norm_(ctxt_model.parameters(), grad_norm)\n",
    "            torch.nn.utils.clip_grad_norm_(cand_model.parameters(), grad_norm)\n",
    "            torch.nn.utils.clip_grad_norm_(m.parameters(), grad_norm)\n",
    "            #Step the optimizer and scheduler\n",
    "            #Reset gradients\n",
    "            optim_cand.step()\n",
    "            scheduler_cand.step()\n",
    "            optim_cand.zero_grad()\n",
    "            optim_ctxt.step()\n",
    "            optim_ctxt.zero_grad()\n",
    "            scheduler_ctxt.step()\n",
    "            optim_m.step()\n",
    "            scheduler_m.step()\n",
    "            optim_m.zero_grad()\n",
    "            \n",
    "    #Reset gradients at the end of epoch    \n",
    "    optim_cand.zero_grad()\n",
    "    optim_ctxt.zero_grad()\n",
    "    optim_m.zero_grad()\n",
    "    #Put model to eval mode\n",
    "    ctxt_model.eval()\n",
    "    cand_model.eval()\n",
    "    m.eval()\n",
    "    #This will store validation loss\n",
    "    all_loss=0\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        num_all = 0\n",
    "        for step, batch in enumerate(valid_dataloader):\n",
    "            context_input, candidate_input, e_ids  = batch\n",
    "            \n",
    "            longest_cand = torch.max(torch.argmin(candidate_input,dim=1))\n",
    "            candidate_input = candidate_input[:,:longest_cand]\n",
    "            \n",
    "            context_token_idx, context_segment_idx, context_mask = to_bert_input(context_input.to(device),device)\n",
    "            candidate_token_idx, candidate_segment_idx, candidate_mask = to_bert_input(candidate_input.to(device),device)\n",
    "            context_rep = ctxt_model(context_token_idx, context_segment_idx, context_mask)[0][:,0,:]\n",
    "            cand_rep = cand_model(candidate_token_idx, candidate_segment_idx, candidate_mask)[0][:,0,:]\n",
    "            scores = context_rep.mul(cand_rep)\n",
    "            scores = m(scores)\n",
    "            loss = torch.nn.functional.cross_entropy(scores, e_ids.to(device),weight=torch.tensor(class_weights).to(device))\n",
    "            all_loss+=loss\n",
    "            outputs = np.argmax(scores.cpu().detach(), axis=1)\n",
    "            outputs = np.sum(outputs.numpy() == e_ids.numpy())\n",
    "            num_correct += outputs\n",
    "            num_all += context_rep.size(0)\n",
    "    all_loss/=len(valid_dataloader)\n",
    "    print(\"Val_Loss: \",all_loss)\n",
    "    print(\"Val_Acc: \",num_correct/num_all)\n",
    "    print(\"Train_loss\",np.mean(avg_loss))\n",
    "    ctxt_model.train()\n",
    "    cand_model.train()\n",
    "    m.train()\n",
    "torch.save(ctxt_model,INP_PATH+\"randomneg_ctxt_model_1.pt\")\n",
    "torch.save(cand_model,INP_PATH+\"randomneg_cand_model_1.pt\")\n",
    "torch.save(m,INP_PATH+\"randomneg_m_1.pt\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "BiEncoderRandomNeg",
   "notebookOrigID": 3254413,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
