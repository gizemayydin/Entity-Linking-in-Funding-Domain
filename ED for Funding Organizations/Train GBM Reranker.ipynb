{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs\n",
    "\n",
    "* `path`: In this path, these must be present:\n",
    "    * `entities.pkl`: Pickle file that stores a dictionary. In this dictionary, keys are IDs of entities. For each entity, the value is a dictionary with a single key:\n",
    "        * `Labels`: List of strings where each string is an alternative name/label of the organization.\n",
    "       Will be unpickled as:\n",
    "       ```python\n",
    "with open(path+'entities.pkl','rb') as f:\n",
    "            entity_labels=pickle.load(f)\n",
    "       ```\n",
    "     \n",
    "    * `commonness.json`: Stores estimated commonness values. Should be calculated beforehand. In this json file, keys are mentions, and values are also in json format. For each mention that a commonness value is calculated, the json contains the entity IDs with their corresponding commonness values. Example:\n",
    "    ```json\n",
    "    {\n",
    "        \"mention_1\":{\n",
    "                         \"entity_1\":0.9,\n",
    "                         \"entity_2\":0.05,\n",
    "                         \"entity_3\":0.05\n",
    "        },\n",
    "        \"mention_2\": {\n",
    "                         \"entity_1\":0.6,\n",
    "                         \"entity_4\":0.3\n",
    "        },\n",
    "        \"mention_3\":{\n",
    "                         \"entity_5\":1.0\n",
    "        }\n",
    "    }\n",
    "            \n",
    "    ```\n",
    "    Mention, entity pairs with a commonness of 0 do not need to be included.\n",
    "    * `link_prob.json`: Stores estimated link probabilities. Should be calculated beforehand. Keys are mentions and values are link probabilities\n",
    "    * `popularity.json`: Stores estimated entity popularity values. Should be calculated beforehand. Keys are entities and values are popularities.\n",
    "    * `train_preds.pkl` and `dev_preds.pkl`: Can be obtained by running `Prediction with Biencoder.ipynb`\n",
    "    * `entity_pool.pkl`\n",
    "    * `train.jsonl`\n",
    "    * `dev.jsonl`\n",
    "    \n",
    "# Outputs\n",
    "* Model is written to `\"lgbm12.pkl\"`. Also, threshold selection (for NIL mention detection) is performed at the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import time\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fname = \"train.jsonl\"\n",
    "monitor_fname = \"dev.jsonl\"\n",
    "seed = 0\n",
    "num_cands = 12\n",
    "file_train_cands = \"train_preds.pkl\"\n",
    "file_monitor_cands = \"dev_preds.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data \n",
    "train_samples = []\n",
    "with open(path_b+train_fname, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        train_samples.append(json.loads(line.strip()))\n",
    "print(len(train_samples))\n",
    "\n",
    "monitor_samples = []\n",
    "with open(path_b+monitor_fname, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        monitor_samples.append(json.loads(line.strip()))\n",
    "print(len(monitor_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+'entities.pkl','rb') as f:\n",
    "    entity_labels=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+'entity_pool.pkl',\"rb\") as f:\n",
    "    entity_pool = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+file_train_cands,'rb') as f:\n",
    "    train_init = pickle.load(f)\n",
    "\n",
    "with open(path+file_monitor_cands,'rb') as f:\n",
    "    monitor_init = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_correct_entities = [x['label_id'] for x in train_samples]\n",
    "train_mentions = [x['mention'] for x in train_samples]\n",
    "\n",
    "monitor_correct_entities = [x['label_id'] for x in monitor_samples]\n",
    "monitor_mentions = [x['mention'] for x in monitor_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+'commonness.json','r',encoding='utf-8') as f:\n",
    "    commonness = json.load(f)\n",
    "with open(path+'link_prob.json','r',encoding='utf-8') as f:\n",
    "    link_probability = json.load(f)\n",
    "with open(path+'popularity.json','r',encoding='utf-8') as f:\n",
    "    popularity = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bert_scores = []\n",
    "train_fw_scores = []\n",
    "train_fw_scores2 = []\n",
    "train_target = []\n",
    "train_unique_id = []\n",
    "train_entity = []\n",
    "train_commonness = []\n",
    "train_popularity = []\n",
    "train_link_probability = []\n",
    "\n",
    "for i in range(len(train_samples)):\n",
    "    candidates = train_init[i]\n",
    "    this_mention = train_mentions[i]\n",
    "    for j in range(num_cands):\n",
    "        #Get FW score\n",
    "        \n",
    "        this_ent_labels = entity_labels[str(candidates[j][1])]['Labels']\n",
    "        fw_score = 0\n",
    "        for lbl in this_ent_labels:\n",
    "            fw_score = max(fw_score,fuzz.token_set_ratio(this_mention,lbl)/100)\n",
    "        train_fw_scores.append(fw_score)\n",
    "        \n",
    "        train_commonness.append(commonness.get(this_mention.lower(),{}).get(str(candidates[j][1]),0.))\n",
    "        train_popularity.append(popularity.get(str(candidates[j][1]),0.))\n",
    "        train_link_probability.append(link_probability.get(this_mention.lower(),0.))\n",
    "        \n",
    "        fw_score2 = 0\n",
    "        for lbl in this_ent_labels:\n",
    "            fw_score2 = max(fw_score2,fuzz.token_sort_ratio(this_mention,lbl)/100)\n",
    "        train_fw_scores2.append(fw_score2)\n",
    "        \n",
    "        #Get BERT score\n",
    "        train_bert_scores.append(candidates[j][0])\n",
    "        \n",
    "        #Get target\n",
    "        if train_correct_entities[i] is not None:\n",
    "            #if str(candidates[j][1]) in entity_pool[train_correct_entities[i]]:\n",
    "            if str(candidates[j][1]) == train_correct_entities[i]:\n",
    "                train_target.append(1)\n",
    "            else:\n",
    "                train_target.append(0)\n",
    "        else:\n",
    "            train_target.append(0)\n",
    "        \n",
    "        train_unique_id.append(i)\n",
    "        train_entity.append(candidates[j][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.DataFrame({'ID':train_unique_id,'Commonness':train_commonness,'BERT':train_bert_scores,\n",
    "                       'Popularity':train_popularity,'Link_Probability':train_link_probability,\n",
    "                       'FW':train_fw_scores,'FW2':train_fw_scores2,'Entity':train_entity,'Target':train_target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_bert_scores = []\n",
    "monitor_fw_scores = []\n",
    "monitor_fw_scores2 = []\n",
    "monitor_target = []\n",
    "monitor_unique_id = []\n",
    "monitor_entity = []\n",
    "monitor_commonness = []\n",
    "monitor_popularity = []\n",
    "monitor_link_probability = []\n",
    "\n",
    "for i in range(len(monitor_samples)):\n",
    "    candidates = monitor_init[i]\n",
    "    this_mention = monitor_mentions[i]\n",
    "    for j in range(num_cands):\n",
    "        #Get FW score\n",
    "        this_ent_labels = entity_labels[str(candidates[j][1])]['Labels']\n",
    "        fw_score = 0\n",
    "        for lbl in this_ent_labels:\n",
    "            fw_score = max(fw_score,fuzz.token_set_ratio(this_mention,lbl)/100)\n",
    "        monitor_fw_scores.append(fw_score)\n",
    "        \n",
    "        fw_score2 = 0\n",
    "        for lbl in this_ent_labels:\n",
    "            fw_score2 = max(fw_score2,fuzz.token_sort_ratio(this_mention,lbl)/100)\n",
    "        monitor_fw_scores2.append(fw_score2)\n",
    "        \n",
    "        #Get BERT score\n",
    "        monitor_bert_scores.append(candidates[j][0])\n",
    "        \n",
    "        monitor_commonness.append(commonness.get(this_mention.lower(),{}).get(str(candidates[j][1]),0.))\n",
    "        monitor_popularity.append(popularity.get(str(candidates[j][1]),0.))\n",
    "        monitor_link_probability.append(link_probability.get(this_mention.lower(),0.))\n",
    "        \n",
    "        #Get target\n",
    "        if monitor_correct_entities[i] is not None:\n",
    "            #if str(candidates[j][1]) in entity_pool[monitor_correct_entities[i]]:\n",
    "            if str(candidates[j][1]) == train_correct_entities[i]:\n",
    "                monitor_target.append(1)\n",
    "            else:\n",
    "                monitor_target.append(0)\n",
    "        else:\n",
    "            monitor_target.append(0)\n",
    "        \n",
    "        monitor_unique_id.append(i)\n",
    "        monitor_entity.append(candidates[j][1])\n",
    "monitor_df=pd.DataFrame({'ID':monitor_unique_id,'Commonness':monitor_commonness,'BERT':monitor_bert_scores,\n",
    "                         'Popularity':monitor_popularity,'Link_Probability':monitor_link_probability,\n",
    "                         'FW':monitor_fw_scores,'FW2':monitor_fw_scores2,'Entity':monitor_entity,'Target':monitor_target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thresholded_preds(th,pred,scores):\n",
    "    thresholded_preds = []\n",
    "    for i in range(len(pred)):\n",
    "        if scores[i]>=th:\n",
    "            thresholded_preds.append(pred[i])\n",
    "        else:\n",
    "            thresholded_preds.append(None)\n",
    "    return thresholded_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df= train_df.astype({'Commonness':float,\n",
    "                           'BERT':float,\n",
    "                           'Popularity':float,\n",
    "                           'Link_Probability':float,\n",
    "                           'FW2':float})\n",
    "monitor_df= monitor_df.astype({'Commonness':float,\n",
    "                           'BERT':float,\n",
    "                           'Popularity':float,\n",
    "                           'Link_Probability':float,\n",
    "                           'FW2':float})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params=dict()\n",
    "params['max_bin']=63\n",
    "params['learning_rate']=0.1\n",
    "params['min_data_in_leaf']=100\n",
    "params['bagging_freq']=1\n",
    "params['bagging_fraction']=0.9\n",
    "params['lambda_l1']=1\n",
    "params['lambda_l2']=1\n",
    "params['min_gain_to_split']=1\n",
    "params['objective']='binary' \n",
    "params['metric']='binary_logloss' \n",
    "params[\"is_unbalance\"] = False\n",
    "params[\"seed\"] = 25\n",
    "params[\"extra_trees\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['Commonness', 'BERT', 'FW2', 'Popularity','Link_Probability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train=lgb.Dataset(train_df[selected_features], label=train_df['Target'].values)\n",
    "model_lgb=lgb.train(params,d_train,100,valid_sets=[lgb.Dataset(monitor_df[selected_features], label=monitor_df['Target'].values),d_train],verbose_eval=50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(model_lgb,importance_type='gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Pred'] = model_lgb.predict(train_df[selected_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = train_df.copy(deep=True)\n",
    "temp=temp.loc[temp.groupby('ID').Pred.idxmax().values][['ID','Pred','Entity']]\n",
    "train_entities = temp.Entity.values\n",
    "train_scores = temp.Pred.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find best threshold based on micro avg acc\n",
    "thresholds = np.arange(0,1.001,0.001)\n",
    "best_th = None\n",
    "max_score = 0\n",
    "for th in thresholds:\n",
    "    ctr = 0\n",
    "    thresholded_preds =  get_thresholded_preds(th,train_entities,train_scores)\n",
    "    for i in range(len(train_correct_entities)):\n",
    "        if thresholded_preds[i] is None and train_correct_entities[i] is None:\n",
    "            ctr+=1\n",
    "        elif train_correct_entities[i] is not None and str(thresholded_preds[i]) in entity_pool[train_correct_entities[i]]:\n",
    "            ctr+=1\n",
    "    if ctr>max_score:\n",
    "        max_score=ctr\n",
    "        best_th = th\n",
    "print(\"Best threshold: \",best_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate micro avg acc per threshold\n",
    "print(\"TRAIN\")\n",
    "thresholds = [0, 0.5,best_th]\n",
    "for th in thresholds:\n",
    "    print(\"Threshold: \",th)\n",
    "    ctr = 0\n",
    "    thresholded_preds =  get_thresholded_preds(th,train_entities,train_scores)\n",
    "    for i in range(len(train_correct_entities)):\n",
    "        if thresholded_preds[i] is None and train_correct_entities[i] is None:\n",
    "            ctr+=1\n",
    "        elif train_correct_entities[i] is not None and str(thresholded_preds[i]) in entity_pool[train_correct_entities[i]]:\n",
    "            ctr+=1\n",
    "    print('Micro Average Accuracy: ',np.round((100*ctr)/len(train_correct_entities),2),'%\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lgbm12.pkl','wb') as f:\n",
    "    pickle.dump(model_lgb,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
