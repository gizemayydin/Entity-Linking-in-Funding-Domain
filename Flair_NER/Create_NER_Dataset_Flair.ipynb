{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############LIBRARIES################\n",
    "import sys\n",
    "sys.path.append(\"..\\\\..\\\\CreateNERSilverSet_NewPipelineOutput\")\n",
    "sys.path.append(\"..\\\\..\\\\PrepareBERT_forDatabricks\")\n",
    "\n",
    "from NERSilverSetFromSentences import *\n",
    "from GrantAndOrganizationMentionsFromGoldSet import grant_and_organization_mentions_from_gold_set\n",
    "from AnnotateForBert import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizerFast\n",
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aydxng\\Anaconda3\\envs\\Python37\\lib\\site-packages\\urllib3\\connectionpool.py:988: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "C:\\Users\\aydxng\\Anaconda3\\envs\\Python37\\lib\\site-packages\\urllib3\\connectionpool.py:988: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    }
   ],
   "source": [
    "############VARIABLES################\n",
    "#Load tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######READ THE DATA##########\n",
    "path = \"C:\\\\Users\\\\aydxng\\\\Documents\\\\ds-fundingbodies-linkingcomponent-masterthesis\\\\Thesis\\\\CreateNERSilverSet_NewPipelineOutput\\\\\"\n",
    "with open(path+'df_sent_divided.pkl','rb') as f:\n",
    "    df= pickle.load(f)\n",
    "    \n",
    "#Discard the samples in \"Validation\" folder\n",
    "folders_validation = ['Validation\\\\Round_2','Validation\\\\Round_3','Validation\\\\Round_6','Validation\\\\Round_7','Validation\\\\Round_9',\n",
    "                      'Validation\\\\Round_11','Validation\\\\Round_13','Validation\\\\Round_17','Validation\\\\Round_18','Validation\\\\Round_19',\n",
    "                      'Validation\\\\Round_20','Validation\\\\Round_22','Validation\\\\Round_23','Validation\\\\Round_25','Validation\\\\Round_27',\n",
    "                      'Validation\\\\Round_28','Validation\\\\Round_30','Validation\\\\Round_32']\n",
    "df = df[~df.Folder.isin(folders_validation)].copy(deep=True)\n",
    "\n",
    "#Split train/valid/test\n",
    "train = df[df.Dataset=='Training'].reset_index(drop=True).copy(deep=True)\n",
    "valid = df[df.Dataset=='Validation'].reset_index(drop=True).copy(deep=True)\n",
    "\n",
    "#Drop not-needed columns\n",
    "train.drop(['Dataset','Pipeline_Span_Tags_IOB','Pipeline_Span_Tags_GRT_IOB','Pipeline_Span_Tags_ORG_IOB'],\n",
    "       axis=1, inplace=True)\n",
    "valid.drop(['Dataset','Pipeline_Span_Tags_IOB','Pipeline_Span_Tags_GRT_IOB','Pipeline_Span_Tags_ORG_IOB'],\n",
    "       axis=1, inplace=True)\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in file:  00812K_622654249.ann\n",
      "Error in file:  00901K_618203602.ann\n",
      "Error in file:  07147K_618282364.ann\n",
      "Error in file:  09209K_358501993.ann\n",
      "Error in file:  06041T_618636215.ann\n",
      "Error in file:  06821T_620696511.ann\n",
      "Error in file:  07347T_621231937.ann\n",
      "Error in file:  07451R_605779630.ann\n",
      "Error in file:  07964R_355725573.ann\n",
      "Error in file:  09112R_619440406.ann\n",
      "Error in file:  16263R_359218191.ann\n",
      "Error in file:  17677R_354095974.ann\n",
      "Error in file:  17677R_354095974.ann\n",
      "Error in file:  S0141933115000630.ann\n",
      "Error in file:  S0141933115000630.ann\n",
      "Error in file:  S0141933115000630.ann\n",
      "Error in file:  S0165011412002217.ann\n",
      "Error in file:  S0166093415003067.ann\n",
      "Error in file:  S0166218X1400016X.ann\n",
      "Error in file:  S0168192316303598.ann\n",
      "Error in file:  S0168192316303598.ann\n",
      "Error in file:  S0169260714002946.ann\n",
      "Error in file:  S0267364914001277.ann\n",
      "Error in file:  S0268005X16300492.ann\n",
      "Error in file:  S0301679X1500242X.ann\n",
      "Error in file:  S0304397511003926.ann\n",
      "Error in file:  S0304397512002356.ann\n",
      "Error in file:  S0304401716304034.ann\n",
      "Error in file:  S0304401716304034.ann\n",
      "Error in file:  S0304423813004317.ann\n",
      "Error in file:  S0370269310009767.ann\n",
      "Error in file:  S0370269310009767.ann\n",
      "Error in file:  S0370269310009767.ann\n",
      "Error in file:  S0380133005702409.ann\n",
      "Error in file:  S0723202014001064.ann\n",
      "Error in file:  S0747717111002021.ann\n",
      "Error in file:  S089360801400046X.ann\n",
      "Error in file:  S0893965914000068.ann\n",
      "Error in file:  S0893965914000068.ann\n",
      "Error in file:  S0896841111001223.ann\n",
      "Error in file:  S0925400513014391.ann\n",
      "Error in file:  S0925400514012507.ann\n",
      "Error in file:  S0960852415004009.ann\n",
      "Error in file:  S0960852415004009.ann\n",
      "Error in file:  S0960852415004009.ann\n",
      "Error in file:  S1000936115000266.ann\n",
      "Error in file:  S100207050860014X.ann\n",
      "Error in file:  S1874569506800163.ann\n",
      "Error in file:  0094-32265685.ann\n",
      "Error in file:  0739-51911113.ann\n",
      "Error in file:  0739-51911113.ann\n",
      "Error in file:  0736-51465135.ann\n",
      "Error in file:  0736-51465135.ann\n",
      "Error in file:  0736-51465135.ann\n",
      "Error in file:  0736-51465135.ann\n",
      "Error in file:  0736-51465135.ann\n",
      "Error in file:  0736-51465135.ann\n",
      "Error in file:  0736-51465135.ann\n"
     ]
    }
   ],
   "source": [
    "######GET GOLD NAMED ENTITIES##########\n",
    "path_to_gold = \"C:\\\\Users\\\\aydxng\\\\OneDrive - Reed Elsevier Group ICO Reed Elsevier Inc\\\\Desktop\\\\ElsevierData\\\\Dataset2020July\\\\Dataset2020July\\\\\"\n",
    "folders_kpi = ['GoldSet_2019\\\\KPI\\\\']\n",
    "folders_trainin_train = ['GoldSet_2019\\\\Trainin-Train\\\\']\n",
    "folders_trainin_validation = ['GoldSet_2019\\\\Trainin-Validation\\\\']\n",
    "folders_trainin_test = ['GoldSet_2019\\\\Trainin-Test\\\\']\n",
    "folders_regression = ['GoldSet_2019\\\\Regression\\\\']\n",
    "folders_improvement = ['Improvement\\\\Round_1\\\\','Improvement\\\\Round_10\\\\','Improvement\\\\Round_12\\\\','Improvement\\\\Round_14\\\\','Improvement\\\\Round_15\\\\','Improvement\\\\Round_21\\\\','Improvement\\\\Round_24\\\\','Improvement\\\\Round_26\\\\','Improvement\\\\Round_29\\\\','Improvement\\\\Round_31\\\\','Improvement\\\\Round_4\\\\','Improvement\\\\Round_5\\\\','Improvement\\\\Round_8\\\\' ]\n",
    "#Gold named entities per folder\n",
    "org_dict_kpi, grant_dict_kpi = grant_and_organization_mentions_from_gold_set([path_to_gold+x for x in folders_kpi])\n",
    "org_dict_trainin_train, grant_dict_trainin_train = grant_and_organization_mentions_from_gold_set([path_to_gold+x for x in folders_trainin_train])\n",
    "org_dict_trainin_validation, grant_dict_trainin_validation = grant_and_organization_mentions_from_gold_set([path_to_gold+x for x in folders_trainin_validation])\n",
    "org_dict_trainin_test, grant_dict_trainin_test = grant_and_organization_mentions_from_gold_set([path_to_gold+x for x in folders_trainin_test])\n",
    "org_dict_regression, grant_dict_regression = grant_and_organization_mentions_from_gold_set([path_to_gold+x for x in folders_regression])\n",
    "org_dict_improvement, grant_dict_improvement = grant_and_organization_mentions_from_gold_set([path_to_gold+x for x in folders_improvement])\n",
    "org_dict_gold = dict()\n",
    "grant_dict_gold = dict()\n",
    "org_dict_gold.update(org_dict_kpi)\n",
    "grant_dict_gold.update(grant_dict_kpi)\n",
    "org_dict_gold.update(org_dict_trainin_train)\n",
    "grant_dict_gold.update(grant_dict_trainin_train)\n",
    "org_dict_gold.update(org_dict_trainin_validation)\n",
    "grant_dict_gold.update(grant_dict_trainin_validation)\n",
    "org_dict_gold.update(org_dict_trainin_test)\n",
    "grant_dict_gold.update(grant_dict_trainin_test)\n",
    "org_dict_gold.update(org_dict_regression)\n",
    "grant_dict_gold.update(grant_dict_regression)\n",
    "org_dict_gold.update(org_dict_improvement)\n",
    "grant_dict_gold.update(grant_dict_improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input df:  27416\n",
      "Number of rows with conflicting annotations:  33\n",
      "Percentage:  0.12036766851473593 %\n"
     ]
    }
   ],
   "source": [
    "########PROCESS TRAINING SET##########\n",
    "res = tokenize_with_bert(train,tokenizer)\n",
    "train['Sentence_Tokenized'] = res[0]\n",
    "train['Token_Spans'] = res[1]\n",
    "\n",
    "#Get Gold span tags for organizations\n",
    "train['Gold_Span_Tags_ORG_IOB'] = get_span_tags_iob(train,org_dict_gold,\"\",\"Token_Spans\")\n",
    "#Get Gold span tags for grants\n",
    "train['Gold_Span_Tags_GRT_IOB'] = get_span_tags_iob(train,grant_dict_gold,\"\",\"Token_Spans\")\n",
    "#Fix the tags\n",
    "train['Gold_Span_Tags_ORG_IOB'] = fix_iob_tags(train['Gold_Span_Tags_ORG_IOB'].values)\n",
    "train['Gold_Span_Tags_GRT_IOB'] = fix_iob_tags(train['Gold_Span_Tags_GRT_IOB'].values)\n",
    "#Merge the tags\n",
    "train['Gold_Span_Tags_IOB'] = merge_grant_and_org_annotations_iob(train,'Gold_Span_Tags_GRT_IOB','Gold_Span_Tags_ORG_IOB')\n",
    "\n",
    "#Separate a part for monitoring the training\n",
    "train_monitor_idx = np.random.choice(train[train.Folder=='GoldSet_2019\\\\KPI'].Filename.unique(),1000,replace=False)\n",
    "train_monitor = train[train.Filename.isin(train_monitor_idx)].reset_index(drop=True).copy(deep=True)\n",
    "train = train[~train.Filename.isin(train_monitor_idx)].reset_index(drop=True).copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input df:  5012\n",
      "Number of rows with conflicting annotations:  3\n",
      "Percentage:  0.05985634477254589 %\n"
     ]
    }
   ],
   "source": [
    "########PROCESS VALIDATION SET##########\n",
    "res = tokenize_with_bert(valid,tokenizer)\n",
    "valid['Sentence_Tokenized'] = res[0]\n",
    "valid['Token_Spans'] = res[1]\n",
    "\n",
    "#Get Gold span tags for organizations\n",
    "valid['Gold_Span_Tags_ORG_IOB'] = get_span_tags_iob(valid,org_dict_gold,\"\",\"Token_Spans\")\n",
    "#Get Gold span tags for grants\n",
    "valid['Gold_Span_Tags_GRT_IOB'] = get_span_tags_iob(valid,grant_dict_gold,\"\",\"Token_Spans\")\n",
    "#Fix the tags\n",
    "valid['Gold_Span_Tags_ORG_IOB'] = fix_iob_tags(valid['Gold_Span_Tags_ORG_IOB'].values)\n",
    "valid['Gold_Span_Tags_GRT_IOB'] = fix_iob_tags(valid['Gold_Span_Tags_GRT_IOB'].values)\n",
    "#Merge the tags\n",
    "valid['Gold_Span_Tags_IOB'] = merge_grant_and_org_annotations_iob(valid,'Gold_Span_Tags_GRT_IOB','Gold_Span_Tags_ORG_IOB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####WRITE TO FILE#########\n",
    "with open(\".\\\\Corpus\\\\training.txt\",'w',encoding='utf-8') as f:\n",
    "    for index, row in train.iterrows():\n",
    "        if len(row['Sentence'])<=1000:\n",
    "            words = row['Sentence_Tokenized']\n",
    "            tokens = row['Gold_Span_Tags_IOB']\n",
    "            for i in range(len(tokens)):\n",
    "                f.write(words[i]+\" \"+tokens[i]+'\\n')\n",
    "            f.write('\\n')\n",
    "with open(\".\\\\Corpus\\\\validation.txt\",'w',encoding='utf-8') as f:\n",
    "    for index, row in train_monitor.iterrows():\n",
    "        if len(row['Sentence'])<=1000:\n",
    "            words = row['Sentence_Tokenized']\n",
    "            tokens = row['Gold_Span_Tags_IOB']\n",
    "            for i in range(len(tokens)):\n",
    "                f.write(words[i]+\" \"+tokens[i]+'\\n')\n",
    "            f.write('\\n')\n",
    "with open(\".\\\\Corpus\\\\test.txt\",'w',encoding='utf-8') as f:\n",
    "    for index, row in valid.iterrows():\n",
    "        words = row['Sentence_Tokenized']\n",
    "        tokens = row['Gold_Span_Tags_IOB']\n",
    "        for i in range(len(tokens)):\n",
    "            f.write(words[i]+\" \"+tokens[i]+'\\n')\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"flair_valid.pkl\",'wb') as f:\n",
    "    pickle.dump(valid,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
